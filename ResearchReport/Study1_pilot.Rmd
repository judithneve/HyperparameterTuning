---
title: "Study 1 pilot"
author: "Judith Neve"
date: '2022-10-31'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(magrittr)
library(pmsampsize)
library(tidyr)
library(MASS)
library(dplyr)
library(pROC)
library(ranger)
library(caret)
library(psych)
set.seed(0070661)
```

# Aim

Identify the best combination of hyperparameters for which tuning leads to the best predictive performance of a prediction model.

# DGM

## Population

```{r}
# factors we vary in population factors: number of predictors, event fraction, sample sizr
p  <- c(8, 16, 32)
EF <- c(0.1, 0.3, 0.5)
N  <- c(0.5, 1) # scrapped x2
```

```{r}
source("DataSimFunctions.R")
n_datasets <- 10 # number of datasets we simulate per scenario
# n_datasets <- 1000
```

```{r}
scenarios <- make_scenarios(n_pred = p, event_fraction = EF, sample_size = N)
save(scenarios, file = "Data/scenarios.RData")
```

## True effect estimation

```{r}
source("GenerateBetas_RR.R")
 
start_seed <- 100
example_n <- 1e5
n_beta_repetitions <- 20

# regression coefficients numerical estimation
betas_matrix <- mean_multiple_betas(
  n_predictors = n_pred, prevalences = event_fraction, n_beta_repetitions = n_beta_repetitions,
  example_n = example_n, start_seed = start_seed
)
# validation of regression coefficients
set.seed(2 * start_seed)
validation_betas_matrix <- validate_betas(
 betas_matrix = betas_matrix[[1]], example_n = example_n, n_predictors = p, prevalences = EF
)
validation_betas_matrix
set.seed(3 * start_seed)
validation_betas_matrix_noint <- validate_betas_noint(
 betas_matrix = betas_matrix[[1]], example_n = example_n, n_predictors = p, prevalences = EF
)
validation_betas_matrix_noint

# TODO: fix beta generation
# TODO: fix beta validation

save(betas_matrix, file = "Data/betas.RData")
```

```{r}
load("Data/betas.RData")
load("Data/scenarios.RData")

# FOR NOW: use p = 8, EF = 0.5
betas_matrix <- betas_matrix[[1]][3,] %>% as.matrix() %>% t()
scenarios <- scenarios %>% filter(n_pred == 8, event_fraction == 0.5, prop_sample_size == 1)
```

```{r}
# simulate data to send through the random forest tuning procedures
set.seed(678*4)
dat <- sim_data(scenarios, betas_matrix, n_datasets)
# TODO: add the generation of a validation dataset
# FOR NOW:
validation_sets <- sim_data(scenarios = scenarios %>% mutate(n = 10000),
                            coefs = betas_matrix,
                            nsim = n_datasets)
```

```{r}
# checking how many datasets were generated
dat[,"dataset_id"] %>% unique() %>% length()
validation_sets[,"dataset_id"] %>% unique() %>% length()
```


# Estimands

Predictive performance and computational time.

# Methods

```{r}
# which hyperparameters are we tuning?
hyperparameter_combinations <- expand.grid(
  mtry = TRUE,
  sample.fraction = c(TRUE, FALSE),
  # num.trees = c(TRUE, FALSE),
  replace = c(TRUE, FALSE),
  min.node.size = TRUE,
  splitrule = c(TRUE, FALSE)
)
hyperparameter_combinations <- rbind(rep(FALSE, ncol(hyperparameter_combinations)),
                                     hyperparameter_combinations)
# this is a dataframe of whether or not we're tuning the given HPs
```

```{r}
set.seed(1*4)

ctrl <- trainControl(
  method = "cv",
  number = 5
)
# TODO: set the folds rather than have them be done within train (?)
```

```{r}
source("TuningFunctions.R")
source("PerformanceMetrics.R")
```

# Methods & Performance measures in a for loop

```{r}
set.seed(500*4)

start_tuning <- Sys.time() # to record the total time the experiment takes
# set up a matrix to be filled with performance of each tuning procedure on each dataset
all_perf <- matrix(NA, nrow = length(unique(dat[,"dataset_id"]))*nrow(hyperparameter_combinations), ncol = 8 + 3 + 1 + 1) %>%
  as.data.frame()
colnames(all_perf) <- c("Runtime",
                        "AUC", "CalibrationSlope", "CalibrationIntercept", "BrierScore", "LogarithmicLoss", "Accuracy", "CohensKappa",
                        "sample_size_prop", "n_pred", "event_fraction",
                        "Tuned hyperparameters",
                        "dataset_id")

tuning_n <- 0 # to keep track of the row in which information has to go
for (dataset_id in unique(dat[,"dataset_id"])) {
  # put data in format that can be used to fit a RF
  dataset <- dat[dat[,"dataset_id"] == dataset_id,] %>%
    as.data.frame() %>%
    mutate(Pred_value = as.numeric(Pred_value)) %>% 
    pivot_wider(names_from = Pred_number, values_from = Pred_value)
  val_dataset <- validation_prep(validation_sets, dataset_id)
  for (combination in 1:nrow(hyperparameter_combinations)) {
    tuning_n <- tuning_n + 1
    cat("tuning_combination", tuning_n, "\n")
    # perform each tuning combination on the dataset
    if (any(hyperparameter_combinations[combination,])) {
      best_hp <- tune_hyperparameters(hyperparameter_combinations[combination,], dataset)
      cat("tuning time = ", best_hp$time, "\n")
      all_perf[tuning_n,"Runtime"] <- best_hp$time
      val_mod <- validate_model(dataset, best_hp)
    } else {
      default_start <- Sys.time()
      val_mod <- ranger(as.factor(Y) ~ .,
                        data = dataset %>% dplyr::select(-id, -sample_size_prop, -n_pred, -event_fraction, -dataset_id),
                        probability = TRUE,
                        num.trees = 500)
      default_end <- Sys.time()
      all_perf[tuning_n,"Runtime"] <- difftime(default_end, default_start, units = "mins")
    }
    # evaluate the model
    predict_val <- predict(val_mod, data = val_dataset)$predictions[,2]
    # record evaluations
    all_perf[tuning_n,2:8] <- performance(predict_val, val_dataset$Y)
    
    all_perf[tuning_n,9:11] <- unique(dataset %>% select(sample_size_prop, n_pred, event_fraction))
    all_perf[tuning_n,12] <- paste(colnames(hyperparameter_combinations)[as.logical(hyperparameter_combinations[combination,])], collapse = " + ")
    all_perf[tuning_n,13] <- dataset_id
  }
}
end_tuning <- Sys.time()

all_tuning_time <- end_tuning - start_tuning

save.image("Data/pilot.RData")
```


