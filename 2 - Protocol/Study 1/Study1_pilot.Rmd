---
title: "Study 1 pilot"
author: "Judith Neve"
date: '2022-10-31'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(magrittr)
library(pmsampsize)
library(tidyr)
library(MASS)
library(dplyr)
library(pROC)
```

# Aim

Identify the best combination of hyperparameters for which tuning leads to the best predictive performance of a prediction model.

# DGM

## Population

```{r}
p  <- c(8, 16, 32)
EF <- c(0.1, 0.3, 0.5)
N  <- c(0.5, 1, 2)
```

```{r}
source("DataSimFunctions.R")
```

```{r}
# scenarios <- make_scenarios(n_pred = p, event_fraction = EF, sample_size = N)
# save(scenarios, file = "scenarios.RData")
load("scenarios.RData")

# n_datasets <- 1000
n_datasets <- 3
```

## True effect estimation

```{r}
# source("GenerateBetas_3step.R")
# 
# start_seed <- 100
# example_n <- 1e5
# n_beta_repetitions <- 20
# 
# betas_matrix <- mean_multiple_betas(
#   n_predictors = n_pred, prevalences = event_fraction, n_beta_repetitions = n_beta_repetitions,
#   example_n = example_n, start_seed = start_seed
# )
# set.seed(2 * start_seed)
# validation_betas_matrix <- validate_betas(
#   betas_matrix = betas_matrix[[1]], example_n = example_n, n_predictors = n_pred, prevalences = event_fraction
# )
# set.seed(3 * start_seed)
# validation_betas_matrix_noint <- validate_betas_noint(
#   betas_matrix = betas_matrix[[1]], example_n = example_n, n_predictors = n_pred, prevalences = event_fraction
# )
# 
# save(betas_matrix, file = "betas_v2.RData")
load("betas_v2.RData")

# TODO: fix beta generation
# TODO: fix beta validation
# FOR NOW: use p = 8, EF = 0.1 & p = 16, EF = 0.3
betas_matrix <- betas_matrix[[1]][1,] %>% as.matrix() %>% t()
scenarios <- scenarios %>% filter((n_pred == 8 & event_fraction == 0.1))
```

```{r}
dat <- sim_data(scenarios, betas_matrix, n_datasets)
# TODO: add the generation of a validation dataset
# FOR NOW:
validation_sets <- sim_data(scenarios = scenarios %>% mutate(n = 10000),
                            coefs = betas_matrix,
                            nsim = n_datasets)
```

```{r}
dat[,"dataset_id"] %>% unique() %>% length()
validation_sets[,"dataset_id"] %>% unique() %>% length()
```


# Estimands

Predictive performance and computational time.

# Methods

```{r}
library(ranger)
library(caret)
# grid search
# optimise accuracy - what is the threshold
# 5-fold CV
```

```{r}
# i.e., which ones are we tuning?
hyperparameter_combinations <- expand.grid(
  mtry = TRUE,
  sample.fraction = TRUE,
  num.trees = c(TRUE, FALSE),
  replace = c(TRUE, FALSE),
  min.node.size = c(TRUE, FALSE),
  splitrule = c(TRUE, FALSE)
)
hyperparameter_combinations <- rbind(rep(FALSE, ncol(hyperparameter_combinations)),
                                     hyperparameter_combinations)
# this is a dataframe of whether or not we're tuning the given HPs
```

```{r}
set.seed(1)

ctrl <- trainControl(
  method = "cv",
  number = 5
)
# TODO: set the folds rather than have them be done within train (?)
```

```{r}
source("tuning_functions.R")
```

```{r}
# TODO: fix function so it can also work on no tuning
best_hp <- tune_hyperparameters(
  hyperparameter_combinations[2,],
  dat[dat[,"dataset_id"] == "5_1",] %>%
    as.data.frame() %>%
    pivot_wider(names_from = Pred_number, values_from = Pred_value)
)

runtime <- best_hp$time
# 3.5 minutes to tune only mtry and samplefrac
# 10 minutes to tune replace on top of this
```

# Performance measures

```{r}
# make this into a function
# validation dataset

val_d <- validation_prep(validation_sets, "1_1")
val_mod <- validate_model(
  dat[dat[,"dataset_id"] == "5_1",] %>%
    as.data.frame() %>%
    pivot_wider(names_from = Pred_number, values_from = Pred_value),
  best_hp
)

predict_val <- predict(val_mod, data = val_d)$predictions[,2]
```

```{r}
library(psych)
```

```{r}
source("PerformanceMetrics.R")
```

```{r}
performance(predict_val, val_d$Y)
```

# Methods & Performance measures in a for loop

```{r}
# load("3sets_beforeuning.RData")
source("DataSimFunctions.R")
source("PerformanceMetrics.R")
source("GenerateBetas_3step.R")
source("tuning_functions.R")
hyperparameter_combinations <- hyperparameter_combinations[c(1,15),]
dat <- dat[dat[,"dataset_id"] == "3_1",] # scenario, how many in that scenario

all_perf <- matrix(NA, nrow = length(unique(dat[,"dataset_id"]))*nrow(hyperparameter_combinations), ncol = 8 + 3 + 1) %>%
  as.data.frame()
colnames(all_perf) <- c("Runtime",
                        "AUC", "CalibrationSlope", "CalibrationIntercept", "BrierScore", "LogarithmicLoss", "Accuracy", "CohensKappa",
                        "sample_size_prop", "n_pred", "event_fraction",
                        "Tuned hyperparameters")

dataset_n <- 0
for (dataset_id in unique(dat[,"dataset_id"])) {
  dataset_n <- dataset_n + 1
  dataset <- dat[dat[,"dataset_id"] == dataset_id,] %>%
    as.data.frame() %>%
    mutate(Pred_value = as.numeric(Pred_value)) %>% 
    pivot_wider(names_from = Pred_number, values_from = Pred_value)
  for (combination in 2:nrow(hyperparameter_combinations)) { # can't do none yet
    # TODO: make sure this doesn't overwrite (some manipulation of dataset_n)
    best_hp <- tune_hyperparameters(hyperparameter_combinations[combination,], dataset)
    all_perf[dataset_n,"Runtime"] <- best_hp$time
    
    val_dataset <- validation_prep(validation_sets, dataset_id)
    val_mod <- validate_model(dataset, best_hp)
    
    predict_val <- predict(val_mod, data = val_dataset)$predictions[,2]
    all_perf[dataset_n,2:8] <- performance(predict_val, val_dataset$Y)
    
    all_perf[dataset_n,9:11] <- unique(dataset %>% select(sample_size_prop, n_pred, event_fraction))
    all_perf[dataset_n,12] <- paste(colnames(hyperparameter_combinations)[as.logical(hyperparameter_combinations[combination,])], collapse = " + ")
  }
}
```
