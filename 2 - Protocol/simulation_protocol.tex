\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[table]{xcolor}
\usepackage{soul}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{threeparttable}
\usepackage{amsmath}
\sethlcolor{yellow}
\usepackage[backend=biber,style=numeric-comp,sorting=none]{biblatex}
\addbibresource{Thesis.bib} %Import the bibliography file

\title{Simulation protocol}
\author{Judith Neve}

\begin{document}

\maketitle

\section{Studies}

\subsection{Aims}

\subsubsection{Study 1: Hyperparameters to tune}

Prior findings \cite{probst_tunability_2019} have shown the number of predictors considered at a split and the sample fraction to be the two most influential hyperparameters on model accuracy. However, these findings only investigate the effect of tuning one or two hyperparameters at once. This study aims to extend these findings by considering more combinations of hyperparameters in order to identify the combination of hyperparameters for which tuning leads to the best predictive performance of a prediction model.

\subsubsection{Study 2: Optimisation metric}

This study aims to identify the metric to optimise in the tuning procedure which leads to the best predictive performance of a prediction model. We tune the combination of hyperparameters considered to be the most optimal in Study 1.

\subsubsection{Study 3: Hyperparameter search algorithm}

This study aims to identify the hyperparameter search algorithm which leads to the best model performance. We tune the combination of hyperparameters considered to be the most optimal in Study 1 and optimise the metric considered optimal in Study 2.

\subsection{Data-generating mechanism}

\subsubsection{Population}

Different datasets will be generated for each of the three studies. A full factorial simulation design will be used to consider the influence of data characteristics on tuning procedures. The varying factors will be the number of candidate predictors $p$, the event fraction $EF$, and the sample size $N$. The levels of these three factors are detailed in Table 1. A total of 27 (3*3*3) scenarios will be considered. 1,000 datasets will be generated for each scenario, yielding a total of 27,000 datasets per study.

\begin{table}
    \label{tab:scenarios}
    \centering{
    \caption{Data generating scenarios.}
    \begin{tabular}{|c|c|}
         \hline \rowcolor{orange!50}
         Characteristics & Levels\\
         \hline
         Number of candidate predictors & 8, 16, 32\\
         Event fraction & 0.1, 0.3, 0.5\\
         Sample size & 0.5$n$, $n$, 2$n$\\
         \hline
    \end{tabular}}
    \begin{tablenotes}
      \small
      \item $n$ refers to the minimum sample size required to identify effects for a given number of regression coefficients (here, 1.25$p$) and expected event fraction \cite{riley_calculating_2020} with an AUC of 0.8. This is obtained using the R package \texttt{pmsampsize}.
    \end{tablenotes}
\end{table}

Development and validation data will be simulated under a logistic model with strong interactions. For each observation $i$ ($i$ = 1, ..., $N$), predictors $\mathbf{x}_i$ will be drawn from a $p$-variate normal distribution with parameters detailed in Formula \ref{predictor_dist}.\begin{equation}
    \mathbf{x}_i \sim \text{MVN}(\mathbf{0}, \left[ {\begin{array}{ccc}
    1 & 0.2 & ... \\
    0.2 & 1 & ... \\
    ... & ... & ...\\
  \end{array} } \right])
  \label{predictor_dist}
\end{equation}
Additionally, 0.25$p$ two-way interactions will be computed, with the $j^{th}$ interaction being the product of the $j^{th}$ and the $(j+p/2)^{th}$ predictors.
Then, the binary outcome $y_i$ will be drawn from a Bernoulli distribution conditional on $\mathbf{x}_i$, computed interactions, and the regression coefficients for main and interaction effects of the data generating model, hereafter called "true effect" (Formula \ref{binomial}).\begin{equation}
    P(y_i = 1) = \frac{exp(\beta_0 + \beta*\sum_{j=1}^px_{ij} + \gamma*\sum_{j=1}^{0.25*p}x_{ij}*x_{i(j+0.5p)})}{1 + exp(\beta_0 + \beta*\sum_{j=1}^px_{ij} + \gamma*\sum_{j=1}^{0.25*p}x_{ij}*x_{i(j+0.5p)})}
    \label{binomial}
\end{equation}

A validation dataset (N = 10,000) will be generated for each event fraction and number of candidate predictors combination in order to evaluate model performances. In the most extreme scenario ($EF = 0.1, p = 32$), this yields $\frac{10,000*0.1}{1.25p} = 25$ events per variable, which is well above the 10:1 events per variable rule of thumb.

\subsubsection{True effect estimation}

True effects will be constant across studies. They will be determined as follows: for each combination $k$ of number of candidate predictor and event fraction, the intercept $\beta_0^{(k)}$, predictor main effects $\boldsymbol\beta^{(k)}$, and predictor interaction effects $\boldsymbol\gamma^{(k)}$ will be estimated using a large sample ($N$ = 100,000) approximation. All main effects ($\boldsymbol\beta^{(k)}$) and interaction effects ($\boldsymbol\gamma^{(k)}$) will be set to be equal (i.e., $\beta_1^{(k)} = \beta_2^{(k)} = ... = \beta_p^{(k)}$ and $\gamma_1^{(k)} = \gamma_2^{(k)} = ... = \gamma_{0.25p}^{(k)}$). The estimation will use the R function \texttt{optim}, focused on minimising a loss function measuring the sum of i) the absolute difference between the targeted AUC and the observed AUC in the simulated dataset, and ii) the absolute difference between the targeted event fraction and the average estimated probability $P(y_i = 1|\mathbf{x}_i)$ in the simulated dataset. This estimation will be done in three steps:\begin{enumerate}
    \item Optimise $\beta_0^{(k)}$ and $\boldsymbol\beta^{(k)}$ for a target AUC of 0.7.
    \item Using the optimised $\beta_0^{(k)}$ and $\boldsymbol\beta^{(k)}$ from step 1, optimise $\boldsymbol\gamma^{(k)}$ for a target AUC of 0.8, such that a model ignoring interactions would have an AUC of 0.7 while including the correct interactions would lead to an AUC of 0.8. $\boldsymbol\gamma^{(k)}$ will be constrained to be positive.
    \item Using the optimised $\boldsymbol\beta^{(k)}$ and $\boldsymbol\gamma^{(k)}$, optimise $\beta_0^{(k)}$ for a target AUC of 0.8 to ensure the interactions do not alter the event fraction.
\end{enumerate}
This will be repeated 20 times and the mean of the parameters will be taken to obtain more stable estimates.

Results from this numerical procedure for $\beta_0^{(k)}$, $\boldsymbol\beta^{(k)}$ and $\boldsymbol\gamma^{(k)}$ will be checked using an independently generated dataset of N = 1,000,000. It will be checked whether:\begin{enumerate}
    \item The observed event fraction is at a distance of at most 0.01 from the target event fraction.
    \item The AUC of a model ignoring the interaction terms is at a distance of at most 0.025 from 0.7.
    \item The AUC of a model including the interaction terms is at a distance of at most 0.05 from 0.8.
    \item The estimated coefficients when fitting a logistic regression model are at a distance of at most 0.05 from the coefficients used to generate the dataset.
\end{enumerate}

\subsection{Estimands}

All studies focus on predictive performance for dichotomous outcome models. We also evaluate the computational time for each tuning procedure.

\subsection{Methods}

\subsubsection{Study 1: Hyperparameters to tune}

We will vary which hyperparameters are tuned when fitting a random forest using the R package \texttt{ranger} via the R package \texttt{caret}. We will use grid search (as is the standard in this package) to optimise classification accuracy at a probability threshold of 0.5 (as is the default in \texttt{caret}). 5-fold cross-validation will be used as part of the tuning procedure.

\cite{probst_tunability_2019} found \texttt{mtry} (the number of predictors randomly sampled to make a split) and \texttt{sample.fraction} (the proportion of the data that is used to fit a single tree) to be the pair of predictors with the highest influence on accuracy. We would therefore suggest to always tune these two hyperparameters. However, \cite{scornet_tuning_2017} demonstrates that \texttt{sample.fraction} has a similar effect to and \texttt{min.node.size} (that is, the minimum number of observations for a node to be formed, i.e., the point at which a split should not be computed regardless of impurity). As \texttt{caret} allows tuning for \texttt{min.node.size} but not for \texttt{sample.fraction} in its settings, we opt to always tune \texttt{mtry} and \texttt{min.node.size} to increase to user-friendliness of our possible findings.

All combinations of the following hyperparameters will be tuned in conjunction with \texttt{mtry} and \texttt{min.node.size}:
\begin{itemize}
    \item \texttt{num.trees}, that is, the number of trees the random forest fits and therefore averages over.
    \item \texttt{replace}, that is, whether the data used to fit a single tree is sampled with or without replacement.
    \item \texttt{sample.fraction}, that is, the proportion of the data that is used to fit a single tree.
    \item \texttt{splitrule}, that is, the way in which a split is picked.
\end{itemize}

Default values and tuning ranges are presented in Table \ref{tab:hyp_ranges}.

\begin{table}[]
    \centering
    \caption{Hyperparameter tuning ranges}
    \begin{tabular}{|c|c|c|}
         \hline \rowcolor{orange!50}
         Hyperparameter & Default & Range\\
         \hline
         \texttt{mtry} & $\sqrt{p}$ (rounded down) & 1-$p$\\
         \texttt{min.node.size} & 1 & 1-10\\ % STILL TO DETERMINE
         \hline
         \texttt{num.trees} & 500 & 100, 200, ..., 1000\\
         \texttt{replace} & TRUE & TRUE, FALSE\\
         \texttt{sample.fraction} & 1 & 0.1, 0.2, ..., 0.9, 1\\
         \texttt{splitrule} & gini & gini, hellinger, extratrees\\
         \hline
    \end{tabular}
    \begin{tablenotes}
      \small
      \item For \texttt{splitrule} = extratrees, an additional parameter should be considered regarding the number of random splits to consider. This will be set to its default of 1.
    \end{tablenotes}
    \label{tab:hyp_ranges}
\end{table}

This leads to 16 ($\sum_{h=0}^4\binom{4}{h}$) different combinations. The number of predictors considered at each split and the sample fraction will be included in all combinations. Hyperparameters not included in a given combination will be set to their default value. In addition, a random forest will be fit using the default hyperparameters to establish the baseline. All considered combinations will be used to fit a random forest on each simulated dataset, leading to 459,000 (17*27,000) tuning procedures being performed.

\subsubsection{Study 2: Optimisation metric}

We will vary the metric to optimise when fitting a random forest using the R package \texttt{ranger} via the R package \texttt{caret}. We will use grid search (as is standard in this package) to tune the hyperparameters considered optimal in Study 1. 5-fold cross-validation will be used as part of the tuning procedure. The following candidate metrics will be considered:\begin{itemize}
    \item Classification accuracy, which measures the proportion of correctly classified observations.
    \item Cohen's Kappa, which measures the proportion of correctly classified observations while accounting for chance.
    \item Brier score, which measures the difference between the predicted probability and the true outcome. This can be decomposed into a calibration component and a refinement component, which is related to the AUC. As such, the Brier score can be seen as a composite measire of calibration and discrimination \cite{luijken_impact_2019}.
    \item Logarithmic loss, which measures the difference between the predicted probability and the true outcome while penalising overconfident misclassifications.
    \item AUC, which measures how well classes can be differentiated.
    \item Calibration intercept (if possible to implement), which measures the distance between the average predicted risk and the event rate.
    \item Calibration slope (if possible to implement), which measures the extent of over- or underestimation.
\end{itemize}

Target values for each of these metrics are detailed in Table \ref{tab:metrics}.

\begin{table}[]
    \caption{Optimisation metric targets}
    \centering
    \begin{tabular}{|c|c|c|}
        \hline \rowcolor{orange!50}
        Metric & Target & Range of possible values \\
        \hline
        Accuracy & 1 & [0, 1]\\
        Kappa & 1 & [0, 1]\\
        Brier score & 0 & [0, 1]\\
        Logarithmic loss & 0 & [0, 1]\\
        AUC & 1 & [0.5, 1]\\
        Calibration intercept & 0 & [$-\infty$, $\infty$]\\
        Calibration slope & 1 & [0, $\infty$]\\
        \hline
    \end{tabular}
    \label{tab:metrics}
\end{table}

That is, each dataset will be tuned 7 times, leading to 189,000 tuning procedures.

\subsubsection{Study 3: Hyperparameter search algorithm}

We will vary the hyperparameter search algorithm when fitting a random forest. We will tune the hyperparameters considered most optimal in Study 1 and optimise the metric considered most optimal in Study 2. 5-fold cross-validation will be used as part of the tuning procedure. The following candidate hyperparameter search algorithms will be considered:\begin{itemize}
    \item Model-free search algorithms:\begin{itemize}
        \item Grid search using the R package \texttt{caret},
        \item Random search using the R package \texttt{caret},
    \end{itemize}
    \item Bayesian optimisation: SMAC using the R package \texttt{tuneRanger},
    \item Multifidelity: Hyperband using the R package \texttt{mlr3hyperband},
    \item Metaheuristic: genetic algorithm using the R package \texttt{GA}.
\end{itemize}

That is, each dataset will be tuned 5 times, leading to 132,000 tuning procedures.

\subsection{Performance measures}

For each tuning procedure performed, primary outcomes will be:
\begin{itemize}
    \item Discrimination (AUC),
    \item Calibration slope (calculated using \cite{benvancalster_benvancalsterclassimb_calibration_2022}),
    \item Root mean square of the log of the calibration slope over all the runs of a scenario (RMSD(slope)), as used in \cite{van_calster_regression_2020},
    \item Computational time.
\end{itemize}
Secondary outcomes will be:
\begin{itemize}
    \item Calibration intercept,
    \item Brier score,
    \item Logarithmic loss,
    \item Classification accuracy with a threshold of 0.5,
    \item Kappa.
\end{itemize}

Model performance metrics will be estimated using the predictions of the model on an independently generated validation set generated under the same data generating mechanisms. For each data simulation scenario and tuning procedure combination, we will compute the average and spread of each of these performance measures, leading to a table of the form of Table \ref{tab:study1}, Table \ref{tab:study2}, and Table \ref{tab:study3} for Studies 1, 2, and 3, respectively.

\begin{table}
    \centering
    \caption{Study 1 outcome table.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{*{8}{|M}|}
         \hline \rowcolor{orange!50}
         \multicolumn{3}{|c|}{Data simulation settings} & Hyperparameters tuned & \multicolumn{3}{|c|}{Performance metrics} & Time\\
         \hline \rowcolor{yellow!50}
         $p$ & Event fraction & Sample size & & AUC & Calibration slope & RMSD(slope) &\\
         \hline
         8 & 0.1 & 0.5$N$ & none & Mean (Variance) & Median (IQR) & NA & Mean (Variance)\\
         16 & 0.1 & 0.5$N$ & none & Mean (Variance) & Median (IQR) & NA & Mean (Variance)\\
         ... & ... & ... & none & ... & ... & ... & ...\\
         \hline
         8 & 0.1 & 0.5$N$ & mtry + min.node.size & Mean (Variance) & Median (IQR) & NA & Mean (Variance)\\
         16 & 0.1 & 0.5$N$ & mtry + min.node.size & Mean (Variance) & Median (IQR) & NA & Mean (Variance)\\
         ... & ... & ... & mtry + min.node.size & ... & ... & ... & ...\\
         \hline
         ... & ... & ... & ... & ... & ... & ...& ...\\
         \hline
    \end{tabular}}
    \begin{tablenotes}
      \small
      \item The final table will have 459 rows.
    \end{tablenotes}
    \label{tab:study1}
\end{table}

\begin{table}
    \centering
    \caption{Study 2 outcome table.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{*{9}{|M}|}
         \hline \rowcolor{orange!50}
         \multicolumn{3}{|c|}{Data simulation settings} & Optimisation metric & \multicolumn{3}{|c|}{Performance metrics} & Time\\
         \hline \rowcolor{yellow!50}
         $p$ & Event fraction & Sample size & & AUC & Calibration slope & RMSD(slope) &\\
         \hline
         8 & 0.1 & 0.5$N$ & Accuracy & Mean (Variance) & Median (IQR) & NA & Mean (Variance)\\
         16 & 0.1 & 0.5$N$ & Accuracy & Mean (Variance) & Median (IQR) & NA & Mean (Variance)\\
         ... & ... & ... & Accuracy & ... & ... & ... & ...\\
         \hline
         8 & 0.1 & 0.5$N$ & Kappa & Mean (Variance) & Median (IQR) & NA & Mean (Variance)\\
         16 & 0.1 & 0.5$N$ & Kappa & Mean (Variance) & Median (IQR) & NA & Mean (Variance)\\
         ... & ... & ... & Kappa & ... & ... & ... & ...\\
         \hline
         ... & ... & ... & ... & ... & ... & ... & ...\\
         \hline
    \end{tabular}}
    \begin{tablenotes}
      \small
      \item The final table will have 189 rows.
    \end{tablenotes}
    \label{tab:study2}
\end{table}

\begin{table}
    \centering
    \caption{Study 3 outcome table.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{*{9}{|M}|}
         \hline \rowcolor{orange!50}
         \multicolumn{3}{|c|}{Data simulation settings} & Hyperparameters search algorithm & \multicolumn{3}{|c|}{Performance metrics} & Time\\
         \hline \rowcolor{yellow!50}
         $p$ & Event fraction & Sample size & & AUC & Calibration slope & RMSD(slope) &\\
         \hline
         8 & 0.1 & 0.5$N$ & Grid search & Mean (Variance) & Median (IQR) & NA & Mean (Variance)\\
         16 & 0.1 & 0.5$N$ & Grid search & Mean (Variance) & Median (IQR) & NA & Mean (Variance)\\
         ... & ... & ... & Grid search & ... & ... & ... & ...\\
         \hline
         8 & 0.1 & 0.5$N$ & Random search & Mean (Variance) & Median (IQR) & NA & Mean (Variance)\\
         16 & 0.1 & 0.5$N$ & Random search & Mean (Variance) & Median (IQR) & NA & Mean (Variance)\\
         ... & ... & ... & Random search & ... & ... & ... & ...\\
         \hline
         ... & ... & ... & ... & ... & ... & ... & ...\\
         \hline
    \end{tabular}}
    \begin{tablenotes}
      \small
      \item The final table will have 135 rows.
    \end{tablenotes}
    \label{tab:study3}
\end{table}

We will evaluate and compare performance between hyperparameter combinations, optimisation metrics, and hyperparameter seach algorithms. This will be done using visualisations (e.g., scatterplots with time on the x-axis and performance metrics on the y-axis) and average performances and their spread. We aim to visually assess whether certain hyperparameter combinations, optimisation metrics, or hyperparameter seach algorithms have a notably larger runtime compared to others, for a relatively low increase in performance. The best hyperparameter combination, optimisation metric, and hyperparameter seach algorithm for model performance will be selected in Studies 1, 2, and 3, respectively. Selection will be done considering all primary outcomes.

\section{Error handling}

\subsection{Degenerate outcome distributions}

The number of datasets with zero events or non-events per simulation scenario will be reported. These datasets will not be used further. If this occurs for a validation dataset, a new validation dataset will be generated to replace it.

\subsection{Non-converging calibration slopes}

The number of non-converging calibration slopes per data simulation scenario and factor being varied (i.e., hyperparameter combination in study 1, optimisation metric in study 2, hyperparameter search algorithm in study 3) will be reported. Non-converging calibration slopes will be imputed as the highest calibration slope for the given setting, as this would typically occur for severely underfit models.

\newpage
\printbibliography

\end{document}
