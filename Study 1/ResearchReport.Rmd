---
output:
  bookdown::pdf_book:
    citation_package: biblatex
    toc: false
bibliography: ["Thesis.bib"]
header-includes:
  \usepackage[backend=biber,style=nature,sorting=none]{biblatex}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(MASS)
library(pROC)
library(tidyverse)
library(viridis)
library(xtable)
library(kableExtra)
library(bookdown)
```

```{=tex}
\begin{titlepage}

\begin{center}
\Large Methodology and Statistics for the Behavioural, Biomedical and Social Sciences\\
\vspace{0.15\textwidth}
\textbf{\LARGE RESEARCH REPORT} 

\vspace{0.05\textwidth}

\rule{\textwidth}{1pt}\\[0.8cm]

\textbf { \LARGE Tuning strategies for random forest hyperparameters in medical applications}
\\ [0.5cm]

\rule{\textwidth}{1pt}

\vspace{0.1\textwidth}
\LARGE{\textbf{Judith N\`eve}\\
0070661}
\end{center}
\begin{Large}
\begin{center}
    \vspace{0.3\textwidth}
    \textbf{Supervisors}\\
    Dr. Maarten van Smeden\\
    Zo\"e Dunias\\
\end{center}
\end{Large}

\vspace{0.1\textwidth}
\begin{large}
\textbf{Word count}: 2500/2500
\end{large}
\end{titlepage}
```

# Introduction

Machine learning models are increasingly used in medicine for diagnostic and prognostic purposes [@wessler_tufts_2017], for instance assisting patients' medical decision-making or risk stratification in clinical trials. A popular set of techniques in clinical research is tree-based methods [@mitchell_machine_1997], among which random forests are particularly prevalent [@uddin_comparing_2019]. Random forests' results are influenced by hyperparameters (e.g. the number of predictors sampled to make a split) [@mantovani_empirical_2019], which need to be set before training a model. Previous studies have shown classification accuracy (i.e., the proportion of observations correctly classified) to be similar when using hyperparameter software defaults compared to the optimal hyperparameters for a given dataset [@bernard_influence_2009]$^,$[@probst_hyperparameters_2019]. Classification accuracy is however not sufficient to evaluate a model's clinical utility: identifying a patient as positive or negative may not carry sufficient information, as the patient's predicted risk is a key element of medical decision-making. Moreover, classification accuracy depends on the chosen classification threshold, i.e., the predicted risk value above which an observation is classified as positive. For clinical purposes, discrimination (i.e., positive patients will have higher risk predictions than negative patients) and calibration (i.e., the extent to which risk predictions reflect patients' true risk) [@van_calster_calibration_2019] are two much more important performance metrics. Models with low discrimination may misclassify patients, while miscalibrated models can lead to over- or undertreatment if over- or underestimating patients' risks. Random forests have been found to suffer from miscalibration [@benedettoCanMachineLearning2020]$^,$[@djulbegovicDiagnosticPredictiveModel2019]. We therefore aim to improve calibration performance in random forests.

A well-known way to improve model performance is to perform hyperparameter tuning, which identifies hyperparameter values for which a performance metric (e.g. classification accuracy) is optimal for a given prediction model. This procedure requires choosing which metric is optimised, which hyperparameters are tuned, and how candidate hyperparameter values are generated. The latter two can greatly impact computational time: generally, the more hyperparameters are tuned, the larger computational time will be. Hyperparameter search algorithms may consider all possible hyperparameter values and their combinations, or use information collected earlier in the tuning procedure to tune efficiently. Computational time is important to consider in conjunction to the improvement in model performance: greater computational times and longer training periods of models may lead to a strain on resources and increase carbon emissions. For complex models, this can be as much as five times the average emissions of a car over its lifetime [@strubell_energy_2019].

There has been little focus on comparing tuning procedures on clinically relevant performance metrics. Previous research suggests the most gain in random forest performance as measured by classification accuracy, discrimination, and the Brier score could be made by tuning the number of predictors sampled to make a split and the proportion of the sample used to fit the tree [@probst_tunability_2019]. Yang and Shami [@yang_hyperparameter_2020] found tuning using most hyperparameter search algorithms improved classification accuracy compared to default hyperparameters. Some search algorithms had greatly increased computational times without improving classification accuracy further than shorter search algorithms. So far, no study has compared different optimisation metrics' effects on model performance or computational time. Moreover, existing studies focusing on tuning procedures for random forests use high-dimensional datasets (datasets with more features than observations) whereas clinical research typically is low-dimensional (datasets have more observations than features), for which guidance regarding hyperparameter tuning is lacking [@ellenbach_improved_2021].

The current project addresses the following question: what are the optimal hyperparameter tuning strategies for balancing predictive performance and computational time of tree-based methods in low-dimensional settings? There will be three simulation studies in this project to identify (i) which hyperparameters to tune, (ii) which metric to optimise, and (iii) which search algorithm to use for optimal model performance. Methods and preliminary results from the first study are presented in this report.

# Methods

This method section follows the ADEMP approach [@morris_using_2019].

## Aim

Using datasets from the OpenML platform [@bischlOpenMLBenchmarkingSuites2021], Probst and colleagues [@probst_tunability_2019] found the number of predictors considered at a split and the sample fraction to be the two most influential hyperparameters on the discriminative performance of prediction models. These findings only investigate the effect of tuning one or two hyperparameters at once. This study aimed to extend these findings by considering (i) model calibration in addition to model discrimination and (ii) more combinations of hyperparameters. We examined the effect of tuning different combinations of hyperparameters on the performance of a prediction model and evaluate performance improvements in context of required computational times.

## Data-generating mechanism

### Data-generating scenarios

The full-scale study will use a full factorial simulation design to consider the influence of data characteristics on model predictive performance and computational time. The varying simulation factors will be the number of candidate predictors $p$ (range: 8, 16), the event fraction $EF$ (range: 0.1, 0.3, 0.5), and the sample size $N$ (range: $0.5n, n$). $n$ is the minimum sample size required to identify effects for a given number of regression coefficients (here, 1.25$p$), expected event fraction (here, $EF$), and expected AUC (here, 0.8), as calculated by Riley and colleagues [@riley_calculating_2020]. A total of 12 (2\*3\*2) scenarios will be considered. For each scenario, 1,000 training datasets will be generated, yielding a total of 12,000 datasets. Preliminary results presented in this report were found using 10 iterations of the $p = 8, EF = 0.5, N = n$ scenario.

Training and validation datasets were simulated under a logistic model with strong interactions. For each observation $i$ ($i$ = 1, ..., $n$), predictors $\mathbf{x}_i$ were drawn from an octovariate normal distribution with parameters detailed in Formula \ref{predictor_dist}. Two two-way interactions were included, with the $h^{th}$ ($h = 1, 2$) interaction being the product of the $h^{th}$ and the $(h+4)^{th}$ predictors. The binary outcome $y_i$ was drawn from a Bernoulli distribution conditional on the predictor values (Formula \ref{binomial}).

```{=tex}
\begin{equation}
    \mathbf{x}_i \sim \text{MVN}(\mathbf{0}, \left[ {\begin{array}{ccc}
    1 & 0.2 & ... \\
    0.2 & 1 & ... \\
    ... & ... & ...\\
  \end{array} } \right]).
  \label{predictor_dist}
\end{equation}
```

```{=tex}
\begin{equation}
    P(y_i = 1|\mathbf{x}_i) = \frac{exp(\beta_0 + \sum_{j=1}^p\beta x_{ij} + \sum_{h=1}^{0.25*p}\gamma x_{ih}x_{i(h+0.5p)})}{1 + exp(\beta_0 + \sum_{j=1}^p\beta x_{ij} + \sum_{h=1}^{0.25*p}\gamma x_{ih}x_{i(h+0.5p)})},
    \label{binomial}
\end{equation}
```

where $\beta_0$, $\beta$, $\gamma$ are respectively the intercept, main effect regression coefficient, and interaction effect regression coefficient of the data generating model, hereafter called "true effects".

A validation dataset ($N = 10,000$) was generated for each training dataset in order to evaluate model performances. In the studied scenario, this yielded $\frac{10,000*0.5}{1.25*8} = 500$ expected events per variable.

### True effect estimation

True effects were determined for each unique combination of $p$ and $EF$. Let $k$ denote a given combination. For each combination, the intercept $\beta_0^{(k)}$, predictor main effects $\boldsymbol\beta^{(k)}$, and predictor interaction effects $\boldsymbol\gamma^{(k)}$ were estimated using a large sample ($N = 100,000$) approximation. All main effects ($\boldsymbol\beta^{(k)}$) and interaction effects ($\boldsymbol\gamma^{(k)}$) were set to be equal (i.e., $\beta_1^{(k)} = \beta_2^{(k)} = ... = \beta_p^{(k)}$ and $\gamma_1^{(k)} = \gamma_2^{(k)} = ... = \gamma_{0.25p}^{(k)}$). The estimation used the R function `optim`, focused on minimising a loss function measuring the sum of (i) the absolute difference between the targeted AUC and the observed AUC in the simulated dataset, and (ii) the absolute difference between the targeted event fraction and the average estimated probability $P(y_i = 1|\mathbf{x}_i)$ in the simulated dataset. This estimation was done in three steps:

1. Optimise $\beta_0^{(k)}$ and $\boldsymbol\beta^{(k)}$ for a target AUC of 0.7.
2. Using the optimised $\beta_0^{(k)}$ and $\boldsymbol\beta^{(k)}$ from step 1, optimise $\boldsymbol\gamma^{(k)}$ for a target AUC of 0.8, such that a model ignoring interactions would have an AUC of 0.7 while including the correct interactions would lead to an AUC of 0.8. $\boldsymbol\gamma^{(k)}$ will be constrained to be positive.
3. Using the optimised $\boldsymbol\beta^{(k)}$ and $\boldsymbol\gamma^{(k)}$, optimise $\beta_0^{(k)}$ for a target AUC of 0.8 to ensure the interactions do not alter the event fraction.

```{r}
load("Data/betas_v2.RData")
betas_pilot <- betas_matrix[[1]][3,] %>% round(2)
```

This was repeated 20 times. The mean of the parameters was taken to obtain more stable estimates. True effects for the scenario presented in this report were $\beta_0$ = `r betas_pilot["intercept"]`, $\beta$ = `r betas_pilot["beta"]`, $\gamma$ = `r betas_pilot["gamma"]`.

```{r, results='hide'}
source("GenerateBetas.R")
start_seed <- 100
example_n <- 1e5
p  <- c(8, 16, 32)
EF <- c(0.1, 0.3, 0.5)
set.seed(2 * start_seed)
validation_betas_matrix <- validate_betas(
 betas_matrix = betas_matrix[[1]], example_n = example_n, n_predictors = p, prevalences = EF
)
set.seed(3 * start_seed)
validation_betas_matrix_noint <- validate_betas_noint(
 betas_matrix = betas_matrix[[1]], example_n = example_n, n_predictors = p, prevalences = EF
)
```

```{r}
source("DataSimFunctions.R")
source("tuning_functions.R")
load("Data/scenarios.RData")
scenarios <- scenarios %>% filter(n_pred == 8, event_fraction == 0.5, prop_sample_size == 1)
set.seed(4*start_seed)
beta_validation <- sim_data(scenarios = scenarios %>% mutate(n = example_n),
                            coefs = betas_matrix[[1]],
                            nsim = 1) %>%
  validation_prep("1_1")
validation_mod <- glm(Y ~ . + X1:X5 + X2:X6,
                      family = binomial(),
                      data = beta_validation)
max_coef_dev <- max(abs(round(coef(validation_mod), 2) - c(betas_pilot["intercept"], rep(betas_pilot["beta"], 8), rep(betas_pilot["gamma"], 2))))
```

A dataset with $N = 100,000$ was generated for each set of coefficients obtained through this numerical procedure. This dataset was used to check:

1. The observed event fraction was at a distance of at most 0.01 from the target event fraction.
2. The AUC of a model ignoring the interaction terms was at a distance of at most 0.025 from 0.7.
3. The AUC of a model including the interaction terms was at a distance of at most 0.05 from 0.8.
4. The estimated coefficients when fitting a logistic regression model were at a distance of at most 0.05 from the coefficients used to generate the dataset.

For the scenario presented in this report, results of these checks were as follows:

1. The observed event fraction was at a distance of `r abs(round(validation_betas_matrix[3,"validation prevalence"] - 0.5, 4))` from 0.5.
2. The AUC of a model ignoring the interaction terms was at a distance of `r abs(round(validation_betas_matrix_noint[3,"validation AUC"] - 0.7, 2))` from 0.7.
3. The AUC of a model including the interaction terms was at a distance of `r abs(round(validation_betas_matrix[3,"validation AUC"] - 0.8, 2))` from 0.8.
4. The estimated coefficients when fitting a logistic regression model were at a distance of at most `r max_coef_dev` from the coefficients used to generate the dataset.

## Methods

We varied the hyperparameters that were tuned when fitting a random forest model using the R package `ranger` [@ranger] via the R package `caret` [@caret]. We used grid search to optimise classification accuracy at a probability threshold of 0.5, which is the software default. 5-fold cross-validation was used as part of the tuning procedure.

Considering all possible combinations of 5 hyperparameters leads to 32 tuning procedures on each dataset. Due to the unfeasibility of such a large-scale approach, we reduced the number of combinations studied: only hyperparameter combinations including at least `mtry` (the number of predictors randomly sampled to make a split) and `min.node.size` (the minimum number of observations for a node to be formed, i.e., the point at which a split should not be computed regardless of impurity) were considered. Other hyperparameters which could be included in combinations were `replace` (whether the data used to fit a single tree is sampled with or without replacement), `sample.fraction` (the proportion of the data used to fit a single tree), and `splitrule` (the way in which a split is picked). Default values and tuning ranges are presented in Table \ref{tab:hyp_ranges}.

```{=tex}
\begin{table}[tb]
    \centering
    \caption{Hyperparameter tuning ranges}
    \begin{tabular}{ccc}
         \hline
         Hyperparameter & Default & Range\\
         \hline
         \texttt{mtry} & $\sqrt{p}$ (rounded down) & 1-$p$\\
         \texttt{min.node.size} & 1 & 1-10\\ % STILL TO DETERMINE
         \hline
         \texttt{replace} & TRUE & TRUE, FALSE\\
         \texttt{sample.fraction} & 1 & 0.1, 0.2, ..., 0.9, 1\\
         \texttt{splitrule} & gini & gini, hellinger, extratrees\\
         \hline
    \end{tabular}
    \begin{tablenotes}
      \small
      \item For \texttt{splitrule} = extratrees, an additional parameter should be considered regarding the number of random splits to consider. This was set to its default of 1.
    \end{tablenotes}
    \label{tab:hyp_ranges}
\end{table}
```

This yielded 8 hyperparameter combinations. Hyperparameters not included in a given combination were set to their default value. A model using the default hyperparameters was fit to establish the baseline. All considered combinations were used to fit a random forest on each training dataset, leading to 90 tuning procedures being performed for these preliminary results.

## Performance measures

Primary outcomes were used to assess which hyperparameter combinations may be advisable to tune. These outcomes were discrimination, calibration slope, root mean square of the log of the calibration slope over all the runs of a scenario (RMSD(slope)), and computational time. Discrimination was measured by the AUC and assessed whether positive observations have higher predicted risk than negative observation. Calibration was measured by the calibration slope (as calculated by Van Calster [@benvancalster_benvancalsterclassimb_calibration_2022]) and assessed the extent to which predicted risks reflect true risks. RMSD(slope) assessed the extent to which the calibration slopes varied within a data-generating mechanism and how much they differed from the ideal value of 1, as used by Van Calster and colleagues [@van_calster_regression_2020]. Computational time was the amount of time for which a given tuning procedure ran, measured in seconds.

Secondary outcomes were included to enable comparisons with later studies of this project. These secondary outcomes will be used as candidate optimisation metrics. They are the classification accuracy (the proportion of correctly classified observations; it was the optimisation metric used in this study) the calibration intercept (the extent to which the mean predicted risk reflects the true prevalence), the Brier score (which has a calibration and discrimination component), the logarithmic loss (how close the predicted risk comes to the observed outcome), and Cohen's Kappa (the proportion of correctly classified observations, accounting for chance).

Model performance metrics were estimated using the predictions of the model on an independently generated validation set generated under the same data generating mechanisms. For each data simulation scenario and tuning procedure combination, we computed the average and spread of each of these performance measures.

We evaluated and compared model predictive performance between hyperparameter combinations. We visualized whether certain hyperparameter combinations had a notably larger runtime compared to others, for a relatively low increase in performance. The best hyperparameter combination was assessed considering all primary outcomes.

# Results

```{r}
load("Data/pilot2.RData")
rm(list=ls()[!ls() %in% c("dat", "validation_sets", "all_perf", "all_tuning_time", "validation_betas_matrix_noint", "validation_betas_matrix")])
```

```{r}
# clean data
all_perf <- all_perf %>%
  mutate(`Tuned hyperparameters` =
           ifelse(`Tuned hyperparameters` == "",
                "None",
                `Tuned hyperparameters`),
         Runtime = Runtime*60)
```

```{r}
df_scale_setter <- tibble(
  Accuracy = rep(c(0, 1), each = 9),
  AUC = rep(c(0.5, 1), each = 9),
  BrierScore = rep(c(0, 1), each = 9),
  CalibrationIntercept = rep(c(-1, 1), each = 9),
  CalibrationSlope = rep(c(0, 2), each = 9),
  CohensKappa = rep(c(0, 1), each = 9),
  LogarithmicLoss = rep(c(0, 1), each = 9),
  Runtime = 0,
  `Tuned hyperparameters` = rep(unique(all_perf$`Tuned hyperparameters`), 2)
) %>%
  pivot_longer(Accuracy:LogarithmicLoss,
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric, levels = c("Accuracy", "AUC", "CalibrationSlope", "BrierScore", "CalibrationIntercept", "LogarithmicLoss", "CohensKappa")))

df_targets <- tibble(
  Accuracy = 1,
  AUC = 1,
  BrierScore = 0,
  CalibrationIntercept = 0,
  CalibrationSlope = 1,
  CohensKappa = 1,
  LogarithmicLoss = 0,
  Runtime = 0,
  `Tuned hyperparameters` = unique(all_perf$`Tuned hyperparameters`)
) %>%
  pivot_longer(Accuracy:LogarithmicLoss,
               names_to = "Metric",
               values_to = "Target") %>%
  mutate(Metric = factor(Metric, levels = c("Accuracy", "AUC", "CalibrationSlope", "BrierScore", "CalibrationIntercept", "LogarithmicLoss", "CohensKappa")))

Metric.labs <- c("Calibration slope", "Calibration intercept", "Brier score", "Logarithmic loss", "Cohen's kappa", "AUC", "Classification accuracy")
names(Metric.labs) <- c("CalibrationSlope", "CalibrationIntercept", "BrierScore", "LogarithmicLoss", "CohensKappa", "AUC", "Accuracy")

HP.labs <- unique(all_perf$`Tuned hyperparameters`)
names(HP.labs) <- HP.labs
names(HP.labs)[2] <- "mtry + sample.fraction + replace\n+ min.node.size + splitrule"
# names(HP.labs)[4] <- "mtry + sample.fraction + min.node.size\n+ splitrule"
# names(HP.labs)[6] <- "mtry + sample.fraction + replace\n+ min.node.size"
HP.labs <- c(HP.labs[1], sort(HP.labs[2:9]))

HPcomb_pal <- viridis::turbo(n = 9)
```

```{r}
summary_table_study1 <- all_perf %>%
  group_by(`Tuned hyperparameters`) %>% 
  summarise(AUC_mean = round(mean(AUC), 2),
            AUC_sd = round(sd(AUC), 2),
            Calslope_median = round(median(CalibrationSlope), 2),
            Calslope_IQR = round(IQR(CalibrationSlope), 2),
            #Accuracy = paste0(round(mean(Accuracy),2), " (", round(sd(Accuracy), 2), ")"),
            `RMSD(slope)` = round(sqrt(mean((log(CalibrationSlope))^2)), 2),
            Runtime_mean = round(mean(Runtime), 1),
            Runtime_sd = round(sd(Runtime), 1))
```

Preliminary results using 10 simulated datasets for the scenario where $p = 8, EF = 0.5, N = n$ are presented here. The average and spread of primary outcomes for each hyperparameter combination are presented in Table \@ref(tab:summary-table).

```{r, results='asis'}
tab <- summary_table_study1 %>% 
  arrange(Runtime_mean, desc = FALSE) %>% 
  mutate(AUC = paste0(AUC_mean, " (", AUC_sd, ")"),
         `Calibration slope` = paste0(Calslope_median, " (", Calslope_IQR, ")"),
         `Runtime (seconds)` = paste0(
           format(Runtime_mean, nsmall = 1),
           " (",
           str_pad(format(Runtime_sd, nsmall = 1, trim = TRUE), 4, pad = "0"),
           ")"
           )) %>% 
  select(`Tuned hyperparameters`, AUC, `Calibration slope`, `RMSD(slope)`, `Runtime (seconds)`)

addtorow <- list()
addtorow$pos <- list(0)
addtorow$command <- paste0(
  paste0("\\multicolumn{1}{c}{", colnames(tab)[1], "}"),
  paste0("& \\multicolumn{1}{c}{", colnames(tab)[2:ncol(tab)], "}", collapse = ""),
  '\\\\',
  "\\multicolumn{1}{c}{}",
  paste0("& \\multicolumn{1}{c}{", c("Mean (SD)", "Median (IQR)", "", "Mean(SD)"), "}", collapse = ""),
  '\\\\'
)

tab %>%
  # mutate(`Tuned hyperparameters` = gsub(" . ", "\n+ ", `Tuned hyperparameters`)) %>% 
  xtable(caption = "Average performance of hyperparameter combinations",
         label = "tab:summary-table",
         align = c(rep("l", 2), rep("c", 3), "r")) %>% 
  print.xtable(include.rownames = FALSE,
               caption.placement = "top",
               table.placement = "b",
               comment = FALSE,
               hline.after = c(-1, 0, 9),
               #size="\\fontsize{8pt}{8pt}\\selectfont",
               scalebox = "0.8",
               add.to.row = addtorow,
               include.colnames = F)
```

These results suggest varying hyperparameter tuning combinations had very limited effects on discrimination, with a mean AUC of `r round(mean(all_perf$AUC),2)` (SD = `r round(sd(all_perf$AUC),2)`) across all hyperparameter combinations. A large effect was observed on model calibration and computational time. Model calibration varied considerably both within and between combinations (Table \@ref(tab:summary-table), Figure \@ref(fig:study1fig)). The best calibration performance was obtained by tuning `mtry`, `replace`, `min.node.size`, and `splitrule`, which had a median calibration slope of `r summary_table_study1$'Calslope_median'[which.min(abs(summary_table_study1$'Calslope_median' - 1))]`. This performance was highly variable, such that the best RMSD(slope) was obtained when tuning no hyperparameters (RMSD(slope) = `r min(summary_table_study1$'RMSD(slope)')`).

```{r study1fig, fig.cap="Performance of each hyperparameter tuning combination", fig.height=8, fig.width=6}
all_perf %>%
  pivot_longer(AUC:CohensKappa,
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric, levels = c("AUC", "CalibrationSlope", "BrierScore", "CalibrationIntercept", "Accuracy", "LogarithmicLoss", "CohensKappa"))) %>% 
  ggplot(aes(x = Runtime,
             y = Performance,
             group = `Tuned hyperparameters`)) +
  geom_point(aes(col = `Tuned hyperparameters`,
                 shape = `Tuned hyperparameters`)) +
  xlim(0, max(all_perf$Runtime)) +
  geom_point(data = df_scale_setter, alpha = 0) +
  geom_hline(data = df_targets, aes(yintercept = Target), col = "red", lty = "dotted") +
  facet_wrap(~ Metric,
             scales = "free_y",
             labeller = labeller(Metric = Metric.labs),
             # nrow = 2) +
             ncol = 2) +
  theme_classic() +
  # theme(legend.position = c(.88, .2),
  theme(legend.position = c(.75, .1),
        legend.text = element_text(size = 8),
        legend.key.size = unit(0.4, 'cm'),
        legend.background = element_rect(),
        panel.spacing = unit(1.2, "lines"),
        plot.caption = element_text(hjust = 0)) +
  scale_color_manual(breaks = HP.labs,
                     labels = names(HP.labs),
                     values = HPcomb_pal) +
  scale_shape_manual(breaks = HP.labs,
                     labels = names(HP.labs),
                     values = c(1, rep(16, 8))) +
  labs(caption = "Red dotted lines show ideal performance.",
       x = "Runtime (seconds)")
```

Runtime did not vary much between different runs of a given hyperparameter combination (Table \@ref(tab:summary-table), Figure \@ref(fig:study1fig)). Large increases in runtime were observed when tuning 4 or more hyperparameters, with no consistent benefit to model performance as measured by primary and secondary outcome performance measures (Figure \@ref(fig:study1fig)).

Secondary outcomes showed similar patterns (Figure \@ref(fig:study1fig). Classification accuracy, Brier score, and Cohen's Kappa showed very little variation within and between hyperparameter combinations. Calibration intercept and logarithmic loss showed variations within, but not between, hyperparameter combinations.

# Discussion

In this report, we studied the impact of the choice of hyperparameters being tuned on model performance and computational time. This was a first step in our aim to identify optimal hyperparameter tuning strategies for balancing predictive performance and computational time of tree-based methods for the development of clinical prediction models in low-dimensional settings. We presented preliminary results computed using one data simulation scenario and ten simulation runs. Tuning led to increases in runtime with large but heterogeneous effects on model calibration. Other performance measures showed little-to-no effects. Computational time was highly influenced by hyperparameter combinations: in this simple scenario, this ranged from 0.2 seconds to over 40 minutes. These preliminary results suggest tuning number of predictors, replacement, minimum node size, and splitting rule leads on average to the best model performance. This hyperparameter combination had large amounts of variations, so tuning no hyperparameter may more reliably lead to the best performances as it performed adequately with less variation in calibration performance. These results are yet to be expanded by considering more simulation runs and data generating scenarios.

These results differ from previous research: Probst and colleagues [@probst_tunability_2019] found tuning the number of predictors considered at a split and the proportion of the data used to build a model to have an impact on model discrimination, whereas tuning these two hyperparameters did not lead to any notable differences in discrimination compared to not performing any tuning in our study. This may be a product of chance due to the small scale of our simulation and this effect being relatively small in random forests compared to other algorithms. Alternatively, this could be due to the data generating mechanism: as all datasets were generated following the same mechanism, it is possible this mechanism aligned well with hyperparameter defaults, as tuning is not beneficial for all data [@probst_hyperparameters_2019]. This can be explored further by considering datasets generated using more predictors, predictors with varying effects, smaller event fractions, or smaller sample sizes. This will be addressed in the full-scale run of this study.

It is also possible optimal hyperparameter values were not identifiable: while grid search is most commonly used, it suffers from some severe limitations. Candidate values for each hyperparameter are determined by the user, leading to possibly missing the optimal value in low-dimensional settings [@van_calster_regression_2020]$^,$[@vansmedenSampleSizeBinary2019] and to computational time increasing exponentially the more hyperparameters are tuned or candidate values are considered. As classification accuracy is optimised, this may lead to missing optimal values for other performance metrics. This highlights possible large systematic problems in random forest tuning procedures, which are heavily focused on obtaining good classification accuracy in high-dimensional settings. It is of utmost importance to identify ways in which these tuning procedures can be adapted for low-dimensional, clinical settings, for which performance is better measured using discrimination and calibration.

This project will aim to have value by providing guidelines regarding possible trade-offs between computationally intensive tuning procedures and model performances. In the continuation of this project, this study will be repeated on a larger scale, and follow-up studies will be conducted to investigate the optimisation metric and hyperparameter candidate value generation algorithms. While this project has so far focused on random forests, other tree-based methods may be explored in future.


\newpage

# References
