---
title: "Tuning strategies for random forest hyperparameters in medical applications"
author: "Judith Neve"
date: '2022-01-10'
output:
  bookdown::pdf_book:
    citation_package: biblatex
bibliography: ["Thesis.bib"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
library(viridis)
library(xtable)
library(kableExtra)
library(bookdown)
```

# Introduction

Machine learning models are increasingly used in medicine for diagnostic and prognostic purposes [@wessler_tufts_2017] assisting patients' medical decision-making. For this purpose, high *classification accuracy* (i.e., the proportion of observations correctly classified) and *discrimination* (i.e., the ability to differentiate between classes) are insufficient: models also need to be well *calibrated* (i.e., risk predictions reflect patients' true risk) [@van_calster_calibration_2019]. Miscalibrated models can lead to over- or undertreatment if over- or underestimating patients' risks.

A popular set of techniques is tree-based methods [@mitchell_machine_1997], among which random forests are particularly prevalent [@uddin_comparing_2019]. Random forests' results are influenced by hyperparameters (e.g. the minimum number of observations in a node) [@mantovani_empirical_2019], which need to be set before fitting the model. While hyperparameter software defaults sometimes lead to appropriate accuracy [@bernard_influence_2009], they can lead to severe miscalibration (work by Dunias, publication not yet available). It is then of utmost importance to identify hyperparameter tuning strategies improving calibration.

Tuning identifies hyperparameter values for which a metric (e.g. accuracy) is optimal for a given model. Multiple elements need to be chosen before the tuning procedure can be carried out: the metric to optimise, the hyperparameters to tune, and the hyperparameter search algorithm. The latter two can greatly impact computation time: generally, the more hyperparameters are tuned, the larger computation time will be. Hyperparameter search algorithms may consider all possible values and combinations, or use information collected earlier in the tuning procedure to tune efficiently. Computation time is important to consider in conjunction to the improvement in model performance: greater computation times and longer training periods of models may lead to a strain on resources and increase carbon emissions. For complex models, this can be as much as five times the average emissions of a car over its lifetime [@strubell_energy_2019].

Previous research suggests the most gain in random forest performance can be made by tuning the following hyperparameters: number of predictors sampled to make a split, proportion of the sample used to fit the tree, and minimum node size [@probst_tunability_2019]. Most hyperparameter search algorithms improve accuracy compared to default hyperparameters, with various effects on computational time (Figure \@ref(fig:yang-fig)) [@yang_hyperparameter_2020].

```{r}
RFclass <- data.frame("OptimisationAlgorithm" = c("Default", "Grid search", "Random search", "BO-GP", "BO-TPE", "Hyperband", "BOHB", "GA", "PSO"),
                      "Category" = c("Default", "Model-free", "Model-free", "Bayesian optimisation", "Bayesian optimisation", "Multifidelity", "Multifidelity", "Metaheuristic", "Metaheuristic"),
                 "Accuracy" = c(90.65, 93.32, 93.38, 93.38, 93.88, 93.38, 90.38, 93.83, 93.73),
                 "CompTime" = c(0.09, 48.62, 16.73, 20.60, 12.58, 8.89, 8.45, 19.19, 12.43)) %>% 
  mutate(Category = factor(Category, levels = c("Default", "Model-free", "Bayesian optimisation", "Multifidelity", "Metaheuristic")))

algo_pal <- viridis::plasma(n = 5)
```


```{r yang-fig, fig.cap="Accuracy VS computational time for different hyperparameter tuning algorithms"}
RFclass %>%
  ggplot(aes(y = Accuracy, x = CompTime, col = Category)) +
  geom_point(size = 3) +
  labs(x = "Computational time (in seconds)",
       y = "Accuracy (%)") +
  theme_minimal() +
  labs(col = "Algorithm category",
       caption = "Adapted from Yang & Shami (2020)") +
  theme(plot.caption = element_text(hjust = 1.65)) +
  scale_color_manual(breaks = levels(RFclass$Category),,
                     values = algo_pal)
```

While there has been some focus on comparing hyperparameter search algorithms, so far, no study has compared different optimisation metrics' effects on model performance or computational time. Additionally, hyperparameter tuning has only been studied with accuracy as the outcome, not calibration or discrimination, which are typically more important in clinical settings.

Larger computational time does not necessarily lead to an improvement in accuracy. It is unknown whether there is a point at which continuing a search algorithm no longer leads to improvements in model calibration and discrimination. The current project will thus address the following question: what are the optimal hyperparameter tuning strategies for balancing predictive performance and computational time of tree-based methods in low-dimensional settings? There will be three simulation studies in this project to identify which hyperparameters to tune, which metric to optimise, and which search algorithm to use for optimal model performance. Methods and preliminary results from the first study are presented in this report.

# Methods

Methods follow the ADEMP approach [@morris_using_2019].

## Aim

Prior findings [@probst_tunability_2019] have shown the number of predictors considered at a split and the sample fraction to be the two most influential hyperparameters on model accuracy. However, these findings only investigate the effect of tuning one or two hyperparameters at once. This study aims to extend these findings by considering more combinations of hyperparameters in order to identify the combination of hyperparameters for which tuning leads to the best predictive performance of a prediction model.

## Data-generating mechanism

### Population

Different datasets will be generated for each of the three studies. A full factorial simulation design will be used to consider the influence of data characteristics on tuning procedures. The varying factors will be the number of candidate predictors $p$, the event fraction $EF$, and the sample size $N$. The levels of these three factors are detailed in Table 1. A total of 27 (3*3*3) scenarios will be considered. 1,000 datasets will be generated for each scenario, yielding a total of 27,000 datasets per study.

\begin{table}
    \label{tab:scenarios}
    \centering{
    \caption{Data generating scenarios.}
    \begin{tabular}{|c|c|}
         \hline \rowcolor{orange!50}
         Characteristics & Levels\\
         \hline
         Number of candidate predictors & 8, 16, 32\\
         Event fraction & 0.1, 0.3, 0.5\\
         Sample size & 0.5$n$, $n$, 2$n$\\
         \hline
    \end{tabular}}
    \begin{tablenotes}
      \small
      \item $n$ refers to the minimum sample size required to identify effects for a given number of regression coefficients (here, 1.25$p$) and expected event fraction \cite{riley_calculating_2020} with an AUC of 0.8. This is obtained using the R package \texttt{pmsampsize}.
    \end{tablenotes}
\end{table}

Development and validation data will be simulated under a logistic model with strong interactions. For each observation $i$ ($i$ = 1, ..., $N$), predictors $\mathbf{x}_i$ will be drawn from a $p$-variate normal distribution with parameters detailed in Formula \ref{predictor_dist}.\begin{equation}
    \mathbf{x}_i \sim \text{MVN}(\mathbf{0}, \left[ {\begin{array}{ccc}
    1 & 0.2 & ... \\
    0.2 & 1 & ... \\
    ... & ... & ...\\
  \end{array} } \right])
  \label{predictor_dist}
\end{equation}
Additionally, 0.25$p$ two-way interactions will be computed, with the $j^{th}$ interaction being the product of the $j^{th}$ and the $(j+p/2)^{th}$ predictors.
Then, the binary outcome $y_i$ will be drawn from a Bernoulli distribution conditional on $\mathbf{x}_i$, computed interactions, and the regression coefficients for main and interaction effects of the data generating model, hereafter called "true effect" (Formula \ref{binomial}).\begin{equation}
    P(y_i = 1) = \frac{exp(\beta_0 + \beta*\sum_{j=1}^px_{ij} + \gamma*\sum_{j=1}^{0.25*p}x_{ij}*x_{i(j+0.5p)})}{1 + exp(\beta_0 + \beta*\sum_{j=1}^px_{ij} + \gamma*\sum_{j=1}^{0.25*p}x_{ij}*x_{i(j+0.5p)})}
    \label{binomial}
\end{equation}

A validation dataset (N = 10,000) will be generated for each event fraction and number of candidate predictors combination in order to evaluate model performances. In the most extreme scenario ($EF = 0.1, p = 32$), this yields $\frac{10,000*0.1}{1.25p} = 25$ events per variable, which is well above the 10:1 events per variable rule of thumb.

### True effect estimation

True effects will be constant across studies. They will be determined as follows: for each combination $k$ of number of candidate predictor and event fraction, the intercept $\beta_0^{(k)}$, predictor main effects $\boldsymbol\beta^{(k)}$, and predictor interaction effects $\boldsymbol\gamma^{(k)}$ will be estimated using a large sample ($N$ = 100,000) approximation. All main effects ($\boldsymbol\beta^{(k)}$) and interaction effects ($\boldsymbol\gamma^{(k)}$) will be set to be equal (i.e., $\beta_1^{(k)} = \beta_2^{(k)} = ... = \beta_p^{(k)}$ and $\gamma_1^{(k)} = \gamma_2^{(k)} = ... = \gamma_{0.25p}^{(k)}$). The estimation will use the R function \texttt{optim}, focused on minimising a loss function measuring the sum of i) the absolute difference between the targeted AUC and the observed AUC in the simulated dataset, and ii) the absolute difference between the targeted event fraction and the average estimated probability $P(y_i = 1|\mathbf{x}_i)$ in the simulated dataset. This estimation will be done in three steps:\begin{enumerate}
    \item Optimise $\beta_0^{(k)}$ and $\boldsymbol\beta^{(k)}$ for a target AUC of 0.7.
    \item Using the optimised $\beta_0^{(k)}$ and $\boldsymbol\beta^{(k)}$ from step 1, optimise $\boldsymbol\gamma^{(k)}$ for a target AUC of 0.8, such that a model ignoring interactions would have an AUC of 0.7 while including the correct interactions would lead to an AUC of 0.8. $\boldsymbol\gamma^{(k)}$ will be constrained to be positive.
    \item Using the optimised $\boldsymbol\beta^{(k)}$ and $\boldsymbol\gamma^{(k)}$, optimise $\beta_0^{(k)}$ for a target AUC of 0.8 to ensure the interactions do not alter the event fraction.
\end{enumerate}
This will be repeated 20 times and the mean of the parameters will be taken to obtain more stable estimates.

Results from this numerical procedure for $\beta_0^{(k)}$, $\boldsymbol\beta^{(k)}$ and $\boldsymbol\gamma^{(k)}$ will be checked using an independently generated dataset of N = 1,000,000. It will be checked whether:\begin{enumerate}
    \item The observed event fraction is at a distance of at most 0.01 from the target event fraction.
    \item The AUC of a model ignoring the interaction terms is at a distance of at most 0.025 from 0.7.
    \item The AUC of a model including the interaction terms is at a distance of at most 0.05 from 0.8.
    \item The estimated coefficients when fitting a logistic regression model are at a distance of at most 0.05 from the coefficients used to generate the dataset.
\end{enumerate}

## Estimands

This study focuses on predictive performance for dichotomous outcome models. We also evaluate the computational time for each tuning procedure.

## Methods

We will vary which hyperparameters are tuned when fitting a random forest using the R package \texttt{ranger} via the R package \texttt{caret}. We will use grid search (as is the standard in this package) to optimise classification accuracy at a probability threshold of 0.5 (as is the default in \texttt{caret}). 5-fold cross-validation will be used as part of the tuning procedure.

\cite{probst_tunability_2019} found \texttt{mtry} (the number of predictors randomly sampled to make a split) and \texttt{sample.fraction} (the proportion of the data that is used to fit a single tree) to be the pair of predictors with the highest influence on accuracy. We would therefore suggest to always tune these two hyperparameters. However, \cite{scornet_tuning_2017} demonstrates that \texttt{sample.fraction} has a similar effect to and \texttt{min.node.size} (that is, the minimum number of observations for a node to be formed, i.e., the point at which a split should not be computed regardless of impurity). As \texttt{caret} allows tuning for \texttt{min.node.size} but not for \texttt{sample.fraction} in its settings, we opt to always tune \texttt{mtry} and \texttt{min.node.size} to increase to user-friendliness of our possible findings.

All combinations of the following hyperparameters will be tuned in conjunction with \texttt{mtry} and \texttt{min.node.size}:
\begin{itemize}
    \item \texttt{replace}, that is, whether the data used to fit a single tree is sampled with or without replacement.
    \item \texttt{sample.fraction}, that is, the proportion of the data that is used to fit a single tree.
    \item \texttt{splitrule}, that is, the way in which a split is picked.
\end{itemize}

Default values and tuning ranges are presented in Table \ref{tab:hyp_ranges}.

\begin{table}[]
    \centering
    \caption{Hyperparameter tuning ranges}
    \begin{tabular}{|c|c|c|}
         \hline \rowcolor{orange!50}
         Hyperparameter & Default & Range\\
         \hline
         \texttt{mtry} & $\sqrt{p}$ (rounded down) & 1-$p$\\
         \texttt{min.node.size} & 1 & 1-10\\ % STILL TO DETERMINE
         \hline
         \texttt{replace} & TRUE & TRUE, FALSE\\
         \texttt{sample.fraction} & 1 & 0.1, 0.2, ..., 0.9, 1\\
         \texttt{splitrule} & gini & gini, hellinger, extratrees\\
         \hline
    \end{tabular}
    \begin{tablenotes}
      \small
      \item For \texttt{splitrule} = extratrees, an additional parameter should be considered regarding the number of random splits to consider. This will be set to its default of 1.
    \end{tablenotes}
    \label{tab:hyp_ranges}
\end{table}

This leads to 16 ($\sum_{h=0}^4\binom{4}{h}$) different combinations. The number of predictors considered at each split and the sample fraction will be included in all combinations. Hyperparameters not included in a given combination will be set to their default value. In addition, a random forest will be fit using the default hyperparameters to establish the baseline. All considered combinations will be used to fit a random forest on each simulated dataset, leading to 459,000 (17*27,000) tuning procedures being performed.

## Performance measures

For each tuning procedure performed, primary outcomes will be:
\begin{itemize}
    \item Discrimination (AUC),
    \item Calibration slope (calculated using \cite{benvancalster_benvancalsterclassimb_calibration_2022}),
    \item Root mean square of the log of the calibration slope over all the runs of a scenario (RMSD(slope)), as used in \cite{van_calster_regression_2020},
    \item Computational time.
\end{itemize}
Secondary outcomes will be:
\begin{itemize}
    \item Calibration intercept,
    \item Brier score,
    \item Logarithmic loss,
    \item Classification accuracy with a threshold of 0.5,
    \item Kappa.
\end{itemize}

Model performance metrics will be estimated using the predictions of the model on an independently generated validation set generated under the same data generating mechanisms. For each data simulation scenario and tuning procedure combination, we will compute the average and spread of each of these performance measures.

We will evaluate and compare performance between hyperparameter combinations. We aim to visually assess whether certain hyperparameter combinations have a notably larger runtime compared to others, for a relatively low increase in performance. The best hyperparameter combination will be selected considering all primary outcomes.


# Results

```{r}
load("Data/pilot2.RData")
rm(list=ls()[!ls() %in% c("dat", "validation_sets", "all_perf", "all_tuning_time")])
```

```{r}
# clean data
all_perf <- all_perf %>%
  mutate(`Tuned hyperparameters` =
           ifelse(`Tuned hyperparameters` == "",
                "None",
                `Tuned hyperparameters`))
```


## Summary table

Table \@ref(tab:summary-table)

```{r summary-table}
summary_table <- all_perf %>%
  group_by(n_pred, event_fraction, sample_size_prop, `Tuned hyperparameters`) %>%
  summarise(mean_AUC = mean(AUC),
            var_AUC = var(AUC),
            median_CalibrationSlope = median(CalibrationSlope),
            IQR_slope = IQR(CalibrationSlope),
            RMSD_slope = sqrt(mean((log(CalibrationSlope))^2)),
            mean_time = mean(Runtime),
            var_time = var(Runtime))
summary_table %>% 
  kbl(booktabs = T, caption = "Classification algorithms and imbalance corrections to be evaluated.") %>% 
  kable_styling(latex_options = c("striped", "hold_position"))
```


```{r}
summary_table_latex <- (all_perf %>%
  group_by(`Tuned hyperparameters`) %>% 
  summarise(AUC = paste0(round(mean(AUC), 2), " (", round(sd(AUC), 2), ")"),
            `Calibration slope` = paste0(round(median(CalibrationSlope), 2), " (", round(IQR(CalibrationSlope), 2), ")"),
            Accuracy = paste0(round(mean(Accuracy),2), " (", round(sd(Accuracy), 2), ")"),
            Runtime = paste0(round(mean(Runtime), 2), " (", round(sd(Runtime), 2), ")")))[c(9,2:8),] %>%
  xtable(caption = "Average performance of hyperparameter combinations")
```

```{r}
df_scale_setter <- tibble(
  Accuracy = rep(c(0, 1), each = 9),
  AUC = rep(c(0.5, 1), each = 9),
  BrierScore = rep(c(0, 1), each = 9),
  CalibrationIntercept = rep(c(-1, 1), each = 9),
  CalibrationSlope = rep(c(0, 2), each = 9),
  CohensKappa = rep(c(0, 1), each = 9),
  LogarithmicLoss = rep(c(0, 1), each = 9),
  Runtime = 0,
  `Tuned hyperparameters` = rep(unique(all_perf$`Tuned hyperparameters`), 2)
) %>%
  pivot_longer(Accuracy:LogarithmicLoss,
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric, levels = c("Accuracy", "AUC", "CalibrationSlope", "BrierScore", "CalibrationIntercept", "LogarithmicLoss", "CohensKappa")))

df_targets <- tibble(
  Accuracy = 1,
  AUC = 1,
  BrierScore = 0,
  CalibrationIntercept = 0,
  CalibrationSlope = 1,
  CohensKappa = 1,
  LogarithmicLoss = 0,
  Runtime = 0,
  `Tuned hyperparameters` = unique(all_perf$`Tuned hyperparameters`)
) %>%
  pivot_longer(Accuracy:LogarithmicLoss,
               names_to = "Metric",
               values_to = "Target") %>%
  mutate(Metric = factor(Metric, levels = c("Accuracy", "AUC", "CalibrationSlope", "BrierScore", "CalibrationIntercept", "LogarithmicLoss", "CohensKappa")))

Metric.labs <- c("Calibration slope", "Calibration intercept", "Brier score", "Logarithmic loss", "Cohen's kappa", "AUC", "Accuracy")
names(Metric.labs) <- c("CalibrationSlope", "CalibrationIntercept", "BrierScore", "LogarithmicLoss", "CohensKappa", "AUC", "Accuracy")

HP.labs <- unique(all_perf$`Tuned hyperparameters`)
names(HP.labs) <- HP.labs
# names(HP.labs)[2] <- "mtry + sample.fraction + replace\n+ min.node.size + splitrule"
# names(HP.labs)[4] <- "mtry + sample.fraction + min.node.size\n+ splitrule"
# names(HP.labs)[6] <- "mtry + sample.fraction + replace\n+ min.node.size"
HP.labs <- c(HP.labs[1], sort(HP.labs[2:9]))

HPcomb_pal <- viridis::turbo(n = 9)
```


```{r}
all_perf %>%
  pivot_longer(AUC:CohensKappa,
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric, levels = c("Accuracy", "AUC", "CalibrationSlope", "BrierScore", "CalibrationIntercept", "LogarithmicLoss", "CohensKappa"))) %>% 
  ggplot(aes(x = Runtime,
             y = Performance,
             group = `Tuned hyperparameters`)) +
  geom_point(aes(col = `Tuned hyperparameters`,
                 shape = `Tuned hyperparameters`)) +
  xlim(0, max(all_perf$Runtime)) +
  geom_point(data = df_scale_setter, alpha = 0) +
  geom_hline(data = df_targets, aes(yintercept = Target), col = "red", lty = "dotted") +
  facet_wrap(~ Metric,
             scales = "free_y",
             labeller = labeller(Metric = Metric.labs),
             # nrow = 2) +
             ncol = 3) +
  theme_minimal() +
  # theme(legend.position = c(.88, .2),
  theme(legend.position = c(.85, .13),
        legend.text = element_text(size = 8),
        legend.key.size = unit(0.4, 'cm'),
        axis.title = element_text(size = 12),
        legend.background = element_rect(),
        panel.spacing = unit(1.2, "lines"),
        strip.text = element_text(colour = "black",
                                  size = 11)) +
  scale_color_manual(breaks = HP.labs,
                     labels = names(HP.labs),
                     values = HPcomb_pal) +
  scale_shape_manual(breaks = HP.labs,
                     labels = names(HP.labs),
                     values = c(1, rep(16, 8)))

#ggsave("plot_grouped2.jpg", width = 30, height = 15, units = "cm")
```

```{r}
all_perf %>%
  pivot_longer(c(Accuracy, AUC, CalibrationSlope),
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric, levels = c("AUC", "CalibrationSlope", "Accuracy"))) %>% 
  ggplot(aes(x = Runtime,
             y = Performance,
             group = `Tuned hyperparameters`)) +
  geom_point(aes(col = `Tuned hyperparameters`,
                 shape = `Tuned hyperparameters`),
             size = 2) +
  xlim(0, max(all_perf$Runtime)) +
  geom_point(data = df_scale_setter %>% filter(Metric %in% c("Accuracy", "AUC", "CalibrationSlope")), alpha = 0) +
  geom_hline(data = df_targets %>% filter(Metric %in% c("Accuracy", "AUC", "CalibrationSlope")), aes(yintercept = Target), col = "red", lty = "dotted") +
  facet_wrap(~ Metric,
             scales = "free_y",
             labeller = labeller(Metric = Metric.labs),
             nrow = 2) +
  theme_minimal() +
  theme(legend.position = c(.8, .22),
  # theme(legend.position = c(.85, .13),
        legend.text = element_text(size = 10),
        legend.key.size = unit(0.6, 'cm'),
        axis.title = element_text(size = 12),
        legend.background = element_rect(),
        panel.spacing = unit(1.2, "lines"),
        strip.text = element_text(colour = "black",
                                  size = 11)) +
  scale_color_manual(breaks = HP.labs,
                     labels = names(HP.labs),
                     values = HPcomb_pal) +
  scale_shape_manual(breaks = HP.labs,
                     labels = names(HP.labs),
                     values = c(1, rep(16, 8)))

#ggsave("plot_grouped_top3.jpg", width = 30, height = 15, units = "cm")
```

```{r}
all_perf %>%
  pivot_longer(AUC:CohensKappa,
               names_to = "Metric",
               values_to = "Performance") %>%
  ggplot(aes(x = Runtime,
             y = Performance)) +
  geom_point() +
  xlim(0, max(all_perf$Runtime)) +
  geom_point(data = df_scale_setter, alpha = 0) +
  geom_hline(data = df_targets, aes(yintercept = Target), col = "red", lty = "dotted") +
  facet_wrap(Metric ~ `Tuned hyperparameters`, ncol = 9, scales = "free_y") +
  theme_minimal() #+
  # geom_point(data = summary_table %>%
  #              dplyr::select(mean_AUC, median_CalibrationSlope, mean_time, `Tuned hyperparameters`) %>%
  #              mutate(AUC = mean_AUC,
  #                     CalibrationSlope = median_CalibrationSlope,
  #                     Runtime = mean_time) %>%
  #              pivot_longer(AUC:CalibrationSlope,
  #                           names_to = "Metric",
  #                           values_to = "Performance"),
  #            size = 3)
#ggsave("plot_facetted.jpg", width = 90, height = 40, units = "cm")
```


```{r}
summary_table %>% 
  pivot_longer(c(mean_AUC, median_CalibrationSlope),
               names_to = "Metric",
               values_to = "Performance") %>% 
  pivot_longer(c(var_AUC, IQR_slope),
               names_to = "XXX",
               values_to = "MC error")
```

# Discussion

