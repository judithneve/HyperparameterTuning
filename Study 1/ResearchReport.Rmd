---
title: "Tuning strategies for random forest hyperparameters in medical applications"
author: "Judith Neve"
date: '2022-01-10'
output:
  bookdown::pdf_book:
    citation_package: biblatex
bibliography: ["Thesis.bib"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
library(viridis)
library(xtable)
library(kableExtra)
library(bookdown)
```

# Introduction

Machine learning models are increasingly used in medicine for diagnostic and prognostic purposes [@wessler_tufts_2017] assisting patients' medical decision-making. For this purpose, high *classification accuracy* (i.e., the proportion of observations correctly classified) and *discrimination* (i.e., the ability to differentiate between classes) are insufficient: models also need to be well *calibrated* (i.e., risk predictions reflect patients' true risk) [@van_calster_calibration_2019]. Miscalibrated models can lead to over- or undertreatment if over- or underestimating patients' risks.

A popular set of techniques is tree-based methods [@mitchell_machine_1997], among which random forests are particularly prevalent [@uddin_comparing_2019]. Random forests' results are influenced by hyperparameters (e.g. the minimum number of observations in a node) [@mantovani_empirical_2019], which need to be set before fitting the model. While hyperparameter software defaults sometimes lead to appropriate accuracy [@bernard_influence_2009], they can lead to severe miscalibration (work by Dunias, publication not yet available). It is then of utmost importance to identify hyperparameter tuning strategies improving calibration.

Tuning identifies hyperparameter values for which a metric (e.g. accuracy) is optimal for a given model. Multiple elements need to be chosen before the tuning procedure can be carried out: the metric to optimise, the hyperparameters to tune, and the hyperparameter search algorithm. The latter two can greatly impact computation time: generally, the more hyperparameters are tuned, the larger computation time will be. Hyperparameter search algorithms may consider all possible values and combinations, or use information collected earlier in the tuning procedure to tune efficiently. Computation time is important to consider in conjunction to the improvement in model performance: greater computation times and longer training periods of models may lead to a strain on resources and increase carbon emissions. For complex models, this can be as much as five times the average emissions of a car over its lifetime [@strubell_energy_2019].

Previous research suggests the most gain in random forest performance can be made by tuning the following hyperparameters: number of predictors sampled to make a split, proportion of the sample used to fit the tree, and minimum node size [@probst_tunability_2019]. Most hyperparameter search algorithms improve accuracy compared to default hyperparameters, with various effects on computational time (Figure \@ref(fig:yang-fig)) [@yang_hyperparameter_2020].

```{r}
RFclass <- data.frame("OptimisationAlgorithm" = c("Default", "Grid search", "Random search", "BO-GP", "BO-TPE", "Hyperband", "BOHB", "GA", "PSO"),
                      "Category" = c("Default", "Model-free", "Model-free", "Bayesian optimisation", "Bayesian optimisation", "Multifidelity", "Multifidelity", "Metaheuristic", "Metaheuristic"),
                 "Accuracy" = c(90.65, 93.32, 93.38, 93.38, 93.88, 93.38, 90.38, 93.83, 93.73),
                 "CompTime" = c(0.09, 48.62, 16.73, 20.60, 12.58, 8.89, 8.45, 19.19, 12.43)) %>% 
  mutate(Category = factor(Category, levels = c("Default", "Model-free", "Bayesian optimisation", "Multifidelity", "Metaheuristic")))

algo_pal <- viridis::plasma(n = 5)
```


```{r yang-fig, fig.cap="Accuracy VS computational time for different hyperparameter tuning algorithms"}
RFclass %>%
  ggplot(aes(y = Accuracy, x = CompTime, col = Category)) +
  geom_point(size = 3) +
  labs(x = "Computational time (in seconds)",
       y = "Accuracy (%)") +
  theme_minimal() +
  labs(col = "Algorithm category",
       caption = "Adapted from Yang & Shami (2020)") +
  theme(plot.caption = element_text(hjust = 1.65)) +
  scale_color_manual(breaks = levels(RFclass$Category),,
                     values = algo_pal)
```

While there has been some focus on comparing hyperparameter search algorithms, so far, no study has compared different optimisation metrics' effects on model performance or computational time. Additionally, hyperparameter tuning has only been studied with accuracy as the outcome, not calibration or discrimination, which are typically more important in clinical settings.

Larger computational time does not necessarily lead to an improvement in accuracy. It is unknown whether there is a point at which continuing a search algorithm no longer leads to improvements in model calibration and discrimination. The current project will thus address the following question: what are the optimal hyperparameter tuning strategies for balancing predictive performance and computational time of tree-based methods in low-dimensional settings? There will be three simulation studies in this project to identify which hyperparameters to tune, which metric to optimise, and which search algorithm to use for optimal model performance.

# Methods

# Results

```{r}
load("Data/pilot2.RData")
rm(list=ls()[!ls() %in% c("dat", "validation_sets", "all_perf", "all_tuning_time")])
```

```{r}
# clean data
all_perf <- all_perf %>%
  mutate(`Tuned hyperparameters` =
           ifelse(`Tuned hyperparameters` == "",
                "None",
                `Tuned hyperparameters`))
```


## Summary table

Table \@ref(tab:summary-table)

```{r summary-table}
summary_table <- all_perf %>%
  group_by(n_pred, event_fraction, sample_size_prop, `Tuned hyperparameters`) %>%
  summarise(mean_AUC = mean(AUC),
            var_AUC = var(AUC),
            median_CalibrationSlope = median(CalibrationSlope),
            IQR_slope = IQR(CalibrationSlope),
            RMSD_slope = sqrt(mean((log(CalibrationSlope))^2)),
            mean_time = mean(Runtime),
            var_time = var(Runtime))
summary_table %>% 
  kbl(booktabs = T, caption = "Classification algorithms and imbalance corrections to be evaluated.") %>% 
  kable_styling(latex_options = c("striped", "hold_position"))
```


```{r}
summary_table_latex <- (all_perf %>%
  group_by(`Tuned hyperparameters`) %>% 
  summarise(AUC = paste0(round(mean(AUC), 2), " (", round(sd(AUC), 2), ")"),
            `Calibration slope` = paste0(round(median(CalibrationSlope), 2), " (", round(IQR(CalibrationSlope), 2), ")"),
            Accuracy = paste0(round(mean(Accuracy),2), " (", round(sd(Accuracy), 2), ")"),
            Runtime = paste0(round(mean(Runtime), 2), " (", round(sd(Runtime), 2), ")")))[c(9,2:8),] %>%
  xtable(caption = "Average performance of hyperparameter combinations")
```

```{r}
df_scale_setter <- tibble(
  Accuracy = rep(c(0, 1), each = 9),
  AUC = rep(c(0.5, 1), each = 9),
  BrierScore = rep(c(0, 1), each = 9),
  CalibrationIntercept = rep(c(-1, 1), each = 9),
  CalibrationSlope = rep(c(0, 2), each = 9),
  CohensKappa = rep(c(0, 1), each = 9),
  LogarithmicLoss = rep(c(0, 1), each = 9),
  Runtime = 0,
  `Tuned hyperparameters` = rep(unique(all_perf$`Tuned hyperparameters`), 2)
) %>%
  pivot_longer(Accuracy:LogarithmicLoss,
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric, levels = c("Accuracy", "AUC", "CalibrationSlope", "BrierScore", "CalibrationIntercept", "LogarithmicLoss", "CohensKappa")))

df_targets <- tibble(
  Accuracy = 1,
  AUC = 1,
  BrierScore = 0,
  CalibrationIntercept = 0,
  CalibrationSlope = 1,
  CohensKappa = 1,
  LogarithmicLoss = 0,
  Runtime = 0,
  `Tuned hyperparameters` = unique(all_perf$`Tuned hyperparameters`)
) %>%
  pivot_longer(Accuracy:LogarithmicLoss,
               names_to = "Metric",
               values_to = "Target") %>%
  mutate(Metric = factor(Metric, levels = c("Accuracy", "AUC", "CalibrationSlope", "BrierScore", "CalibrationIntercept", "LogarithmicLoss", "CohensKappa")))

Metric.labs <- c("Calibration slope", "Calibration intercept", "Brier score", "Logarithmic loss", "Cohen's kappa", "AUC", "Accuracy")
names(Metric.labs) <- c("CalibrationSlope", "CalibrationIntercept", "BrierScore", "LogarithmicLoss", "CohensKappa", "AUC", "Accuracy")

HP.labs <- unique(all_perf$`Tuned hyperparameters`)
names(HP.labs) <- HP.labs
# names(HP.labs)[2] <- "mtry + sample.fraction + replace\n+ min.node.size + splitrule"
# names(HP.labs)[4] <- "mtry + sample.fraction + min.node.size\n+ splitrule"
# names(HP.labs)[6] <- "mtry + sample.fraction + replace\n+ min.node.size"
HP.labs <- c(HP.labs[1], sort(HP.labs[2:9]))

HPcomb_pal <- viridis::turbo(n = 9)
```


```{r}
all_perf %>%
  pivot_longer(AUC:CohensKappa,
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric, levels = c("Accuracy", "AUC", "CalibrationSlope", "BrierScore", "CalibrationIntercept", "LogarithmicLoss", "CohensKappa"))) %>% 
  ggplot(aes(x = Runtime,
             y = Performance,
             group = `Tuned hyperparameters`)) +
  geom_point(aes(col = `Tuned hyperparameters`,
                 shape = `Tuned hyperparameters`)) +
  xlim(0, max(all_perf$Runtime)) +
  geom_point(data = df_scale_setter, alpha = 0) +
  geom_hline(data = df_targets, aes(yintercept = Target), col = "red", lty = "dotted") +
  facet_wrap(~ Metric,
             scales = "free_y",
             labeller = labeller(Metric = Metric.labs),
             # nrow = 2) +
             ncol = 3) +
  theme_minimal() +
  # theme(legend.position = c(.88, .2),
  theme(legend.position = c(.85, .13),
        legend.text = element_text(size = 8),
        legend.key.size = unit(0.4, 'cm'),
        axis.title = element_text(size = 12),
        legend.background = element_rect(),
        panel.spacing = unit(1.2, "lines"),
        strip.text = element_text(colour = "black",
                                  size = 11)) +
  scale_color_manual(breaks = HP.labs,
                     labels = names(HP.labs),
                     values = HPcomb_pal) +
  scale_shape_manual(breaks = HP.labs,
                     labels = names(HP.labs),
                     values = c(1, rep(16, 8)))

#ggsave("plot_grouped2.jpg", width = 30, height = 15, units = "cm")
```

```{r}
all_perf %>%
  pivot_longer(c(Accuracy, AUC, CalibrationSlope),
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric, levels = c("AUC", "CalibrationSlope", "Accuracy"))) %>% 
  ggplot(aes(x = Runtime,
             y = Performance,
             group = `Tuned hyperparameters`)) +
  geom_point(aes(col = `Tuned hyperparameters`,
                 shape = `Tuned hyperparameters`),
             size = 2) +
  xlim(0, max(all_perf$Runtime)) +
  geom_point(data = df_scale_setter %>% filter(Metric %in% c("Accuracy", "AUC", "CalibrationSlope")), alpha = 0) +
  geom_hline(data = df_targets %>% filter(Metric %in% c("Accuracy", "AUC", "CalibrationSlope")), aes(yintercept = Target), col = "red", lty = "dotted") +
  facet_wrap(~ Metric,
             scales = "free_y",
             labeller = labeller(Metric = Metric.labs),
             nrow = 2) +
  theme_minimal() +
  theme(legend.position = c(.8, .22),
  # theme(legend.position = c(.85, .13),
        legend.text = element_text(size = 10),
        legend.key.size = unit(0.6, 'cm'),
        axis.title = element_text(size = 12),
        legend.background = element_rect(),
        panel.spacing = unit(1.2, "lines"),
        strip.text = element_text(colour = "black",
                                  size = 11)) +
  scale_color_manual(breaks = HP.labs,
                     labels = names(HP.labs),
                     values = HPcomb_pal) +
  scale_shape_manual(breaks = HP.labs,
                     labels = names(HP.labs),
                     values = c(1, rep(16, 8)))

#ggsave("plot_grouped_top3.jpg", width = 30, height = 15, units = "cm")
```

```{r}
all_perf %>%
  pivot_longer(AUC:CohensKappa,
               names_to = "Metric",
               values_to = "Performance") %>%
  ggplot(aes(x = Runtime,
             y = Performance)) +
  geom_point() +
  xlim(0, max(all_perf$Runtime)) +
  geom_point(data = df_scale_setter, alpha = 0) +
  geom_hline(data = df_targets, aes(yintercept = Target), col = "red", lty = "dotted") +
  facet_wrap(Metric ~ `Tuned hyperparameters`, ncol = 9, scales = "free_y") +
  theme_minimal() #+
  # geom_point(data = summary_table %>%
  #              dplyr::select(mean_AUC, median_CalibrationSlope, mean_time, `Tuned hyperparameters`) %>%
  #              mutate(AUC = mean_AUC,
  #                     CalibrationSlope = median_CalibrationSlope,
  #                     Runtime = mean_time) %>%
  #              pivot_longer(AUC:CalibrationSlope,
  #                           names_to = "Metric",
  #                           values_to = "Performance"),
  #            size = 3)
#ggsave("plot_facetted.jpg", width = 90, height = 40, units = "cm")
```


```{r}
summary_table %>% 
  pivot_longer(c(mean_AUC, median_CalibrationSlope),
               names_to = "Metric",
               values_to = "Performance") %>% 
  pivot_longer(c(var_AUC, IQR_slope),
               names_to = "XXX",
               values_to = "MC error")
```

# Discussion

