---
title: "**Tuning strategies for random forest hyperparameters in medical applications**"
author: "**Judith Neve**"
output:
  bookdown::pdf_book:
    citation_package: biblatex
    toc: false
bibliography: ["Thesis.bib"]
header-includes:
  \usepackage[backend=biber,style=numeric,sorting=none]{biblatex}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(MASS)
library(pROC)
library(tidyverse)
library(viridis)
library(xtable)
library(kableExtra)
library(bookdown)
```

Date: 10/01/2023\
\
**Supervisors**: Maarten van Smeden and Zoe Dunias (Utrecht Medical Center Utrecht)\
\
Word Count: XXXX\
\
What I don't want feedback on:

-	Look of the title page

What I want feedback on:

-	Overall look of the report (not detailed)
-	Do I use enough references -- are there points at which my argumentation would benefit from including a reference
- DGM methods: lots of focus (words) taken up -- is this necessary -- should I focus on only the scenario I'm talking about
- Talking about my results: they're preliminary -- should I be drawing conclusions as if they weren't (in terms of phrasing)?
- I struggled with the discussion (because I don't have a lot of results to discuss?). Is it too short? Is it too repetitive? Is the end too off-topic/dramatic?

Notes to self:

- make sure tables/figures only appear once they've been referenced in-text/don't appear between announcing a list and the list AND appear at the top or bottom of each page
- check whether caret uses defaults or tunes when not wanting to tune a given HP (verbose iterations)
- if need more words, take out coefficients checks? - could but these into a table or unite them into one list instead of 2 (will probably do this)
- fix first row of table 2 alignment -- also decimal places of "none"? -- also last row

- make sure intro is modest about what is known vs some results from previous studies
- past tense
- references (simplify)

\newpage

# Introduction

Machine learning models are increasingly used in medicine for diagnostic and prognostic purposes [@wessler_tufts_2017], for instance in assisting patients' medical decision-making or risk stratification in clinical trials. A popular set of techniques in clinical research is tree-based methods [@mitchell_machine_1997], among which random forests are particularly prevalent [@uddin_comparing_2019]. Random forests' results are influenced by hyperparameters (e.g. the number of predictors sampled to make a split) [@mantovani_empirical_2019], which need to be set before training the model. Previous studies have shown limited differences in classification accuracy (i.e., the proportion of observations correctly classified) when using hyperparameter software defaults compared to the optimal hyperparameters for a given dataset [@bernard_influence_2009] [@probst_hyperparameters_2019]. However, classification accuracy is not sufficient to evaluate a model's clinical utility: it is not enough to correctly identify a patient as positive or negative, as the predicted risk is a key element of medical decision-making. Moreover, classification accuracy is dependent on the chosen classification threshold, that is, the cut-off predicted risk value between an observation being classified as positive or negative. For clinical purposes, discrimination (i.e., positive patients will have higher risk predictions than negative patients) and calibration (i.e., the extent to which risk predictions reflect patients' true risk) [@van_calster_calibration_2019] are two much more important performance metrics. Models with low discrimination may misclassify patients, while miscalibrated models can lead to over- or undertreatment if over- or underestimating patients' risks. Random forests have however been found to suffer from miscalibration [@benedettoCanMachineLearning2020] [@djulbegovicDiagnosticPredictiveModel2019]. We therefore aim to investigate how to improve calibration performance in random forests.

A well-known way to improve model performance is to perform hyperparameter tuning, which identifies hyperparameter values for which a performance metric (e.g. classification accuracy) is optimal for a given prediction model. This procedure requires to choose which metric is optimised, which hyperparameters are tuned, and how candidate hyperparameter values are generated. The latter two can greatly impact computation time: generally, the more hyperparameters are tuned, the larger computation time will be. Hyperparameter search algorithms may consider all possible values and combinations, or use information collected earlier in the tuning procedure to tune efficiently. Computation time is important to consider in conjunction to the improvement in model performance: greater computation times and longer training periods of models may lead to a strain on resources and increase carbon emissions. For complex models, this can be as much as five times the average emissions of a car over its lifetime [@strubell_energy_2019].

There has been little focus on comparing tuning procedures on clinically relevant performance metrics. Previous research suggests the most gain in random forest performance as measured by classification accuracy, discrimination, and the Brier score can be made by tuning the number of predictors sampled to make a split and the proportion of the sample used to fit the tree [@probst_tunability_2019]. @yang_hyperparameter_2020 found that tuning using most hyperparameter search algorithms improved classification accuracy compared to default hyperparameters. Some search algorithms had greatly increased computational times, which showed no clear relationship with classification accuracy. So far, no study has compared different optimisation metrics' effects on model performance or computational time. Moreover, existing studies focusing on tuning procedures for random forests use high-dimensional datasets (i.e., datasets with more features than observations) whereas clinical research typically is low-dimensional (i.e., datasets have more observations than features), for which guidance regarding hyperparameter tuning is lacking [@ellenbach_improved_2021].

The current project thus addresses the following question: what are the optimal hyperparameter tuning strategies for balancing predictive performance and computational time of tree-based methods in low-dimensional settings? There will be three simulation studies in this project to identify (i) which hyperparameters to tune, (ii) which metric to optimise, and (iii) which search algorithm to use for optimal model performance. Methods and preliminary results from the first study are presented in this report.

# Methods

This method section follows the ADEMP approach [@morris_using_2019].

## Aim

Using datasets from the OpenML platform [@bischlOpenMLBenchmarkingSuites2021], @probst_tunability_2019 found the number of predictors considered at a split and the sample fraction to be the two most influential hyperparameters on the discriminative performance of prediction models. However, these findings only investigate the effect of tuning one or two hyperparameters at once. This study aims to extend these findings by considering (i) model calibration in addition to model discrimination and (ii) more combinations of hyperparameters in order to identify the combination of hyperparameters for which tuning leads to the best predictive performance of a prediction model. We evaluate improvements in predictive performance in context of required computational times.

## Data-generating mechanism

### Data-generating scenarios

In the full-scale study, a full factorial simulation design will be used to consider the influence of data characteristics on model predictive performance and computational time. The varying simulation factors will be (i) the number of candidate predictors $p$ (range: 8, 16), (ii) the event fraction $EF$ (range: 0.1, 0.3, 0.5), and (iii) the sample size $N$ (range: $0.5n, n$). $n$ is the minimum sample size required to identify effects for a given number of regression coefficients (here, 1.25$p$), expected event fraction, and expected AUC (here, 0.8), as calculated by @riley_calculating_2020. A total of 12 (2\*3\*2) scenarios will be considered. For each scenario, 1,000 training datasets will be generated, yielding a total of 12,000 datasets. Preliminary results presented in this report are for 10 iterations of the $p = 8, EF = 0.5, N = n$ scenario.

Training and validation data is simulated under a logistic model with strong interactions. For each observation $i$ ($i$ = 1, ..., $N$), predictors $\mathbf{x}_i$ are drawn from a $p$-variate normal distribution with parameters detailed as follows.

```{=tex}
\begin{equation}
    \mathbf{x}_i \sim \text{MVN}(\mathbf{0}, \left[ {\begin{array}{ccc}
    1 & 0.2 & ... \\
    0.2 & 1 & ... \\
    ... & ... & ...\\
  \end{array} } \right]).
  \label{predictor_dist}
\end{equation}
```

0.25$p$ two-way interactions are included, with the $h^{th}$ ($h = 1, ..., 0.25p$) interaction being the product of the $h^{th}$ and the $(h+0.5p)^{th}$ predictors. Then, the binary outcome $y_i$ is drawn from a Bernoulli distribution conditional on the predictor values (Formula \ref{binomial}).

```{=tex}
\begin{equation}
    P(y_i = 1|\mathbf{x}_i) = \frac{exp(\beta_0 + \sum_{j=1}^p\beta x_{ij} + \sum_{h=1}^{0.25*p}\gamma x_{ih}x_{i(h+0.5p)})}{1 + exp(\beta_0 + \sum_{j=1}^p\beta x_{ij} + \sum_{h=1}^{0.25*p}\gamma x_{ih}x_{i(h+0.5p)})},
    \label{binomial}
\end{equation}
```

where $\beta_0$, $\beta$, $\gamma$ are respectively the intercept, main effect regression coefficient, and interaction effect regression coefficient of the data generating model, hereafter called "true effects".

A validation dataset ($N = 10,000$) is generated for each training dataset in order to evaluate model performances. In the most extreme scenario ($EF = 0.1, p = 16$), this yields $\frac{10,000*0.1}{1.25p} = 50$ expected events per variable.

### True effect estimation

True effects are determined for each unique combination of $p$ and $EF$. Let $k$ denote a given combination. For each combination, the intercept $\beta_0^{(k)}$, predictor main effects $\boldsymbol\beta^{(k)}$, and predictor interaction effects $\boldsymbol\gamma^{(k)}$ are estimated using a large sample ($N = 100,000$) approximation. All main effects ($\boldsymbol\beta^{(k)}$) and interaction effects ($\boldsymbol\gamma^{(k)}$) are set to be equal (i.e., $\beta_1^{(k)} = \beta_2^{(k)} = ... = \beta_p^{(k)}$ and $\gamma_1^{(k)} = \gamma_2^{(k)} = ... = \gamma_{0.25p}^{(k)}$). The estimation uses use the R function `optim`, focused on minimising a loss function measuring the sum of (i) the absolute difference between the targeted AUC and the observed AUC in the simulated dataset, and (ii) the absolute difference between the targeted event fraction and the average estimated probability $P(y_i = 1|\mathbf{x}_i)$ in the simulated dataset. This estimation is done in three steps:

1. Optimise $\beta_0^{(k)}$ and $\boldsymbol\beta^{(k)}$ for a target AUC of 0.7.
2. Using the optimised $\beta_0^{(k)}$ and $\boldsymbol\beta^{(k)}$ from step 1, optimise $\boldsymbol\gamma^{(k)}$ for a target AUC of 0.8, such that a model ignoring interactions would have an AUC of 0.7 while including the correct interactions would lead to an AUC of 0.8. $\boldsymbol\gamma^{(k)}$ will be constrained to be positive.
3. Using the optimised $\boldsymbol\beta^{(k)}$ and $\boldsymbol\gamma^{(k)}$, optimise $\beta_0^{(k)}$ for a target AUC of 0.8 to ensure the interactions do not alter the event fraction.

```{r}
load("Data/betas_v2.RData")
betas_pilot <- betas_matrix[[1]][3,] %>% round(2)
```

This is repeated 20 times and the mean of the parameters is taken to obtain more stable estimates. True effects for the scenario presented in this report are $\beta_0$ = `r betas_pilot["intercept"]`, $\beta$ = `r betas_pilot["beta"]`, $\gamma$ = `r betas_pilot["gamma"]`.

```{r, results='hide'}
source("GenerateBetas.R")
start_seed <- 100
example_n <- 1e5
p  <- c(8, 16, 32)
EF <- c(0.1, 0.3, 0.5)
set.seed(2 * start_seed)
validation_betas_matrix <- validate_betas(
 betas_matrix = betas_matrix[[1]], example_n = example_n, n_predictors = p, prevalences = EF
)
set.seed(3 * start_seed)
validation_betas_matrix_noint <- validate_betas_noint(
 betas_matrix = betas_matrix[[1]], example_n = example_n, n_predictors = p, prevalences = EF
)
```

```{r}
source("DataSimFunctions.R")
source("tuning_functions.R")
load("Data/scenarios.RData")
scenarios <- scenarios %>% filter(n_pred == 8, event_fraction == 0.5, prop_sample_size == 1)
set.seed(4*start_seed)
beta_validation <- sim_data(scenarios = scenarios %>% mutate(n = example_n),
                            coefs = betas_matrix[[1]],
                            nsim = 1) %>%
  validation_prep("1_1")
validation_mod <- glm(Y ~ . + X1:X5 + X2:X6,
                      family = binomial(),
                      data = beta_validation)
max_coef_dev <- max(abs(round(coef(validation_mod), 2) - c(betas_pilot["intercept"], rep(betas_pilot["beta"], 8), rep(betas_pilot["gamma"], 2))))
```

Results from the numerical procedure for $\beta_0^{(k)}$, $\boldsymbol\beta^{(k)}$ and $\boldsymbol\gamma^{(k)}$ are checked using an independently generated dataset of $N = 100,000$. It is checked whether:

1. The observed event fraction is at a distance of at most 0.01 from the target event fraction.
2. The AUC of a model ignoring the interaction terms is at a distance of at most 0.025 from 0.7.
3. The AUC of a model including the interaction terms is at a distance of at most 0.05 from 0.8.
4. The estimated coefficients when fitting a logistic regression model are at a distance of at most 0.05 from the coefficients used to generate the dataset.

For the presented scenario, results of these checks are as follows:

1. The observed event fraction was at a distance of `r abs(round(validation_betas_matrix[3,"validation prevalence"] - 0.5, 4))` from 0.5.
2. The AUC of a model ignoring the interaction terms was at a distance of `r abs(round(validation_betas_matrix_noint[3,"validation AUC"] - 0.7, 2))` from 0.7.
3. The AUC of a model including the interaction terms was at a distance of `r abs(round(validation_betas_matrix[3,"validation AUC"] - 0.8, 2))` from 0.8.
4. The estimated coefficients when fitting a logistic regression model were at a distance of at most `r max_coef_dev` from the coefficients used to generate the dataset.

## Methods

We varied the hyperparameters that are tuned when fitting a random forest using the R package `ranger` [@ranger] via the R package `caret` [@caret]. We used grid search to optimise classification accuracy at a probability threshold of 0.5, which is the software default. 5-fold cross-validation was used as part of the tuning procedure.

Considering all possible combinations of 5 hyperparameters leads to 32 tuning procedures on each dataset. Due to the unfeasibility of such a large-scale approach, we reduced the number of combinations studied:  only hyperparameter combinations including at least `mtry` (the number of predictors randomly sampled to make a split) and `min.node.size` (the minimum number of observations for a node to be formed, i.e., the point at which a split should not be computed regardless of impurity) are considered. Other hyperparameters that may be included in combinations are (i) `replace` (whether the data used to fit a single tree is sampled with or without replacement), (ii) `sample.fraction` (the proportion of the data that is used to fit a single tree), and (iii) `splitrule` (the way in which a split is picked).

Default values and tuning ranges are presented in Table \ref{tab:hyp_ranges}.

```{=tex}
\begin{table}[]
    \centering
    \caption{Hyperparameter tuning ranges}
    \begin{tabular}{|c|c|c|}
         \hline
         Hyperparameter & Default & Range\\
         \hline
         \texttt{mtry} & $\sqrt{p}$ (rounded down) & 1-$p$\\
         \texttt{min.node.size} & 1 & 1-10\\ % STILL TO DETERMINE
         \hline
         \texttt{replace} & TRUE & TRUE, FALSE\\
         \texttt{sample.fraction} & 1 & 0.1, 0.2, ..., 0.9, 1\\
         \texttt{splitrule} & gini & gini, hellinger, extratrees\\
         \hline
    \end{tabular}
    \begin{tablenotes}
      \small
      \item For \texttt{splitrule} = extratrees, an additional parameter should be considered regarding the number of random splits to consider. This is set to its default of 1.
    \end{tablenotes}
    \label{tab:hyp_ranges}
\end{table}
```

This leads to 8 different combinations. Hyperparameters not included in a given combination are set to their default value. In addition, a model is built using the default hyperparameters to establish the baseline. All considered combinations are used to fit a random forest on each training dataset, leading to 90 tuning procedures being performed for these preliminary results.

## Performance measures

Primary outcomes are used to assess which hyperparameter combinations may be advisable to tune. These are discrimination, calibration slope, root mean square of the log of the calibration slope over all the runs of a scenario (RMSD(slope)), and computational time. Discrimination is measured by the AUC and represents whether positive observations have higher predicted risk than negative observation. Calibration is measured by the calibration slope (as calculated by @benvancalster_benvancalsterclassimb_calibration_2022) and represents the extent to which predicted risks reflect true risks. RMSD(slope) represents the extent to which the calibration slopes vary within a data-generating mechanism and how much they differ from the ideal value of 1, as used by @van_calster_regression_2020. Computational time is the amount of time for which a tuning procedure ran, measured in seconds.

Secondary outcomes are also presented to enable comparisons with later studies of this project. These secondary outcomes will be used as candidate optimisation metrics. They are the classification accuracy (the proportion of correctly classified observations; it is the optimisation metric used in this study) the calibration intercept (the extent to which the mean predicted risk reflects the true prevalence), the Brier score (which has a calibration and discrimination component), the logarithmic loss (how close the predicted risk comes to the observed outcome), and Cohen's Kappa (the proportion of correctly classified observations, accounting for chance).

Model performance metrics are estimated using the predictions of the model on an independently generated validation set generated under the same data generating mechanisms. For each data simulation scenario and tuning procedure combination, we compute the average and spread of each of these performance measures.

We evaluate and compare performance between hyperparameter combinations. We visualize whether certain hyperparameter combinations have a notably larger runtime compared to others, for a relatively low increase in performance. The best hyperparameter combination is selected considering all primary outcomes.

# Results

```{r}
load("Data/pilot2.RData")
rm(list=ls()[!ls() %in% c("dat", "validation_sets", "all_perf", "all_tuning_time", "validation_betas_matrix_noint", "validation_betas_matrix")])
```

```{r}
# clean data
all_perf <- all_perf %>%
  mutate(`Tuned hyperparameters` =
           ifelse(`Tuned hyperparameters` == "",
                "None",
                `Tuned hyperparameters`),
         Runtime = Runtime*60)
```

```{r}
df_scale_setter <- tibble(
  Accuracy = rep(c(0, 1), each = 9),
  AUC = rep(c(0.5, 1), each = 9),
  BrierScore = rep(c(0, 1), each = 9),
  CalibrationIntercept = rep(c(-1, 1), each = 9),
  CalibrationSlope = rep(c(0, 2), each = 9),
  CohensKappa = rep(c(0, 1), each = 9),
  LogarithmicLoss = rep(c(0, 1), each = 9),
  Runtime = 0,
  `Tuned hyperparameters` = rep(unique(all_perf$`Tuned hyperparameters`), 2)
) %>%
  pivot_longer(Accuracy:LogarithmicLoss,
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric, levels = c("Accuracy", "AUC", "CalibrationSlope", "BrierScore", "CalibrationIntercept", "LogarithmicLoss", "CohensKappa")))

df_targets <- tibble(
  Accuracy = 1,
  AUC = 1,
  BrierScore = 0,
  CalibrationIntercept = 0,
  CalibrationSlope = 1,
  CohensKappa = 1,
  LogarithmicLoss = 0,
  Runtime = 0,
  `Tuned hyperparameters` = unique(all_perf$`Tuned hyperparameters`)
) %>%
  pivot_longer(Accuracy:LogarithmicLoss,
               names_to = "Metric",
               values_to = "Target") %>%
  mutate(Metric = factor(Metric, levels = c("Accuracy", "AUC", "CalibrationSlope", "BrierScore", "CalibrationIntercept", "LogarithmicLoss", "CohensKappa")))

Metric.labs <- c("Calibration slope", "Calibration intercept", "Brier score", "Logarithmic loss", "Cohen's kappa", "AUC", "Classification accuracy")
names(Metric.labs) <- c("CalibrationSlope", "CalibrationIntercept", "BrierScore", "LogarithmicLoss", "CohensKappa", "AUC", "Accuracy")

HP.labs <- unique(all_perf$`Tuned hyperparameters`)
names(HP.labs) <- HP.labs
names(HP.labs)[2] <- "mtry + sample.fraction + replace\n+ min.node.size + splitrule"
# names(HP.labs)[4] <- "mtry + sample.fraction + min.node.size\n+ splitrule"
# names(HP.labs)[6] <- "mtry + sample.fraction + replace\n+ min.node.size"
HP.labs <- c(HP.labs[1], sort(HP.labs[2:9]))

HPcomb_pal <- viridis::turbo(n = 9)
```

```{r}
summary_table_study1 <- all_perf %>%
  group_by(`Tuned hyperparameters`) %>% 
  summarise(AUC_mean = round(mean(AUC), 2),
            AUC_sd = round(sd(AUC), 2),
            Calslope_median = round(median(CalibrationSlope), 2),
            Calslope_IQR = round(IQR(CalibrationSlope), 2),
            #Accuracy = paste0(round(mean(Accuracy),2), " (", round(sd(Accuracy), 2), ")"),
            `RMSD(slope)` = round(sqrt(mean((log(CalibrationSlope))^2)), 2),
            Runtime_mean = round(mean(Runtime), 1),
            Runtime_sd = round(sd(Runtime), 1))
```

Preliminary results using 10 simulated datasets for the scenario where $p = 8, EF = 0.5, N = n$ are presented here. The average and spread of primary outcomes for each hyperparameter combination are presented in Table \@ref(tab:summary-table).

```{r, results='asis'}
(summary_table_study1 %>% 
  mutate(AUC = paste0(AUC_mean, " (", AUC_sd, ")"),
         `Calibration slope` = paste0(Calslope_median, " (", Calslope_IQR, ")"),
         `Runtime (seconds)` = paste0(Runtime_mean, " (", formatC(Runtime_sd, digits = 3, flag = "0", format = "fg"), ")")) %>% 
  select(`Tuned hyperparameters`, AUC, `Calibration slope`, `RMSD(slope)`, `Runtime (seconds)`) %>% 
   rbind(c("", "Mean (SD)", "Median (IQR)", "", "Mean (SD)")))[c(10, 9,1:8),] %>%
  # mutate(`Tuned hyperparameters` = gsub(" . ", "\n+ ", `Tuned hyperparameters`)) %>% 
  xtable(caption = "Average performance of hyperparameter combinations",
         label = "tab:summary-table",
         align = c(rep("l", 2), rep("c", 3), "r")) %>% 
  print.xtable(include.rownames = FALSE,
               caption.placement = "top",
               comment = FALSE,
               hline.after = c(-1, 1, 10),
               #size="\\fontsize{8pt}{8pt}\\selectfont",
               scalebox = "0.8")
```

These results suggest that varying hyperparameter tuning combinations has very limited effects on discrimination, with a mean AUC of `r round(mean(all_perf$AUC),2)` (SD = `r round(sd(all_perf$AUC),2)`) across all hyperparameter combinations. A large effect was observed on model calibration and computational time. Model calibration varied both within and between combinations (Table \@ref(tab:summary-table), Figure \@ref(fig:study1fig)). The best calibration performance was obtained by tuning `mtry`, `replace`, `min.node.size`, and `splitrule`, which had a median calibration slope of `r summary_table_study1$'Calslope_median'[which.min(abs(summary_table_study1$'Calslope_median' - 1))]`. This performance was variable, such that the best RMSD(slope) was obtained when tuning no hyperparameters (RMSD(slope) = `r min(summary_table_study1$'RMSD(slope)')`).

```{r study1fig, fig.cap="Performance of each hyperparameter tuning combination", fig.height=8, fig.width=6}
all_perf %>%
  pivot_longer(AUC:CohensKappa,
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric, levels = c("AUC", "CalibrationSlope", "BrierScore", "CalibrationIntercept", "Accuracy", "LogarithmicLoss", "CohensKappa"))) %>% 
  ggplot(aes(x = Runtime,
             y = Performance,
             group = `Tuned hyperparameters`)) +
  geom_point(aes(col = `Tuned hyperparameters`,
                 shape = `Tuned hyperparameters`)) +
  xlim(0, max(all_perf$Runtime)) +
  geom_point(data = df_scale_setter, alpha = 0) +
  geom_hline(data = df_targets, aes(yintercept = Target), col = "red", lty = "dotted") +
  facet_wrap(~ Metric,
             scales = "free_y",
             labeller = labeller(Metric = Metric.labs),
             # nrow = 2) +
             ncol = 2) +
  theme_classic() +
  # theme(legend.position = c(.88, .2),
  theme(legend.position = c(.75, .1),
        legend.text = element_text(size = 8),
        legend.key.size = unit(0.4, 'cm'),
        legend.background = element_rect(),
        panel.spacing = unit(1.2, "lines"),
        plot.caption = element_text(hjust = 0)) +
  scale_color_manual(breaks = HP.labs,
                     labels = names(HP.labs),
                     values = HPcomb_pal) +
  scale_shape_manual(breaks = HP.labs,
                     labels = names(HP.labs),
                     values = c(1, rep(16, 8))) +
  labs(caption = "Red dotted lines show ideal performance.",
       x = "Runtime (seconds)")
```

Runtime did not vary much between different runs of a given hyperparameter combination (Table \@ref(tab:summary-table), Figure \@ref(fig:study1fig)). Large increases in runtime were observed when tuning 4 or more hyperparameters, with no consistent benefit in model performance as measured by primary and secondary outcome performance measures (Figure \@ref(fig:study1fig)). Taken together, these results suggest tuning hyperparameters has a large, but inconsistent, effect on model performance.

Secondary outcomes showed similar patterns (Figure \@ref(fig:study1fig). Classification accuracy, Brier score, and Cohen's Kappa showed very little variation within and between hyperparameter combinations. Calibration intercept and logarithmic loss showed variations within, but not between, hyperparameter combinations.

# Discussion

In this report, we studied the impact of the choice of hyperparameters being tuned on model performance and computational time. This is a first step in our aim to identify optimal hyperparameter tuning strategies for balancing predictive performance and computational time of tree-based methods for the development of clinical prediction models in low-dimensional settings. We presented preliminary results computed using one data simulation scenario and ten simulation runs. Tuning led to increases in runtime with large but heterogeneous effects on model calibration. Other performance measures showed little-to-no effects. Computational time was highly influenced by hyperparameter combinations: in this simple scenario, this ranged from 0.2 seconds to over 40 minutes. These preliminary results suggest tuning number of predictors, replacement, minimum node size, and splitting rule lead on average to the best model performance. This combination however had large amounts of variations, so tuning no hyperparameter may more reliably lead to the best performances as it performed adequately with less variation in calibration performance. These results are yet to be expanded by considering more simulation runs and data generating scenarios.

These results differ from previous research: @probst_tunability_2019 showed tuning the number of predictors considered at a split and the proportion of the data used to build a model to have an impact on model discrimination, whereas tuning these two hyperparameters did not lead to any notable differences in discrimination compared to not performing any tuning in our study. This inconsistency may be simply a product of chance due to the small scale of our simulation and this effect being relatively small in random forests compared to other algorithms. However, this could also be due to the data generating mechanism: as all datasets are generated following the same mechanism, it is possible that this mechanism aligns well with hyperparameter defaults, as tuning is not beneficial for all data [@probst_hyperparameters_2019]. This can be explored further by considering datasets generated using more predictors, predictors with varying effects, smaller event fractions, or smaller sample sizes.

It is also possible optimal hyperparameter values were not identifiable: while grid search is most commonly used, it suffers from some severe limitations. Candidate values for each hyperparameter are determined by the user, leading to possibly missing the optimal value in low-dimensional settings [@van_calster_regression_2020] [@vansmedenSampleSizeBinary2019] and to computational time increasing exponentially the more hyperparameters are tuned or candidate values are considered. As classification accuracy is optimised, this may lead to missing optimal values for other performance metrics. This highlights possible large systematic problems in random forest tuning procedures, which are heavily focused on obtaining good classification accuracy in high-dimensional settings. It is then of utmost importance to identify ways in which these tuning procedures can be adapted for low-dimensional, clinical settings, for which performance is better measured using discrimination and calibration.

This project will aim to have value by providing guidelines regarding possible trade-offs between computationally intensive tuning procedures and model performances. In the continuation of this project, this study will be repeated on a larger scale, and follow-up studies will be conducted to investigate the optimisation metric and hyperparameter candidate value generation algorithms. While this project has so far focused on random forests, other tree-based methods may also be explored in future.


\newpage

# References
