---
title: "**Tuning strategies for random forest hyperparameters in medical applications**"
author: "**Judith Neve**"
output:
  bookdown::pdf_book:
    citation_package: biblatex
    toc: false
bibliography: ["Thesis.bib"]
header-includes:
  \usepackage[backend=biber,style=numeric,sorting=none]{biblatex}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
library(viridis)
library(xtable)
library(kableExtra)
library(bookdown)
```

Date: 10/01/2023\
\
**Supervisors**: Maarten van Smeden and Zoë Dunias (Utrecht Medical Center Utrecht)\
\
Word Count: XXXX\
\


\newpage

# Introduction

Machine learning models are increasingly used in medicine for diagnostic and prognostic purposes [@wessler_tufts_2017] assisting patients' medical decision-making. For this purpose, *classification accuracy* (i.e., the proportion of observations correctly classified), which is most often used in assessing model performance, is not a sufficient measure to evaluate a model's clinical utility: it is not enough to correctly identify a patient as positive or negative, as the predicted risk is a key element of decision-making. Moreover, accuracy is dependent on the chosen classification threshold, that is, the cut-off predicted risk value between an observation being classified as positive or negative. For clinical purposes, *discrimination* (i.e., the ability to differentiate between classes regardless of the classification threshold) and *calibration* (i.e., the extent to which risk predictions reflect patients' true risk) [@van_calster_calibration_2019] are two much more important performance metrics. Models with low discrimination may misclassify patients, while miscalibrated models can lead to over- or undertreatment if over- or underestimating patients' risks.

A popular set of techniques is tree-based methods [@mitchell_machine_1997], among which random forests are particularly prevalent [@uddin_comparing_2019]. Random forests' results are influenced by hyperparameters (e.g. the number of predictors sampled to make a split) [@mantovani_empirical_2019], which need to be set before fitting the model. While hyperparameter software defaults sometimes lead to appropriate accuracy [@bernard_influence_2009], they can lead to severe miscalibration (work by Dunias, publication not yet available). It is then of utmost importance to identify hyperparameter tuning strategies improving calibration.

Tuning identifies hyperparameter values for which a performance metric (e.g. accuracy) is optimal for a given model. This procedure requires to choose which metric is optimised, which hyperparameters are tuned, and how candidate hyperparameter values are generated. The latter two can greatly impact computation time: generally, the more hyperparameters are tuned, the larger computation time will be. Hyperparameter search algorithms may consider all possible values and combinations, or use information collected earlier in the tuning procedure to tune efficiently. Computation time is important to consider in conjunction to the improvement in model performance: greater computation times and longer training periods of models may lead to a strain on resources and increase carbon emissions. For complex models, this can be as much as five times the average emissions of a car over its lifetime [@strubell_energy_2019].

There has been little focus on comparing tuning procedures on clinically relevant performance metrics. Previous research suggests the most gain in random forest performance can be made by tuning the number of predictors sampled to make a split and the proportion of the sample used to fit the tree [@probst_tunability_2019]. Tuning using most hyperparameter search algorithms improves accuracy compared to default hyperparameters, but can greatly increase computation time [@yang_hyperparameter_2020]; larger computational time shows no clear relationship with accuracy. So far, no study has compared different optimisation metrics' effects on model performance or computational time. 

The current project will thus address the following question: what are the optimal hyperparameter tuning strategies for balancing predictive performance and computational time of tree-based methods in low-dimensional settings? There will be three simulation studies in this project to identify (1) which hyperparameters to tune, (2) which metric to optimise, and (3) which search algorithm to use for optimal model performance. Methods and preliminary results from the first study are presented in this report.

# Methods

Methods follow the ADEMP approach [@morris_using_2019].

## Aim

@probst_tunability_2019 have shown the number of predictors considered at a split and the sample fraction to be the two most influential hyperparameters on model accuracy. However, these findings only investigate the effect of tuning one or two hyperparameters at once. This study aims to extend these findings by considering more combinations of hyperparameters in order to identify the combination of hyperparameters for which tuning leads to the best predictive performance of a prediction model.

## Data-generating mechanism

### Population

A full factorial simulation design will be used to consider the influence of data characteristics on tuning procedures. The varying factors will be the number of candidate predictors $p$ (range: 8, 16), the event fraction $EF$ (range: 0.1, 0.3, 0.5), and the sample size $N$ (range: $0.5n, n$, where $n$ is the minimum sample size required to identify effects for a given number of regression coefficients (here, 1.25$p$) and expected event fraction [@riley_calculating_2020] with an AUC of 0.8). A total of 12 (2\*3\*2) scenarios will be considered. 1,000 datasets will be generated for each scenario, yielding a total of 12,000 datasets. Preliminary results presented in this report are for 10 iterations of the $p = 8, EF = 0.5, N = n$ scenario.

Development and validation data is simulated under a logistic model with strong interactions. For each observation $i$ ($i$ = 1, ..., $N$), predictors $\mathbf{x}_i$ are drawn from a $p$-variate normal distribution with parameters detailed in Formula \ref{predictor_dist}.
```{=tex}
\begin{equation}
    \mathbf{x}_i \sim \text{MVN}(\mathbf{0}, \left[ {\begin{array}{ccc}
    1 & 0.2 & ... \\
    0.2 & 1 & ... \\
    ... & ... & ...\\
  \end{array} } \right])
  \label{predictor_dist}
\end{equation}
```

Additionally, 0.25$p$ two-way interactions will be computed, with the $j^{th}$ interaction being the product of the $j^{th}$ and the $(j+p/2)^{th}$ predictors. Then, the binary outcome $y_i$ will be drawn from a Bernoulli distribution conditional on $\mathbf{x}_i$, computed interactions, and the regression coefficients for main and interaction effects of the data generating model, hereafter called "true effect" (Formula \ref{binomial}).\begin{equation}
    P(y_i = 1) = \frac{exp(\beta_0 + \beta*\sum_{j=1}^px_{ij} + \gamma*\sum_{j=1}^{0.25*p}x_{ij}*x_{i(j+0.5p)})}{1 + exp(\beta_0 + \beta*\sum_{j=1}^px_{ij} + \gamma*\sum_{j=1}^{0.25*p}x_{ij}*x_{i(j+0.5p)})}
    \label{binomial}
\end{equation}

A validation dataset (N = 10,000) will be generated for each event fraction and number of candidate predictors combination in order to evaluate model performances. In the most extreme scenario ($EF = 0.1, p = 32$), this yields $\frac{10,000*0.1}{1.25p} = 25$ events per variable, which is well above the 10:1 events per variable rule of thumb.

### True effect estimation

True effects will be constant across studies. They will be determined as follows: for each combination $k$ of number of candidate predictor and event fraction, the intercept $\beta_0^{(k)}$, predictor main effects $\boldsymbol\beta^{(k)}$, and predictor interaction effects $\boldsymbol\gamma^{(k)}$ will be estimated using a large sample ($N$ = 100,000) approximation. All main effects ($\boldsymbol\beta^{(k)}$) and interaction effects ($\boldsymbol\gamma^{(k)}$) will be set to be equal (i.e., $\beta_1^{(k)} = \beta_2^{(k)} = ... = \beta_p^{(k)}$ and $\gamma_1^{(k)} = \gamma_2^{(k)} = ... = \gamma_{0.25p}^{(k)}$). The estimation will use the R function \texttt{optim}, focused on minimising a loss function measuring the sum of i) the absolute difference between the targeted AUC and the observed AUC in the simulated dataset, and ii) the absolute difference between the targeted event fraction and the average estimated probability $P(y_i = 1|\mathbf{x}_i)$ in the simulated dataset. This estimation will be done in three steps:

```{=tex}
\begin{enumerate}
    \item Optimise $\beta_0^{(k)}$ and $\boldsymbol\beta^{(k)}$ for a target AUC of 0.7.
    \item Using the optimised $\beta_0^{(k)}$ and $\boldsymbol\beta^{(k)}$ from step 1, optimise $\boldsymbol\gamma^{(k)}$ for a target AUC of 0.8, such that a model ignoring interactions would have an AUC of 0.7 while including the correct interactions would lead to an AUC of 0.8. $\boldsymbol\gamma^{(k)}$ will be constrained to be positive.
    \item Using the optimised $\boldsymbol\beta^{(k)}$ and $\boldsymbol\gamma^{(k)}$, optimise $\beta_0^{(k)}$ for a target AUC of 0.8 to ensure the interactions do not alter the event fraction.
\end{enumerate}
```
This will be repeated 20 times and the mean of the parameters will be taken to obtain more stable estimates.

Results from this numerical procedure for $\beta_0^{(k)}$, $\boldsymbol\beta^{(k)}$ and $\boldsymbol\gamma^{(k)}$ will be checked using an independently generated dataset of N = 1,000,000. It will be checked whether:

```{=tex}
\begin{enumerate}
    \item The observed event fraction is at a distance of at most 0.01 from the target event fraction.
    \item The AUC of a model ignoring the interaction terms is at a distance of at most 0.025 from 0.7.
    \item The AUC of a model including the interaction terms is at a distance of at most 0.05 from 0.8.
    \item The estimated coefficients when fitting a logistic regression model are at a distance of at most 0.05 from the coefficients used to generate the dataset.
\end{enumerate}
```
## Estimands

This study focuses on predictive performance for dichotomous outcome models. We also evaluate the computational time for each tuning procedure.

## Methods

We will vary which hyperparameters are tuned when fitting a random forest using the R package \texttt{ranger} via the R package \texttt{caret}. We will use grid search (as is the standard in this package) to optimise classification accuracy at a probability threshold of 0.5 (as is the default in \texttt{caret}). 5-fold cross-validation will be used as part of the tuning procedure.

\cite{probst_tunability_2019} found \texttt{mtry} (the number of predictors randomly sampled to make a split) and \texttt{sample.fraction} (the proportion of the data that is used to fit a single tree) to be the pair of predictors with the highest influence on accuracy. We would therefore suggest to always tune these two hyperparameters. However, \cite{scornet_tuning_2017} demonstrates that \texttt{sample.fraction} has a similar effect to and \texttt{min.node.size} (that is, the minimum number of observations for a node to be formed, i.e., the point at which a split should not be computed regardless of impurity). As \texttt{caret} allows tuning for \texttt{min.node.size} but not for \texttt{sample.fraction} in its settings, we opt to always tune \texttt{mtry} and \texttt{min.node.size} to increase to user-friendliness of our possible findings.

All combinations of the following hyperparameters will be tuned in conjunction with \texttt{mtry} and \texttt{min.node.size}:

```{=tex}
\begin{itemize}
    \item \texttt{replace}, that is, whether the data used to fit a single tree is sampled with or without replacement.
    \item \texttt{sample.fraction}, that is, the proportion of the data that is used to fit a single tree.
    \item \texttt{splitrule}, that is, the way in which a split is picked.
\end{itemize}
```
Default values and tuning ranges are presented in Table \ref{tab:hyp_ranges}.

```{=tex}
\begin{table}[]
    \centering
    \caption{Hyperparameter tuning ranges}
    \begin{tabular}{|c|c|c|}
         \hline
         Hyperparameter & Default & Range\\
         \hline
         \texttt{mtry} & $\sqrt{p}$ (rounded down) & 1-$p$\\
         \texttt{min.node.size} & 1 & 1-10\\ % STILL TO DETERMINE
         \hline
         \texttt{replace} & TRUE & TRUE, FALSE\\
         \texttt{sample.fraction} & 1 & 0.1, 0.2, ..., 0.9, 1\\
         \texttt{splitrule} & gini & gini, hellinger, extratrees\\
         \hline
    \end{tabular}
    \begin{tablenotes}
      \small
      \item For \texttt{splitrule} = extratrees, an additional parameter should be considered regarding the number of random splits to consider. This will be set to its default of 1.
    \end{tablenotes}
    \label{tab:hyp_ranges}
\end{table}
```
This leads to 16 ($\sum_{h=0}^4\binom{4}{h}$) different combinations. The number of predictors considered at each split and the sample fraction will be included in all combinations. Hyperparameters not included in a given combination will be set to their default value. In addition, a random forest will be fit using the default hyperparameters to establish the baseline. All considered combinations will be used to fit a random forest on each simulated dataset, leading to 459,000 (17\*27,000) tuning procedures being performed.

## Performance measures

For each tuning procedure performed, primary outcomes will be:

```{=tex}
\begin{itemize}
    \item Discrimination (AUC),
    \item Calibration slope (calculated using \cite{benvancalster_benvancalsterclassimb_calibration_2022}),
    \item Root mean square of the log of the calibration slope over all the runs of a scenario (RMSD(slope)), as used in \cite{van_calster_regression_2020},
    \item Computational time.
\end{itemize}
```
Secondary outcomes will be:

```{=tex}
\begin{itemize}
    \item Calibration intercept,
    \item Brier score,
    \item Logarithmic loss,
    \item Classification accuracy with a threshold of 0.5,
    \item Kappa.
\end{itemize}
```
Model performance metrics will be estimated using the predictions of the model on an independently generated validation set generated under the same data generating mechanisms. For each data simulation scenario and tuning procedure combination, we will compute the average and spread of each of these performance measures.

We will evaluate and compare performance between hyperparameter combinations. We aim to visually assess whether certain hyperparameter combinations have a notably larger runtime compared to others, for a relatively low increase in performance. The best hyperparameter combination will be selected considering all primary outcomes.

# Results

```{r}
load("Data/pilot2.RData")
rm(list=ls()[!ls() %in% c("dat", "validation_sets", "all_perf", "all_tuning_time")])
```

```{r}
# clean data
all_perf <- all_perf %>%
  mutate(`Tuned hyperparameters` =
           ifelse(`Tuned hyperparameters` == "",
                "None",
                `Tuned hyperparameters`))
```

## Summary table

Table \@ref(tab:summary-table)

```{r summary-table}
summary_table <- all_perf %>%
  group_by(n_pred, event_fraction, sample_size_prop, `Tuned hyperparameters`) %>%
  summarise(mean_AUC = mean(AUC),
            var_AUC = var(AUC),
            median_CalibrationSlope = median(CalibrationSlope),
            IQR_slope = IQR(CalibrationSlope),
            RMSD_slope = sqrt(mean((log(CalibrationSlope))^2)),
            mean_time = mean(Runtime),
            var_time = var(Runtime))
summary_table %>% 
  kbl(booktabs = T, caption = "Classification algorithms and imbalance corrections to be evaluated.") %>% 
  kable_styling(latex_options = c("striped", "hold_position"))
```

```{r}
summary_table_latex <- (all_perf %>%
  group_by(`Tuned hyperparameters`) %>% 
  summarise(AUC = paste0(round(mean(AUC), 2), " (", round(sd(AUC), 2), ")"),
            `Calibration slope` = paste0(round(median(CalibrationSlope), 2), " (", round(IQR(CalibrationSlope), 2), ")"),
            Accuracy = paste0(round(mean(Accuracy),2), " (", round(sd(Accuracy), 2), ")"),
            Runtime = paste0(round(mean(Runtime), 2), " (", round(sd(Runtime), 2), ")")))[c(9,2:8),] %>%
  xtable(caption = "Average performance of hyperparameter combinations")
```

```{r}
df_scale_setter <- tibble(
  Accuracy = rep(c(0, 1), each = 9),
  AUC = rep(c(0.5, 1), each = 9),
  BrierScore = rep(c(0, 1), each = 9),
  CalibrationIntercept = rep(c(-1, 1), each = 9),
  CalibrationSlope = rep(c(0, 2), each = 9),
  CohensKappa = rep(c(0, 1), each = 9),
  LogarithmicLoss = rep(c(0, 1), each = 9),
  Runtime = 0,
  `Tuned hyperparameters` = rep(unique(all_perf$`Tuned hyperparameters`), 2)
) %>%
  pivot_longer(Accuracy:LogarithmicLoss,
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric, levels = c("Accuracy", "AUC", "CalibrationSlope", "BrierScore", "CalibrationIntercept", "LogarithmicLoss", "CohensKappa")))

df_targets <- tibble(
  Accuracy = 1,
  AUC = 1,
  BrierScore = 0,
  CalibrationIntercept = 0,
  CalibrationSlope = 1,
  CohensKappa = 1,
  LogarithmicLoss = 0,
  Runtime = 0,
  `Tuned hyperparameters` = unique(all_perf$`Tuned hyperparameters`)
) %>%
  pivot_longer(Accuracy:LogarithmicLoss,
               names_to = "Metric",
               values_to = "Target") %>%
  mutate(Metric = factor(Metric, levels = c("Accuracy", "AUC", "CalibrationSlope", "BrierScore", "CalibrationIntercept", "LogarithmicLoss", "CohensKappa")))

Metric.labs <- c("Calibration slope", "Calibration intercept", "Brier score", "Logarithmic loss", "Cohen's kappa", "AUC", "Accuracy")
names(Metric.labs) <- c("CalibrationSlope", "CalibrationIntercept", "BrierScore", "LogarithmicLoss", "CohensKappa", "AUC", "Accuracy")

HP.labs <- unique(all_perf$`Tuned hyperparameters`)
names(HP.labs) <- HP.labs
# names(HP.labs)[2] <- "mtry + sample.fraction + replace\n+ min.node.size + splitrule"
# names(HP.labs)[4] <- "mtry + sample.fraction + min.node.size\n+ splitrule"
# names(HP.labs)[6] <- "mtry + sample.fraction + replace\n+ min.node.size"
HP.labs <- c(HP.labs[1], sort(HP.labs[2:9]))

HPcomb_pal <- viridis::turbo(n = 9)
```

```{r}
all_perf %>%
  pivot_longer(AUC:CohensKappa,
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric, levels = c("Accuracy", "AUC", "CalibrationSlope", "BrierScore", "CalibrationIntercept", "LogarithmicLoss", "CohensKappa"))) %>% 
  ggplot(aes(x = Runtime,
             y = Performance,
             group = `Tuned hyperparameters`)) +
  geom_point(aes(col = `Tuned hyperparameters`,
                 shape = `Tuned hyperparameters`)) +
  xlim(0, max(all_perf$Runtime)) +
  geom_point(data = df_scale_setter, alpha = 0) +
  geom_hline(data = df_targets, aes(yintercept = Target), col = "red", lty = "dotted") +
  facet_wrap(~ Metric,
             scales = "free_y",
             labeller = labeller(Metric = Metric.labs),
             # nrow = 2) +
             ncol = 3) +
  theme_classic() +
  # theme(legend.position = c(.88, .2),
  theme(legend.position = c(.85, .13),
        legend.text = element_text(size = 8),
        legend.key.size = unit(0.4, 'cm'),
        axis.title = element_text(size = 12),
        legend.background = element_rect(),
        panel.spacing = unit(1.2, "lines"),
        strip.text = element_text(colour = "black",
                                  size = 11)) +
  scale_color_manual(breaks = HP.labs,
                     labels = names(HP.labs),
                     values = HPcomb_pal) +
  scale_shape_manual(breaks = HP.labs,
                     labels = names(HP.labs),
                     values = c(1, rep(16, 8)))

#ggsave("plot_grouped2.jpg", width = 30, height = 15, units = "cm")
```

```{r}
all_perf %>%
  pivot_longer(c(Accuracy, AUC, CalibrationSlope),
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric, levels = c("AUC", "CalibrationSlope", "Accuracy"))) %>% 
  ggplot(aes(x = Runtime,
             y = Performance,
             group = `Tuned hyperparameters`)) +
  geom_point(aes(col = `Tuned hyperparameters`,
                 shape = `Tuned hyperparameters`),
             size = 2) +
  xlim(0, max(all_perf$Runtime)) +
  geom_point(data = df_scale_setter %>% filter(Metric %in% c("Accuracy", "AUC", "CalibrationSlope")), alpha = 0) +
  geom_hline(data = df_targets %>% filter(Metric %in% c("Accuracy", "AUC", "CalibrationSlope")), aes(yintercept = Target), col = "red", lty = "dotted") +
  facet_wrap(~ Metric,
             scales = "free_y",
             labeller = labeller(Metric = Metric.labs),
             nrow = 2) +
  theme_classic() +
  theme(legend.position = c(.8, .22),
  # theme(legend.position = c(.85, .13),
        legend.text = element_text(size = 10),
        legend.key.size = unit(0.6, 'cm'),
        axis.title = element_text(size = 12),
        legend.background = element_rect(),
        panel.spacing = unit(1.2, "lines"),
        strip.text = element_text(colour = "black",
                                  size = 11)) +
  scale_color_manual(breaks = HP.labs,
                     labels = names(HP.labs),
                     values = HPcomb_pal) +
  scale_shape_manual(breaks = HP.labs,
                     labels = names(HP.labs),
                     values = c(1, rep(16, 8)))

#ggsave("plot_grouped_top3.jpg", width = 30, height = 15, units = "cm")
```

```{r}
all_perf %>%
  pivot_longer(AUC:CohensKappa,
               names_to = "Metric",
               values_to = "Performance") %>%
  ggplot(aes(x = Runtime,
             y = Performance)) +
  geom_point() +
  xlim(0, max(all_perf$Runtime)) +
  geom_point(data = df_scale_setter, alpha = 0) +
  geom_hline(data = df_targets, aes(yintercept = Target), col = "red", lty = "dotted") +
  facet_wrap(Metric ~ `Tuned hyperparameters`, ncol = 9, scales = "free_y") +
  theme_classic() #+
  # geom_point(data = summary_table %>%
  #              dplyr::select(mean_AUC, median_CalibrationSlope, mean_time, `Tuned hyperparameters`) %>%
  #              mutate(AUC = mean_AUC,
  #                     CalibrationSlope = median_CalibrationSlope,
  #                     Runtime = mean_time) %>%
  #              pivot_longer(AUC:CalibrationSlope,
  #                           names_to = "Metric",
  #                           values_to = "Performance"),
  #            size = 3)
#ggsave("plot_facetted.jpg", width = 90, height = 40, units = "cm")
```

```{r}
summary_table %>% 
  pivot_longer(c(mean_AUC, median_CalibrationSlope),
               names_to = "Metric",
               values_to = "Performance") %>% 
  pivot_longer(c(var_AUC, IQR_slope),
               names_to = "XXX",
               values_to = "MC error")
```

# Discussion
