---
title: "**Tuning strategies for random forest hyperparameters in medical applications**"
author: "**Judith Neve**"
output:
  bookdown::pdf_book:
    citation_package: biblatex
    toc: false
bibliography: ["Thesis.bib"]
header-includes:
  \usepackage[backend=biber,style=numeric,sorting=none]{biblatex}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(MASS)
library(pROC)
library(tidyverse)
library(viridis)
library(xtable)
library(kableExtra)
library(bookdown)
```

Date: 10/01/2023\
\
**Supervisors**: Maarten van Smeden and Zoe Dunias (Utrecht Medical Center Utrecht)\
\
Word Count: XXXX\
\
What I don't want feedback on:

-	Look of the title page

What I want feedback on:

-	Overall look of the report (not detailed)
-	Do I use enough references -- are there points at which my argumentation would benefit from including a reference
- DGM methods: lots of focus (words) taken up -- is this necessary -- should I focus on only the scenario I'm talking about
- Talking about my results: they're preliminary -- should I be drawing conclusions as if they weren't (in terms of phrasing)?
- I struggled with the discussion (because I don't have a lot of results to discuss?). Is it too short? Is it too repetitive? Is the end too off-topic/dramatic?

\newpage

# Introduction

Machine learning models are increasingly used in medicine for diagnostic and prognostic purposes [@wessler_tufts_2017] assisting patients' medical decision-making. For this purpose, *classification accuracy* (i.e., the proportion of observations correctly classified), which is most often used in assessing model performance, is not a sufficient measure to evaluate a model's clinical utility: it is not enough to correctly identify a patient as positive or negative, as the predicted risk is a key element of decision-making. Moreover, accuracy is dependent on the chosen classification threshold, that is, the cut-off predicted risk value between an observation being classified as positive or negative. For clinical purposes, *discrimination* (i.e., the ability to differentiate between classes regardless of the classification threshold) and *calibration* (i.e., the extent to which risk predictions reflect patients' true risk) [@van_calster_calibration_2019] are two much more important performance metrics. Models with low discrimination may misclassify patients, while miscalibrated models can lead to over- or undertreatment if over- or underestimating patients' risks.

A popular set of techniques is tree-based methods [@mitchell_machine_1997], among which random forests are particularly prevalent [@uddin_comparing_2019]. Random forests' results are influenced by hyperparameters (e.g. the number of predictors sampled to make a split) [@mantovani_empirical_2019], which need to be set before fitting the model. While hyperparameter software defaults sometimes lead to appropriate accuracy [@bernard_influence_2009], they can lead to severe miscalibration (work by Dunias, publication not yet available). It is then of utmost importance to identify hyperparameter tuning strategies improving calibration.

Tuning identifies hyperparameter values for which a performance metric (e.g. accuracy) is optimal for a given model. This procedure requires to choose which metric is optimised, which hyperparameters are tuned, and how candidate hyperparameter values are generated. The latter two can greatly impact computation time: generally, the more hyperparameters are tuned, the larger computation time will be. Hyperparameter search algorithms may consider all possible values and combinations, or use information collected earlier in the tuning procedure to tune efficiently. Computation time is important to consider in conjunction to the improvement in model performance: greater computation times and longer training periods of models may lead to a strain on resources and increase carbon emissions. For complex models, this can be as much as five times the average emissions of a car over its lifetime [@strubell_energy_2019].

There has been little focus on comparing tuning procedures on clinically relevant performance metrics. Previous research suggests the most gain in random forest performance can be made by tuning the number of predictors sampled to make a split and the proportion of the sample used to fit the tree [@probst_tunability_2019]. Tuning using most hyperparameter search algorithms improves accuracy compared to default hyperparameters, but can greatly increase computation time [@yang_hyperparameter_2020]; larger computational time shows no clear relationship with accuracy. So far, no study has compared different optimisation metrics' effects on model performance or computational time. 

The current project will thus address the following question: what are the optimal hyperparameter tuning strategies for balancing predictive performance and computational time of tree-based methods in low-dimensional settings? There will be three simulation studies in this project to identify (i) which hyperparameters to tune, (ii) which metric to optimise, and (iii) which search algorithm to use for optimal model performance. Methods and preliminary results from the first study are presented in this report.

# Methods

Methods follow the ADEMP approach [@morris_using_2019].

## Aim

@probst_tunability_2019 have shown the number of predictors considered at a split and the sample fraction to be the two most influential hyperparameters on model discrimination. However, these findings only investigate the effect of tuning one or two hyperparameters at once. This study aims to extend these findings by considering more combinations of hyperparameters in order to identify the combination of hyperparameters for which tuning leads to the best predictive performance of a prediction model.

## Data-generating mechanism

### Population

A full factorial simulation design will be used to consider the influence of data characteristics on tuning procedures. The varying factors will be the number of candidate predictors $p$ (range: 8, 16), the event fraction $EF$ (range: 0.1, 0.3, 0.5), and the sample size $N$ (range: $0.5n, n$, where $n$ is the minimum sample size required to identify effects for a given number of regression coefficients (here, 1.25$p$) and expected event fraction as calculated by @riley_calculating_2020 with an AUC of 0.8). A total of 12 (2\*3\*2) scenarios will be considered. 1,000 training datasets will be generated for each scenario, yielding a total of 12,000 datasets. Preliminary results presented in this report are for 10 iterations of the $p = 8, EF = 0.5, N = n$ scenario.

Training and validation data is simulated under a logistic model with strong interactions. For each observation $i$ ($i$ = 1, ..., $N$), predictors $\mathbf{x}_i$ are drawn from a $p$-variate normal distribution with parameters detailed in Formula \ref{predictor_dist}.

```{=tex}
\begin{equation}
    \mathbf{x}_i \sim \text{MVN}(\mathbf{0}, \left[ {\begin{array}{ccc}
    1 & 0.2 & ... \\
    0.2 & 1 & ... \\
    ... & ... & ...\\
  \end{array} } \right])
  \label{predictor_dist}
\end{equation}
```

0.25$p$ two-way interactions are included, with the $j^{th}$ ($j = 1, ..., 0.25p$) interaction being the product of the $j^{th}$ and the $(j+0.5p)^{th}$ predictors. Then, the binary outcome $y_i$ is drawn from a Bernoulli distribution conditional on the predictor values (Formula \ref{binomial}).

```{=tex}
\begin{equation}
    P(y_i = 1|\mathbf{x}_i) = \frac{exp(\beta_0 + \beta*\sum_{j=1}^px_{ij} + \gamma*\sum_{j=1}^{0.25*p}x_{ij}*x_{i(j+0.5p)})}{1 + exp(\beta_0 + \beta*\sum_{j=1}^px_{ij} + \gamma*\sum_{j=1}^{0.25*p}x_{ij}*x_{i(j+0.5p)})},
    \label{binomial}
\end{equation}
```

where $\beta_0$, $\beta$, $\gamma$ are respectively the intercept, main effect regression coefficient, and interaction effect regression coefficient of the data generating model, hereafter called "true effects".

A validation dataset ($N = 10,000$) is generated for each training dataset in order to evaluate model performances. In the most extreme scenario ($EF = 0.1, p = 16$), this yields $\frac{10,000*0.1}{1.25p} = 50$ events per variable, which is well above the 10:1 events per variable rule of thumb.

### True effect estimation

True effects are determined as follows: for each combination $k$ of number of candidate predictor and event fraction, the intercept $\beta_0^{(k)}$, predictor main effects $\boldsymbol\beta^{(k)}$, and predictor interaction effects $\boldsymbol\gamma^{(k)}$ are estimated using a large sample ($N = 100,000$) approximation. All main effects ($\boldsymbol\beta^{(k)}$) and interaction effects ($\boldsymbol\gamma^{(k)}$) are set to be equal (i.e., $\beta_1^{(k)} = \beta_2^{(k)} = ... = \beta_p^{(k)}$ and $\gamma_1^{(k)} = \gamma_2^{(k)} = ... = \gamma_{0.25p}^{(k)}$). The estimation uses use the R function `optim`, focused on minimising a loss function measuring the sum of (i) the absolute difference between the targeted AUC and the observed AUC in the simulated dataset, and (ii) the absolute difference between the targeted event fraction and the average estimated probability $P(y_i = 1|\mathbf{x}_i)$ in the simulated dataset. This estimation is done in three steps:

1. Optimise $\beta_0^{(k)}$ and $\boldsymbol\beta^{(k)}$ for a target AUC of 0.7.
2. Using the optimised $\beta_0^{(k)}$ and $\boldsymbol\beta^{(k)}$ from step 1, optimise $\boldsymbol\gamma^{(k)}$ for a target AUC of 0.8, such that a model ignoring interactions would have an AUC of 0.7 while including the correct interactions would lead to an AUC of 0.8. $\boldsymbol\gamma^{(k)}$ will be constrained to be positive.
3. Using the optimised $\boldsymbol\beta^{(k)}$ and $\boldsymbol\gamma^{(k)}$, optimise $\beta_0^{(k)}$ for a target AUC of 0.8 to ensure the interactions do not alter the event fraction.

NOTE TO MAARTEN & ZOE: this is the way the betas were generated for the results I generated, so even though we don't think this is the correct way to optimise this, I think it is better to be transparent about how it was done for now

```{r}
load("Data/betas_v2.RData")
betas_pilot <- betas_matrix[[1]][3,] %>% round(2)
```

This was repeated 20 times and the mean of the parameters was taken to obtain more stable estimates. True effects for the presented scenario were estimated to be $\beta_0$ = `r betas_pilot["intercept"]`, $\beta$ = `r betas_pilot["beta"]`, $\gamma$ = `r betas_pilot["gamma"]`.

Results from this numerical procedure for $\beta_0^{(k)}$, $\boldsymbol\beta^{(k)}$ and $\boldsymbol\gamma^{(k)}$ were checked using an independently generated dataset of $N = 100,000$. It was checked whether:

1. The observed event fraction is at a distance of at most 0.01 from the target event fraction.
2. The AUC of a model ignoring the interaction terms is at a distance of at most 0.025 from 0.7.
3. The AUC of a model including the interaction terms is at a distance of at most 0.05 from 0.8.
4. The estimated coefficients when fitting a logistic regression model are at a distance of at most 0.05 from the coefficients used to generate the dataset.

```{r, results='hide'}
source("GenerateBetas.R")
start_seed <- 100
example_n <- 1e5
p  <- c(8, 16, 32)
EF <- c(0.1, 0.3, 0.5)
set.seed(2 * start_seed)
validation_betas_matrix <- validate_betas(
 betas_matrix = betas_matrix[[1]], example_n = example_n, n_predictors = p, prevalences = EF
)
set.seed(3 * start_seed)
validation_betas_matrix_noint <- validate_betas_noint(
 betas_matrix = betas_matrix[[1]], example_n = example_n, n_predictors = p, prevalences = EF
)
```

```{r}
source("DataSimFunctions.R")
source("tuning_functions.R")
load("Data/scenarios.RData")
scenarios <- scenarios %>% filter(n_pred == 8, event_fraction == 0.5, prop_sample_size == 1)
beta_validation <- sim_data(scenarios = scenarios %>% mutate(n = example_n),
                            coefs = betas_matrix[[1]],
                            nsim = 1) %>%
  validation_prep("1_1")
validation_mod <- glm(Y ~ . + X1:X5 + X2:X6,
                      family = binomial(),
                      data = beta_validation)
max_coef_dev <- max(abs(round(coef(validation_mod), 2) - c(betas_pilot["intercept"], rep(betas_pilot["beta"], 8), rep(betas_pilot["gamma"], 2))))
```


For the presented scenario, results of these checks were as follows:

1. The observed event fraction was at a distance of `r abs(round(validation_betas_matrix[3,"validation prevalence"] - 0.5, 4))` from 0.5.
2. The AUC of a model ignoring the interaction terms was at a distance of `r abs(round(validation_betas_matrix_noint[3,"validation AUC"] - 0.7, 2))` from 0.7.
3. The AUC of a model including the interaction terms was at a distance of `r abs(round(validation_betas_matrix[3,"validation AUC"] - 0.8, 2))` from 0.8.
4. The estimated coefficients when fitting a logistic regression model were at a distance of at most `r max_coef_dev` from the coefficients used to generate the dataset.

Condition 2 was thus not fulfilled; however, as these results are only preliminary, this discrepancy was disregarded.

## Estimands

This study focuses on predictive performance for dichotomous outcome models. We also evaluate the computational time for each tuning procedure.

## Methods

We vary which hyperparameters are tuned when fitting a random forest using the R package `ranger` [@ranger] via the R package `caret` [@caret]. We use grid search (as is the standard in this package) to optimise classification accuracy at a probability threshold of 0.5 (as is the default). 5-fold cross-validation is used as part of the tuning procedure.

Considering all possible combinations of 5 hyperparameters would lead to $\sum_{h=0}^5\binom{5}{h} = 32$ tuning procedures on each dataset. Due to the unfeasibility of such a large-scale approach, we reduce the number of combinations we consider: @probst_tunability_2019 found `mtry` (the number of predictors randomly sampled to make a split) and `sample.fraction` (the proportion of the data that is used to fit a single tree) to be the pair of predictors with the highest influence on discrimination. We would therefore suggest to always tune these two hyperparameters. However, @scornet_tuning_2017 demonstrates that `sample.fraction` has a similar effect to `min.node.size` (that is, the minimum number of observations for a node to be formed, i.e., the point at which a split should not be computed regardless of impurity). As `caret` allows tuning for `min.node.size` but not for `sample.fraction` in its settings, we opt to always tune `mtry` and `min.node.size` to increase user-friendliness of our findings.

All combinations of the following hyperparameters will be tuned in conjunction with `mtry` and `min.node.size`:

- `replace`, that is, whether the data used to fit a single tree is sampled with or without replacement.
- `sample.fraction`, that is, the proportion of the data that is used to fit a single tree.
- `splitrule`, that is, the way in which a split is picked.

Default values and tuning ranges are presented in Table \ref{tab:hyp_ranges}.

```{=tex}
\begin{table}[]
    \centering
    \caption{Hyperparameter tuning ranges}
    \begin{tabular}{|c|c|c|}
         \hline
         Hyperparameter & Default & Range\\
         \hline
         \texttt{mtry} & $\sqrt{p}$ (rounded down) & 1-$p$\\
         \texttt{min.node.size} & 1 & 1-10\\ % STILL TO DETERMINE
         \hline
         \texttt{replace} & TRUE & TRUE, FALSE\\
         \texttt{sample.fraction} & 1 & 0.1, 0.2, ..., 0.9, 1\\
         \texttt{splitrule} & gini & gini, hellinger, extratrees\\
         \hline
    \end{tabular}
    \begin{tablenotes}
      \small
      \item For \texttt{splitrule} = extratrees, an additional parameter should be considered regarding the number of random splits to consider. This is set to its default of 1.
    \end{tablenotes}
    \label{tab:hyp_ranges}
\end{table}
```

This leads to 8 ($\sum_{h=0}^3\binom{3}{h}$) different combinations. Hyperparameters not included in a given combination are set to their default value. In addition, a model is built using the default hyperparameters to establish the baseline. All considered combinations are used to fit a random forest on each training dataset, leading to 90 (9\*10) tuning procedures being performed for these preliminary results.

## Performance measures

For each tuning procedure performed, primary outcomes are:

- Discrimination (AUC),
- Calibration slope (as calculated by @benvancalster_benvancalsterclassimb_calibration_2022),
- Root mean square of the log of the calibration slope over all the runs of a scenario (RMSD(slope)), as used by @van_calster_regression_2020,
- Computational time.

Secondary outcomes are:

- Calibration intercept,
- Brier score,
- Logarithmic loss,
- Classification accuracy with a threshold of 0.5,
- Kappa.

Model performance metrics are estimated using the predictions of the model on an independently generated validation set generated under the same data generating mechanisms. For each data simulation scenario and tuning procedure combination, we compute the average and spread of each of these performance measures.

We evaluate and compare performance between hyperparameter combinations. We aim to visually assess whether certain hyperparameter combinations have a notably larger runtime compared to others, for a relatively low increase in performance. The best hyperparameter combination is selected considering all primary outcomes.

# Results

```{r}
load("Data/pilot2.RData")
rm(list=ls()[!ls() %in% c("dat", "validation_sets", "all_perf", "all_tuning_time", "validation_betas_matrix_noint", "validation_betas_matrix")])
```

```{r}
# clean data
all_perf <- all_perf %>%
  mutate(`Tuned hyperparameters` =
           ifelse(`Tuned hyperparameters` == "",
                "None",
                `Tuned hyperparameters`),
         Runtime = Runtime*60)
```

```{r}
summary_table_study1 <- all_perf %>%
  group_by(`Tuned hyperparameters`) %>% 
  summarise(AUC_mean = round(mean(AUC), 2),
            AUC_sd = round(sd(AUC), 2),
            Calslope_median = round(median(CalibrationSlope), 2),
            Calslope_IQR = round(IQR(CalibrationSlope), 2),
            #Accuracy = paste0(round(mean(Accuracy),2), " (", round(sd(Accuracy), 2), ")"),
            `RMSD(slope)` = round(sqrt(mean((log(CalibrationSlope))^2)), 2),
            Runtime_mean = round(mean(Runtime), 2),
            Runtime_sd = round(sd(Runtime), 2))
```

In this section we focus on preliminary results using 10 simulated datasets for the scenario where $p = 8, EF = 0.5, N = n$. The average and spread of primary outcomes for each hyperparameter combination are presented in Table \@ref(tab:summary-table). Discrimination performance did not vary between hyperparameter combinations, with a mean AUC of `r round(mean(all_perf$AUC),2)` (SD = `r round(sd(all_perf$AUC),2)`) across all hyperparameter combinations. The data generation mechanism had an AUC of `r round(validation_betas_matrix_noint[3,"validation AUC"], 2)` without interactions and an AUC of `r round(validation_betas_matrix[3,"validation AUC"], 2)` with interactions. The obtained AUC values thus show better performance than a model not accounting for interactions, but this performance does not closely approach the best possible performance. Tuning does not appear to affect model discrimination.

Calibration performance showed much more variability both within and between combinations (Table \@ref(tab:summary-table), Figure \@ref(fig:study1fig)): median calibration slopes for a given hyperparameter combination varied between `r min(summary_table_study1$Calslope_median)` (i.e., large overprediction of risk) when tuning only `mtry` and `min.node.size` and `r max(summary_table_study1$Calslope_median)` (i.e., large underprediction of risk) when tuning all hyperparameters. Within a combination, interquartile ranges varied between `r min(summary_table_study1$Calslope_IQR)` and `r max(summary_table_study1$Calslope_IQR)`. Slope RMSD suggests the best calibration performance is obtained when tuning no hyperparameters (RMSD(slope) = `r min(summary_table_study1$'RMSD(slope)')`).

Runtime did not vary much between different runs of a given hyperparameter combination (Table \@ref(tab:summary-table), Figure \@ref(fig:study1fig)). Large increases in runtime were observed when tuning 4 or more hyperparameters, with no observable benefit in model performance as measured by primary and secondary outcome performance measures (Figure \@ref(fig:study1fig)). Taken together, these results suggest not tuning hyperparameters yields the best performances: tuning hyperparameters does not lead to improved model performance and increases computational time.

```{r, results='asis'}
(summary_table_study1 %>% 
  mutate(AUC = paste0(AUC_mean, " (", AUC_sd, ")"),
         `Calibration slope` = paste0(Calslope_median, " (", Calslope_IQR, ")"),
         `Runtime (seconds)` = paste0(Runtime_mean, " (", Runtime_sd, ")")) %>% 
  select(`Tuned hyperparameters`, AUC, `Calibration slope`, `RMSD(slope)`, `Runtime (seconds)`) %>% 
   rbind(c("", "Mean (SD)", "Median (IQR)", "", "Mean (SD)")))[c(10, 9,1:8),] %>%
  # mutate(`Tuned hyperparameters` = gsub(" . ", "\n+ ", `Tuned hyperparameters`)) %>% 
  xtable(caption = "Average performance of hyperparameter combinations",
         label = "tab:summary-table",
         align = c(rep("|c", 5), "|c|")) %>% 
  print.xtable(include.rownames = FALSE,
               caption.placement = "top",
               comment = FALSE,
               hline.after = c(-1, 1, 10),
               #size="\\fontsize{8pt}{8pt}\\selectfont",
               scalebox = "0.8")
```

```{r}
df_scale_setter <- tibble(
  Accuracy = rep(c(0, 1), each = 9),
  AUC = rep(c(0.5, 1), each = 9),
  BrierScore = rep(c(0, 1), each = 9),
  CalibrationIntercept = rep(c(-1, 1), each = 9),
  CalibrationSlope = rep(c(0, 2), each = 9),
  CohensKappa = rep(c(0, 1), each = 9),
  LogarithmicLoss = rep(c(0, 1), each = 9),
  Runtime = 0,
  `Tuned hyperparameters` = rep(unique(all_perf$`Tuned hyperparameters`), 2)
) %>%
  pivot_longer(Accuracy:LogarithmicLoss,
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric, levels = c("Accuracy", "AUC", "CalibrationSlope", "BrierScore", "CalibrationIntercept", "LogarithmicLoss", "CohensKappa")))

df_targets <- tibble(
  Accuracy = 1,
  AUC = 1,
  BrierScore = 0,
  CalibrationIntercept = 0,
  CalibrationSlope = 1,
  CohensKappa = 1,
  LogarithmicLoss = 0,
  Runtime = 0,
  `Tuned hyperparameters` = unique(all_perf$`Tuned hyperparameters`)
) %>%
  pivot_longer(Accuracy:LogarithmicLoss,
               names_to = "Metric",
               values_to = "Target") %>%
  mutate(Metric = factor(Metric, levels = c("Accuracy", "AUC", "CalibrationSlope", "BrierScore", "CalibrationIntercept", "LogarithmicLoss", "CohensKappa")))

Metric.labs <- c("Calibration slope", "Calibration intercept", "Brier score", "Logarithmic loss", "Cohen's kappa", "AUC", "Accuracy")
names(Metric.labs) <- c("CalibrationSlope", "CalibrationIntercept", "BrierScore", "LogarithmicLoss", "CohensKappa", "AUC", "Accuracy")

HP.labs <- unique(all_perf$`Tuned hyperparameters`)
names(HP.labs) <- HP.labs
names(HP.labs)[2] <- "mtry + sample.fraction + replace\n+ min.node.size + splitrule"
# names(HP.labs)[4] <- "mtry + sample.fraction + min.node.size\n+ splitrule"
# names(HP.labs)[6] <- "mtry + sample.fraction + replace\n+ min.node.size"
HP.labs <- c(HP.labs[1], sort(HP.labs[2:9]))

HPcomb_pal <- viridis::turbo(n = 9)
```

```{r study1fig, fig.cap="Performance of each hyperparameter tuning combination", fig.height=8, fig.width=6}
all_perf %>%
  pivot_longer(AUC:CohensKappa,
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric, levels = c("AUC", "CalibrationSlope", "BrierScore", "CalibrationIntercept", "Accuracy", "LogarithmicLoss", "CohensKappa"))) %>% 
  ggplot(aes(x = Runtime,
             y = Performance,
             group = `Tuned hyperparameters`)) +
  geom_point(aes(col = `Tuned hyperparameters`,
                 shape = `Tuned hyperparameters`)) +
  xlim(0, max(all_perf$Runtime)) +
  geom_point(data = df_scale_setter, alpha = 0) +
  geom_hline(data = df_targets, aes(yintercept = Target), col = "red", lty = "dotted") +
  facet_wrap(~ Metric,
             scales = "free_y",
             labeller = labeller(Metric = Metric.labs),
             # nrow = 2) +
             ncol = 2) +
  theme_classic() +
  # theme(legend.position = c(.88, .2),
  theme(legend.position = c(.75, .1),
        legend.text = element_text(size = 8),
        legend.key.size = unit(0.4, 'cm'),
        legend.background = element_rect(),
        panel.spacing = unit(1.2, "lines"),
        plot.caption = element_text(hjust = 0)) +
  scale_color_manual(breaks = HP.labs,
                     labels = names(HP.labs),
                     values = HPcomb_pal) +
  scale_shape_manual(breaks = HP.labs,
                     labels = names(HP.labs),
                     values = c(1, rep(16, 8))) +
  labs(caption = "Red dotted lines show ideal performance.",
       x = "Runtime (seconds)")
```

# Discussion

In this report, we studied the impact of the choice of hyperparameters to tune on model performance and computational time. This was done as the first step to identify optimal hyperparameter tuning strategies for balancing predictive performance and computational time of tree-based methods in low-dimensional settings. We presented preliminary results computed using only one data simulation scenario and ten simulation runs. These preliminary results suggest tuning no hyperparameter may lead to the best performances as tuning was not found to lead to any improvements in model performance. These results are yet to be expanded by considering more simulation runs and data generating scenarios.

These results are inconsistent with previous research: @probst_tunability_2019 showed tuning the number of predictors considered at a split and the proportion of the data used to build a model to have an important impact on model discrimination, whereas tuning these two hyperparameters did not lead to any notable differences compared to not performing any tuning in our study. This inconsistency may be simply a product of chance due to the small scale of these results. However, this could also be due to the data generating mechanism: as all datasets are generated following the same mechanism, it is possible that this mechanism aligns well with hyperparameter defaults. This can be explored further by considering datasets generated using more predictors, predictors with varying effects, smaller event fractions, or smaller sample sizes.

The reason for this inconsistency may also be due to using accuracy as a default optimisation metric. @bernard_influence_2009 show hyperparameter defaults lead to satisfactory accuracy, which may in turn lead tuning procedures focusing on accuracy to resort to hyperparameter defaults. Candidate hyperparameter values may also be determined in a too-simplistic manner. While grid search is most commonly used, it suffers from some severe limitations. Candidate values for each hyperparameter are determined by the user, leading to possibly missing the optimal value in low-dimensional settings [@van_calster_regression_2020] [@vansmedenSampleSizeBinary2019] and to computation time increasing exponentially the more hyperparameters are tuned or candidate values are considered. This highlights possible large systematic problems in random forest tuning procedures, which are heavily focused on obtaining good accuracy in high-dimensional settings. It is then of utmost importance to identify ways in which these tuning procedures can be adapted for low-dimensional, clinical settings, for which performance is better measured using discrimination and calibration.

This project will aim to have value by providing guidelines regarding possible trade-offs between computationally intensive tuning procedures and model performances. In addition to exploring the effect of choice of hyperparameters that are tuned, the optimisation metric and hyperparameter candidate value generation algorithms will also be examined. While this project has so far focused on random forests, other tree-based methods may also be explored in future.


\newpage

# References
