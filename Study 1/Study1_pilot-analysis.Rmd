---
title: "Study1 pilot analysis"
author: "Judith Neve"
date: '2022-12-08'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(viridis)
library(xtable)
```

```{r}
# load("tuneonce.RData")
# datasets <- dat
# validation_datasets <- validation_sets
# performances <- all_perf %>%
#   mutate(dataset_id = "1_1")
# total_tuning_times <- all_tuning_time
# load("tuneonce2.RData")
# datasets <- rbind(datasets,
#                   dat %>%
#                     as.data.frame() %>%
#                     mutate(dataset_id = "1_2"))
# validation_datasets <- rbind(validation_datasets,
#                              validation_sets %>%
#                                as.data.frame() %>%
#                                mutate(dataset_id = "1_2"))
# performances <- rbind(performances,
#                       all_perf %>%
#                         mutate(dataset_id = "1_2"))
# total_tuning_times <- c(total_tuning_times, all_tuning_time)
# load("tuneonce3.RData")
# datasets <- rbind(datasets,
#                   dat %>%
#                     as.data.frame() %>%
#                     mutate(dataset_id = "1_3"))
# validation_datasets <- rbind(validation_datasets,
#                              validation_sets %>%
#                                as.data.frame() %>%
#                                mutate(dataset_id = "1_3"))
# performances <- rbind(performances,
#                       all_perf %>%
#                         mutate(dataset_id = "1_3"))
# total_tuning_times <- c(total_tuning_times, all_tuning_time)
# load("tuneonce4_2datasets.RData")
# datasets <- rbind(datasets,
#                   dat %>%
#                     as.data.frame() %>%
#                     mutate(dataset_id =
#                            recode(dataset_id,
#                                   `1_1` = "1_4",
#                                   `1_1` = "1_5")))
# validation_datasets <- rbind(validation_datasets,
#                              validation_sets %>%
#                                as.data.frame() %>%
#                                mutate(dataset_id =
#                                        recode(dataset_id,
#                                               `1_1` = "1_4",
#                                               `1_1` = "1_5")))
# performances <- rbind(performances,
#                       all_perf %>%
#                         mutate(dataset_id =
#                            recode(dataset_id,
#                                   `1_1` = "1_4",
#                                   `1_1` = "1_5")))
# total_tuning_times <- c(total_tuning_times, all_tuning_time)
```


```{r}
load("Data/pilot2.RData")
load("Data/betas_v2.RData")
rm(list=ls()[!ls() %in% c("dat", "validation_sets", "all_perf", "all_tuning_time")])
```

#### Check the probability distributions

```{r}
load("Data/betas_v2.RData")

beta <- betas_matrix[[1]][3,3:5]

dat %>% as_tibble() %>% 
  mutate(Pred_value = as.numeric(Pred_value)) %>% 
  pivot_wider(names_from = Pred_number, values_from = Pred_value) %>% 
  mutate(prob = exp(beta[1] + beta[2]*(X1+X2+X3+X4+X5+X6+X7+X8) + beta[3]*(X1*X5 + X2*X6)) / (1 + exp(beta[1] + beta[2]*(X1+X2+X3+X4+X5+X6+X7+X8) + beta[3]*(X1*X5 + X2*X6)))) %>%
  ggplot(aes(x = prob, fill = Y)) +
  geom_histogram(position = "dodge") +
  facet_wrap(~ dataset_id)
```

```{r}
validation_sets %>% as_tibble() %>% 
  mutate(Pred_value = as.numeric(Pred_value)) %>% 
  pivot_wider(names_from = Pred_number, values_from = Pred_value) %>% 
  mutate(prob = exp(beta[1] + beta[2]*(X1+X2+X3+X4+X5+X6+X7+X8) + beta[3]*(X1*X5 + X2*X6)) / (1 + exp(beta[1] + beta[2]*(X1+X2+X3+X4+X5+X6+X7+X8) + beta[3]*(X1*X5 + X2*X6)))) %>%
  ggplot(aes(x = prob, fill = Y)) +
  geom_histogram(position = "dodge") +
  facet_wrap(~ dataset_id)
```

```{r}
mod <- glm(factor(Y) ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 +
             X1*X5 + X2*X6,
           family = binomial(),
           data = validation_sets %>% as_tibble() %>% 
  mutate(Pred_value = as.numeric(Pred_value)) %>% 
  pivot_wider(names_from = Pred_number, values_from = Pred_value) %>% 
  mutate(prob = exp(beta[1] + beta[2]*(X1+X2+X3+X4+X5+X6+X7+X8) + beta[3]*(X1*X5 + X2*X6)) / (1 + exp(beta[1] + beta[2]*(X1+X2+X3+X4+X5+X6+X7+X8) + beta[3]*(X1*X5 + X2*X6)))))

(predict(mod, type = "response") - (validation_sets %>% as_tibble() %>% 
  mutate(Pred_value = as.numeric(Pred_value)) %>% 
  pivot_wider(names_from = Pred_number, values_from = Pred_value) %>% 
  mutate(prob = exp(beta[1] + beta[2]*(X1+X2+X3+X4+X5+X6+X7+X8) + beta[3]*(X1*X5 + X2*X6)) / (1 + exp(beta[1] + beta[2]*(X1+X2+X3+X4+X5+X6+X7+X8) + beta[3]*(X1*X5 + X2*X6)))))$prob) %>% 
  histogram()
```

```{r}
summary(mod)
```




#### Check the values of tuning times

```{r}
sum(all_perf$Runtime)
(sum(as.numeric(all_tuning_time))*60 - sum(all_perf$Runtime))
```

#### Data cleaning

```{r}
all_perf <- all_perf %>%
  mutate(`Tuned hyperparameters` =
           ifelse(`Tuned hyperparameters` == "",
                "None",
                `Tuned hyperparameters`))
```


## Summary table

```{r}
summary_table <- all_perf %>%
  group_by(n_pred, event_fraction, sample_size_prop, `Tuned hyperparameters`) %>%
  summarise(mean_AUC = mean(AUC),
            var_AUC = var(AUC),
            median_CalibrationSlope = median(CalibrationSlope),
            IQR_slope = IQR(CalibrationSlope),
            RMSD_slope = sqrt(mean((log(CalibrationSlope))^2)),
            mean_time = mean(Runtime),
            var_time = var(Runtime))
summary_table
```


```{r}
summary_table_latex <- (all_perf %>%
  group_by(`Tuned hyperparameters`) %>% 
  summarise(AUC = paste0(round(mean(AUC), 2), " (", round(sd(AUC), 2), ")"),
            `Calibration slope` = paste0(round(median(CalibrationSlope), 2), " (", round(IQR(CalibrationSlope), 2), ")"),
            Accuracy = paste0(round(mean(Accuracy),2), " (", round(sd(Accuracy), 2), ")"),
            Runtime = paste0(round(mean(Runtime), 2), " (", round(sd(Runtime), 2), ")")))[c(9,2:8),] %>%
  xtable(caption = "Average performance of hyperparameter combinations")
summary_table_latex %>% print(include.rownames = F)
```

```{r}
df_scale_setter <- tibble(
  Accuracy = rep(c(0, 1), each = 9),
  AUC = rep(c(0.5, 1), each = 9),
  BrierScore = rep(c(0, 1), each = 9),
  CalibrationIntercept = rep(c(-1, 1), each = 9),
  CalibrationSlope = rep(c(0, 2), each = 9),
  CohensKappa = rep(c(0, 1), each = 9),
  LogarithmicLoss = rep(c(0, 1), each = 9),
  Runtime = 0,
  `Tuned hyperparameters` = rep(unique(all_perf$`Tuned hyperparameters`), 2)
) %>%
  pivot_longer(Accuracy:LogarithmicLoss,
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric, levels = c("Accuracy", "AUC", "CalibrationSlope", "BrierScore", "CalibrationIntercept", "LogarithmicLoss", "CohensKappa")))

df_targets <- tibble(
  Accuracy = 1,
  AUC = 1,
  BrierScore = 0,
  CalibrationIntercept = 0,
  CalibrationSlope = 1,
  CohensKappa = 1,
  LogarithmicLoss = 0,
  Runtime = 0,
  `Tuned hyperparameters` = unique(all_perf$`Tuned hyperparameters`)
) %>%
  pivot_longer(Accuracy:LogarithmicLoss,
               names_to = "Metric",
               values_to = "Target") %>%
  mutate(Metric = factor(Metric, levels = c("Accuracy", "AUC", "CalibrationSlope", "BrierScore", "CalibrationIntercept", "LogarithmicLoss", "CohensKappa")))

Metric.labs <- c("Calibration slope", "Calibration intercept", "Brier score", "Logarithmic loss", "Cohen's kappa", "AUC", "Accuracy")
names(Metric.labs) <- c("CalibrationSlope", "CalibrationIntercept", "BrierScore", "LogarithmicLoss", "CohensKappa", "AUC", "Accuracy")

HP.labs <- unique(all_perf$`Tuned hyperparameters`)
names(HP.labs) <- HP.labs
# names(HP.labs)[2] <- "mtry + sample.fraction + replace\n+ min.node.size + splitrule"
# names(HP.labs)[4] <- "mtry + sample.fraction + min.node.size\n+ splitrule"
# names(HP.labs)[6] <- "mtry + sample.fraction + replace\n+ min.node.size"
HP.labs <- c(HP.labs[1], sort(HP.labs[2:9]))

turbo_pal <- viridis::turbo(n = 9)
```


```{r}
all_perf %>%
  pivot_longer(AUC:CohensKappa,
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric, levels = c("Accuracy", "AUC", "CalibrationSlope", "BrierScore", "CalibrationIntercept", "LogarithmicLoss", "CohensKappa"))) %>% 
  ggplot(aes(x = Runtime,
             y = Performance,
             group = `Tuned hyperparameters`)) +
  geom_point(aes(col = `Tuned hyperparameters`,
                 shape = `Tuned hyperparameters`)) +
  xlim(0, max(all_perf$Runtime)) +
  geom_point(data = df_scale_setter, alpha = 0) +
  geom_hline(data = df_targets, aes(yintercept = Target), col = "red", lty = "dotted") +
  facet_wrap(~ Metric,
             scales = "free_y",
             labeller = labeller(Metric = Metric.labs),
             # nrow = 2) +
             ncol = 3) +
  theme_minimal() +
  # theme(legend.position = c(.88, .2),
  theme(legend.position = c(.85, .13),
        legend.text = element_text(size = 8),
        legend.key.size = unit(0.4, 'cm'),
        axis.title = element_text(size = 12),
        legend.background = element_rect(),
        panel.spacing = unit(1.2, "lines"),
        strip.text = element_text(colour = "black",
                                  size = 11)) +
  scale_color_manual(breaks = HP.labs,
                     labels = names(HP.labs),
                     values = turbo_pal) +
  scale_shape_manual(breaks = HP.labs,
                     labels = names(HP.labs),
                     values = c(1, rep(16, 8)))

ggsave("plot_grouped2.jpg", width = 30, height = 15, units = "cm")
```

```{r}
all_perf %>%
  pivot_longer(c(Accuracy, AUC, CalibrationSlope),
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric, levels = c("AUC", "CalibrationSlope", "Accuracy"))) %>% 
  ggplot(aes(x = Runtime,
             y = Performance,
             group = `Tuned hyperparameters`)) +
  geom_point(aes(col = `Tuned hyperparameters`,
                 shape = `Tuned hyperparameters`),
             size = 2) +
  xlim(0, max(all_perf$Runtime)) +
  geom_point(data = df_scale_setter %>% filter(Metric %in% c("Accuracy", "AUC", "CalibrationSlope")), alpha = 0) +
  geom_hline(data = df_targets %>% filter(Metric %in% c("Accuracy", "AUC", "CalibrationSlope")), aes(yintercept = Target), col = "red", lty = "dotted") +
  facet_wrap(~ Metric,
             scales = "free_y",
             labeller = labeller(Metric = Metric.labs),
             nrow = 2) +
  theme_minimal() +
  theme(legend.position = c(.8, .22),
  # theme(legend.position = c(.85, .13),
        legend.text = element_text(size = 10),
        legend.key.size = unit(0.6, 'cm'),
        axis.title = element_text(size = 12),
        legend.background = element_rect(),
        panel.spacing = unit(1.2, "lines"),
        strip.text = element_text(colour = "black",
                                  size = 11)) +
  scale_color_manual(breaks = HP.labs,
                     labels = names(HP.labs),
                     values = turbo_pal) +
  scale_shape_manual(breaks = HP.labs,
                     labels = names(HP.labs),
                     values = c(1, rep(16, 8)))

ggsave("plot_grouped_top3.jpg", width = 30, height = 15, units = "cm")
```

```{r}
all_perf %>%
  pivot_longer(AUC:CohensKappa,
               names_to = "Metric",
               values_to = "Performance") %>%
  ggplot(aes(x = Runtime,
             y = Performance)) +
  geom_point() +
  xlim(0, max(all_perf$Runtime)) +
  geom_point(data = df_scale_setter, alpha = 0) +
  geom_hline(data = df_targets, aes(yintercept = Target), col = "red", lty = "dotted") +
  facet_wrap(Metric ~ `Tuned hyperparameters`, ncol = 9, scales = "free_y") +
  theme_minimal() #+
  # geom_point(data = summary_table %>%
  #              dplyr::select(mean_AUC, median_CalibrationSlope, mean_time, `Tuned hyperparameters`) %>%
  #              mutate(AUC = mean_AUC,
  #                     CalibrationSlope = median_CalibrationSlope,
  #                     Runtime = mean_time) %>%
  #              pivot_longer(AUC:CalibrationSlope,
  #                           names_to = "Metric",
  #                           values_to = "Performance"),
  #            size = 3)
ggsave("plot_facetted.jpg", width = 90, height = 40, units = "cm")
```


```{r}
summary_table %>% 
  pivot_longer(c(mean_AUC, median_CalibrationSlope),
               names_to = "Metric",
               values_to = "Performance") %>% 
  pivot_longer(c(var_AUC, IQR_slope),
               names_to = "XXX",
               values_to = "MC error")
```

```{r, eval = FALSE}
RFclass <- data.frame("OptimisationAlgorithm" = c("Default", "Grid search", "Random search", "BO-GP", "BO-TPE", "Hyperband", "BOHB", "GA", "PSO"),
                      "Category" = c("Default", "Model-free", "Model-free", "Bayesian optimisation", "Bayesian optimisation", "Multifidelity", "Multifidelity", "Metaheuristic", "Metaheuristic"),
                 "Accuracy" = c(90.65, 93.32, 93.38, 93.38, 93.88, 93.38, 90.38, 93.83, 93.73),
                 "CompTime" = c(0.09, 48.62, 16.73, 20.60, 12.58, 8.89, 8.45, 19.19, 12.43)) %>% 
  mutate(Category = factor(Category, levels = c("Default", "Model-free", "Bayesian optimisation", "Multifidelity", "Metaheuristic")))

algo_pal <- viridis::plasma(n = 5)

RFclass %>%
  ggplot(aes(y = Accuracy, x = CompTime, col = Category)) +
  geom_point(size = 3) +
  labs(x = "Computational time (in seconds)",
       y = "Accuracy (%)") +
  theme_classic() +
  labs(col = "Algorithm category",
       caption = "Adapted from Yang & Shami (2020)") +
  theme(plot.caption = element_text(hjust = 1.65)) +
  scale_color_manual(breaks = levels(RFclass$Category),,
                     values = algo_pal)
```