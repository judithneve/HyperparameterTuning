---
title: Evaluating tuning strategies for random forest hyperparameters with regards to prediction performance of clinical prediction models and computational time
author:
- name: J.A. Neve*
  num: a,b
address:
- num: a
  org: Department of Methodology and Statistics, Utrecht University, Utrecht, Netherlands
- num: b
  org: Julius Centre, University Medical Centre Utrecht, Utrecht, Netherlands
corres: "*Corresponding author \\email{j.a.nevedemevergnies@students.uu.nl}"
# presentaddress: This is sample for present address text this is sample for present address text
authormark: Neve
articletype: Research article
received: 2023-05-08
# revised: 2017-02-01
# accepted: 2017-03-01
abstract: "Write my abstract here - 250 words max"
keywords: Write; my; keywords; here - six max
bibliography: bibliography.bib
output: rticles::sim_article
header-includes:
  - \usepackage{amsmath}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
library(patchwork)
library(viridis)
library(xtable)
options(scipen=999)
```

# Introduction

Machine learning models are increasingly used in medicine for diagnostic and prognostic purposes [@wessler_tufts_2017]. Risk estimates are used to assist clinical decisions (e.g., whether to undergo treatment). A popular set of techniques in clinical research is tree-based methods [@mitchell_machine_1997], among which random forests are particularly prevalent [@uddin_comparing_2019;@andaurnavarroSystematicReviewIdentifies2022]. Random forests' results are influenced by hyperparameters (e.g., the number of predictors sampled to make a split) [@mantovani_empirical_2019], which need to be set before training a model. Previous studies have shown classification accuracy (i.e., the proportion of observations correctly classified) to have little-to-no improvement when using optimal hyperparameters for a given dataset compared to hyperparameter software defaults [@bernard_influence_2009;@probst_hyperparameters_2019]. Classification accuracy is however not sufficient to evaluate a model's clinical utility: identifying a patient as positive or negative may not carry sufficient information, as the patient's predicted risk is a key element of medical decision-making. Moreover, classification accuracy depends on the chosen classification threshold, i.e., the predicted risk value above which an observation is classified as positive. For clinical purposes, discrimination (i.e., positive patients have higher risk predictions than negative patients) and calibration (i.e., risk predictions reflect patients' true risk) [@van_calster_calibration_2019] are two much more important performance metrics. Models with low discrimination may misclassify patients, while miscalibrated models can lead to over- or undertreatment if over- or underestimating patients' risks. Random forests can heavily suffer from miscalibration [@benedettoCanMachineLearning2020;@djulbegovicDiagnosticPredictiveModel2019].

A well-known way to improve model performance is to perform hyperparameter tuning, which identifies hyperparameter values for which a performance metric (e.g., deviance) is optimal for a given prediction model. This procedure requires choosing which metric is optimised, which hyperparameters are tuned, and how candidate hyperparameter values are generated. The latter two can greatly impact computational time: generally, the more hyperparameters are tuned or the more candidate values there are, the larger computational time is. Hyperparameter search algorithms may consider all possible hyperparameter values and their combinations, or use information collected earlier in the tuning procedure to tune efficiently. Computational time is important to consider in conjunction to the improvement in model performance: greater computational times and longer training periods of models may lead to a strain on resources and increase carbon emissions. For complex models, this can be as much as five times the average emissions of a car over its lifetime [@strubell_energy_2019].

The optimal hyperparameter tuning strategy for balancing predictive performance and computational time is still an open question, particularly when considering clinically relevant performance metrics. Previous research suggests the most gain in random forest performance as measured by classification accuracy, discrimination, and the Brier score could be made by tuning the number of predictors sampled to make a split and the proportion of the sample used to fit the tree [@probst_tunability_2019]. Yang and Shami [@yang_hyperparameter_2020] found tuning using most hyperparameter search algorithms improved classification accuracy compared to default hyperparameters. Some search algorithms had greatly increased computational times without improving classification accuracy further than shorter search algorithms. So far, no study has compared different optimisation metrics' effects on model performance or computational time. Moreover, existing studies focusing on tuning procedures for random forests use high-dimensional datasets (datasets with more features than observations) whereas clinical research typically uses low-dimensional datasets (datasets with more observations than features), for which guidance regarding hyperparameter tuning is lacking [@ellenbach_improved_2021].

The current project aims to identify the optimal hyperparameter tuning strategies for balancing predictive performance and computational time of random forests in low-dimensional settings. There are three simulation studies in this project to identify (i) which hyperparameters to tune, (ii) which metric to optimise, and (iii) which search algorithm to use for optimal model performance.

# Methods

We ran three simulation studies in order to identify optimal tuning strategies in controlled conditions. The studies were done sequentially to integrate conclusions from prior studies. We simulated data on which to compare tuning strategies. In this methods section, we detail the simulation methodology for each study following the ADEMP approach [@morris_using_2019].

## Aim

### Study 1: Hyperparameters to tune

Using datasets from the OpenML platform [@bischlOpenMLBenchmarkingSuites2021], Probst and colleagues [@probst_tunability_2019] found the number of predictors considered at a split and the sample fraction to be the two most influential hyperparameters on the discriminative performance of prediction models. These findings only investigated the effect of tuning one or two hyperparameters at once. Our first study aimed to extend these findings by considering (i) model calibration in addition to model discrimination and (ii) more combinations of hyperparameters. We examined the effect of tuning different combinations of hyperparameters on the performance of a prediction model and evaluated performance improvements in context of required computational times.

### Study 2: Optimisation metric

This study aimed to identify a metric to optimise in the tuning procedure leading to the best predictive performance of a prediction model. We tuned the combination of hyperparameters considered optimal in Study 1.

### Study 3: Hyperparameter search algorithm

This study aimed to compare hyperparameter search algorithms' performances with respect to predictive performance and computational time. We tuned the combination of hyperparameters considered optimal in Study 1 using the optimisation metric selected in Study 2.

## Data-generating mechanism

All studies used the same data-generating mechanism. Different datasets were generated for each study.

### Data-generating scenarios

A full factorial simulation design was used to consider the influence of data characteristics on model predictive performance and computational time. The varying simulation factors were the number of candidate predictors $p$ (range: 8, 16), the event fraction $EF$ (range: 0.1, 0.3, 0.5), and the sample size $n$ (range: $0.5N$, $1N$). $N$ is the minimum sample size required to identify effects for a given number of regression coefficients (here, 1.25$p$), expected event fraction, and expected AUC (here, 0.8), as calculated by Riley and colleagues [@riley_calculating_2020]. A total of 12 (2\*3\*2) scenarios were considered. For each scenario, 500 training datasets were generated, yielding a total of 6,000 datasets per study. Scenario sample sizes are detailed in Table \ref{tab:scenarios}.

```{r, results = "asis"}
load("./../DGM_data/scenarios.RData")
colnames(scenarios) <- c("$p$", "$EF$", "prop", "$n$")

addtorow <- list()
addtorow$pos <- list(0)
addtorow$command <- paste(paste(
  colnames(scenarios)[-3], collapse = " & "
),"\\\\")

scenarios %>%
  select(-prop) %>% 
  mutate(`$p$` = as.integer(`$p$`),
         `$n$` = as.integer(`$n$`)) %>% 
  xtable(caption = "Data-generating scenarios", label = "tab:scenarios") %>% 
  print.xtable(include.rownames = FALSE,
               caption.placement = "top",
               comment = FALSE,
               add.to.row = addtorow,
               include.colnames = F)
```

Training and validation datasets were simulated under a logistic model with strong interactions. For each observation $i$ ($i$ = 1, ..., $N$), predictors $\mathbf{x}_i$ were drawn from a $p$-variate normal distribution with mean $\boldsymbol{\mu} = \mathbf{0}$, each predictor's variance set to 1, and two-way predictor correlations set to 0.2. We included 0.25$p$ two-way interactions, with the $h^{th}$ ($h = 1, ..., 0.25p$) interaction being the product of the $h^{th}$ and the $(h+p/4)^{th}$ predictors. The binary outcome $y_i$ was drawn from a Bernoulli distribution conditional on the predictor values (Formula \ref{binomial}).

```{=tex}
\begin{equation}
    P(y_i = 1|\mathbf{x}_i) = \frac{1}{1 + exp(-(\beta_0 + \sum_{j=1}^{p/2}\beta x_{ij} + \sum_{k=1+p/2}^{3p/4}2\beta x_{ik} + \sum_{h=1}^{0.25*p}\gamma x_{ih}x_{i(h+0.25p)}))}.
    \label{binomial}
\end{equation}
```

In Formula \ref{binomial}, $\beta_0$, $\beta$, $\gamma$ are respectively the intercept, main effect regression coefficient, and interaction effect regression coefficient of the data generating model, hereafter called "true effects".

A validation dataset ($n = 100,000$ for study 1 and $n = 10,000$ for studies 2 and 3) to evaluate model performance was generated for each training dataset. The size of the validation dataset was reduced after study 1 due to storage constraints.

### True effect estimation

True effects were determined for each unique combination of $p$ and $EF$. Let $k$ denote a given combination. For each combination, the intercept $\beta_0^{(k)}$, predictor main effects $\boldsymbol\beta^{(k)}$, and predictor interaction effects $\boldsymbol\gamma^{(k)}$ were estimated using a large sample approximation ($n = 100,000$). The first $p/2$ main effects were set to be equal (i.e., $\beta_1^{(k)} = \beta_2^{(k)} = ... = \beta_{p/2}^{(k)}$). The next $p/4$ main effects were set to be twice the strength of the first $p/2$ main effects (i.e., $\beta_{p/2+1}^{(k)} = \beta_{p/2+2}^{(k)} = ... = \beta_{3p/4}^{(k)} = 2\beta_1^{(k)}$). The final $p/4$ main effects were set to 0. All interaction effects $\boldsymbol\gamma^{(k)}$ were set to be equal (i.e., $\gamma_1^{(k)} = \gamma_2^{(k)} = ... = \gamma_{0.25p}^{(k)}$). The R function `optim` was used to minimise a loss function measuring the sum of the squared difference between (i) 0.7 and the observed AUC in a model with no interactions, (ii) 0.8 and the observed AUC in a model with interactions, and (iii) the targeted event fraction and the average estimated probability $P(y_i = 1|\mathbf{x}_i)$ in the simulated dataset. This was repeated 20 times for each scenario. Each generated set of true effects was used to generate a dataset of $n = 100,000$, on which the AUC (with and without interaction) and prevalence were measured. For each scenario, the true effect was selected to be the set of effects with the smallest sum of absolute differences between (i) target prevalence and observed prevalence, (ii) 0.7 and the AUC of a model with no interactions, and (iii) 0.8 and the AUC of a model with interactions. True effects' performances were assessed by generating a new dataset and measuring AUC and prevalence (Table \ref{tab:betastable}).

```{r}
load("./../DGM_data/betas_validation.RData")
```

```{r, results='asis'}
# format the summary table to latex
colnames(validation_betas_matrix) <- c("p", "EF (target)", "$\\beta_0$", "$\\beta$", "$\\gamma$", "AUC (no interaction)", "AUC (interaction)", "EF (obtained)")

addtorow <- list()
addtorow$pos <- list(0)
addtorow$command <- paste(paste(
  colnames(validation_betas_matrix), collapse = " & "
),"\\\\")

validation_betas_matrix %>%
  as.data.frame() %>% 
  mutate(p = as.character(p)) %>% 
  # mutate(`Tuned hyperparameters` = gsub(" . ", "\n+ ", `Tuned hyperparameters`)) %>% 
  xtable(caption = "Performance of selected true effects",
         label = "tab:betastable",
         align = rep("c", ncol(validation_betas_matrix) + 1)
         ) %>% 
  print.xtable(include.rownames = FALSE,
               caption.placement = "top",
               table.placement = "tb",
               comment = FALSE,
               hline.after = c(-1, 0, 6),
               #size="\\fontsize{8pt}{8pt}\\selectfont",
               # scalebox = "0.8",
               add.to.row = addtorow,
               include.colnames = F)
```

## Methods

### Study 1: Hyperparameters to tune

We varied which hyperparameters were tuned when fitting a random forest model using the R package `ranger` [@ranger] via the R package `caret` [@caret]. We used grid search to optimise deviance. 5-fold cross-validation was used.

Considering all possible combinations of 5 hyperparameters leads to 32 tuning procedures on each dataset. Due to the unfeasibility of such a large-scale approach, we reduced the number of combinations studied: only hyperparameter combinations including at least `mtry` (the number of predictors randomly sampled to make a split) and `min.node.size` (the minimum number of observations for a node to be formed, i.e., the point at which a split should not be computed regardless of impurity) were considered. Other hyperparameters which could be included in combinations were `replace` (whether the data used to fit a single tree is sampled with or without replacement), `sample.fraction` (the proportion of the data used to fit a single tree), and `splitrule` (the way in which a split is picked). Default values and tuning ranges used in this study are presented in Table \ref{tab:hyp_ranges}.

```{=tex}
\begin{table}[tb]
    \centering
    \caption{Hyperparameter tuning ranges in studies 1 and 2}
    \begin{tabular}{ccc}
         \hline
         Hyperparameter & Default & Range\\
         \hline
         \texttt{mtry} & $\sqrt{p}$ (rounded down) & 1-$p$\\
         \texttt{min.node.size} & 1 & 1-10\\
         \hline
         \texttt{replace} & TRUE & TRUE, FALSE\\
         \texttt{sample.fraction} & 1 & 0.1, 0.2, ..., 0.9, 1\\
         \texttt{splitrule} & gini & gini, hellinger, extratrees*\\
         \hline
    \end{tabular}
    \begin{tablenotes}
      \item *For \texttt{splitrule} = extratrees, an additional parameter should be considered regarding the number of random splits to consider. This was set to its default of 1.
    \end{tablenotes}
    \label{tab:hyp_ranges}
\end{table}
```

This yielded 8 hyperparameter combinations. Hyperparameters not included in a given combination were set to their default value. A model using the default hyperparameters was fit to establish the baseline. All considered combinations were used to fit a random forest on each training dataset.

### Study 2: Optimisation metric

We varied the metric to optimise when fitting a random forest using the R package `ranger` [@ranger] via the R package `caret` [@caret]. Each dataset was tuned once using each candidate optimisation metric. We used grid search to tune the hyperparameters considered optimal in Study 1. Tuning ranges were identical to those of study 1 (Table \ref{tab:hyp_ranges}). 5-fold cross-validation was used as part of the tuning procedure. The considered candidate optimisation metrics were the deviance (Formula \ref{deviance}; measuring the total deviations between predicted risk and the true outcome), the logarithmic loss (Formula \ref{logloss}; measuring the mean deviations between predicted risk and the true outcome, thus), the AUC (a measure of how well the classes are separated), the calibration intercept (the difference between the average predicted risk and the true event rate), the calibration slope (the extent of over- or underestimation of risk), the Brier score (Formula \ref{brier}; this can be seen as a composite measure of calibration and discrimination [@luijken_impact_2019]), the classification accuracy (the proportion of correctly classified observations) and Cohen's Kappa (the proportion of correctly classified observations, adjusted for chance). These are summarised in Table \ref{tab:metrics} alongside their target values.

```{=tex}
\begin{equation}
    \text{Deviance} = -2*\sum_{i=1}^{n}(y_i*log(\hat{y_i}) + (1-y_i)*log(1-\hat{y_i})).
    \label{deviance}
\end{equation}
```

```{=tex}
\begin{equation}
    \text{Logarithmic loss} = \frac{1}{n}\sum_{i=1}^{n}(y_i*log(\hat{y_i}) + (1-y_i)*log(1-\hat{y_i})).
    \label{logloss}
\end{equation}
```

```{=tex}
\begin{equation}
    \text{Brier score} = \frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y_i})^2.
    \label{brier}
\end{equation}
```

```{=tex}
\begin{table}[tb]
    \caption{Optimisation metric targets}
    \centering
    \begin{tabular}{cccc}
        \hline
        Metric & Target & Range of possible values\\
        \hline
        Deviance & 0 & [0, $\infty$]\\
        Logarithmic loss & 0 & [0, 1]\\
        AUC & 1 & [0.5, 1]\\
        Brier score & 0 & [0, 1]\\
        Calibration intercept* & 0 & [$-\infty$, $\infty$]\\
        Calibration slope** & 1 & [0, $\infty$]\\
        Classification accuracy & 1 & [0, 1]\\
        Cohen's Kappa & 1 & [0, 1]\\
        \hline
    \end{tabular}
    \begin{tablenotes}
      \item *optimised by minimising the squared value.
      \item **optimised by minimising the squared natural log of the value.
    \end{tablenotes}
    \label{tab:metrics}
\end{table}
```

### Study 3: Hyperparameter search algorithm

We varied the hyperparameter search algorithm used when fitting a random forest. We tuned the hyperparameters selected from Study 1 and optimised the metric selected from Study 2. Grid search, random search (both using the R package `caret` [@caret]), and model-based optimisation (using the R package `tuneRanger` [@probst_hyperparameters_2019]) were used.

Grid search takes a set of possible values for each hyperparameter and uses every possible combination to fit a random forest. Random search computes a given number of random combinations of hyperparameter values and uses each of these combinations to fit a random forest. For both of these search algorithms, the combination of hyparparameter values leading to the best performance (here, as measured by the deviance) is selected to be the optimal hyperparameter values. These two algorithms were performed using 5-fold cross-validation. Grid search used a grid with `mtry` going from 1 through to $p$ and `min.node.size` going from 1 through to 10. Random search was done by computing a grid with `mtry` going from 1 through to $p$ and `min.node.size` going from 1 through to $EF*N$, then randomly selected a number of rows equal to $p*10/2$ (such that the number of iterations was half the number of iterations in grid search).

Model-based optimisation first computes a given number (here, 30) of random combinations of hyperparameter values and uses each of these combinations to fit a random forest and assess its performance (here, using the deviance). Then, it fits a regression model with the hyperparameter values as predictors and the performance as the outcome to identify a combination of hyperparameter that is expected to perform better. This combination is tried, and the regression model is fit again including the new point. This is repeated a given number of times (here, 70). The best hyperparameter combination is then assessed to be the mean of the 5% best-performing combinations. This algorithm uses out-of-bag performance rather than cross-validation. We refer to Probst and colleagues [@probst_hyperparameters_2019] for details.

## Performance measures

The same outcomes were used in all studies. Primary outcomes were defined as those which were used to advise on the best tuning procedures. These measures were discrimination, calibration slope, root mean square of the log of the calibration slope over all the runs of a scenario (RMSD(slope)), and computational time. Discrimination was measured by the AUC and assessed whether positive observations have higher predicted risk than negative observation. Calibration was measured by the calibration slope (as calculated by Van Calster [@benvancalster_benvancalsterclassimb_calibration_2022]) and assessed the extent to which predicted risks reflect true risks. RMSD(slope) assessed the extent to which the calibration slopes varied within a data-generating mechanism and how much they differed from the ideal value of 1, as used by Van Calster and colleagues [@van_calster_regression_2020]. Computational time was the amount of time for which a given tuning procedure ran, measured in seconds.

Candidate optimisation metrics from Study 2 were also included as secondary outcomes in all studies. These were the classification accuracy, the calibration intercept, the Brier score, the logarithmic loss, and Cohen's Kappa.

Model performance metrics were estimated using the predictions of the model on a validation set independently generated under the same data generating mechanisms. For each data simulation scenario and tuning procedure combination, we computed the average and Monte Carlo error of each of these performance measures.

We evaluated and compared model predictive performance between hyperparameter combinations, optimisation metrics, and search algorithms, in studies 1, 2, and 3, respectively. We visualized whether certain hyperparameter combinations, optimisation metrics, or hyperparameter search algorithms had a notably larger runtime compared to others, without showing a substantial increase in performance. The best hyperparameter combination, optimisation metric, and hyperparameter search algorithm were assessed considering all primary outcomes.

## Software

Data was simulated and tuning procedures were performed on a high-performance computer. This was done in R version 4.2.2, using packages `MASS`, `dplyr`, `ranger`, `pROC`, `psych`, and in the case of study 3, `tuneRanger` [INCLUDE CITATIONS FOR ALL OF THESE]. Summary statistics and figures were generated on a personal computer using R version 4.2.3 [@base] using packages `tidyverse`, `viridis`, and `xtable` [CITE ALL OF THESE].

## Reproducibility

All code used to generate and analyse data is available at [LINK TO THE GITHUB REPO].

Data cannot be perfectly reproduced due to the nature of this study: the runtime variable is highly dependent on the system on which the script is run and other tasks being performed, so it can never be perfectly reproduced. Additionally, the high-performance computer had two operating systems responding differently to seeds. Runs were assigned to operating systems at random. However, these issues are only a concern with regard to an exact reproduction of the data, and should only lead to minor differences in results and no differences in interpretation.

# Results

## Study 1: Hyperparameters to tune

```{r}
load("./../Study1/Data/results.RData")
```

The average and Monte Carlo error of primary outcomes for each hyperparameter combination are presented in Table \ref{tab:results1}. Effects were homogeneous across scenarios. We illustrate results specific to the scenario where $p = 8$, $EF = 0.3$, and $n = 1N$ in Figure \ref{fig:study1fig}. Calibration plots for the scenarios where $EF = 0.5$ are presented in Figure \ref{fig:s1calplot}. Results for other scenarios can be visualised at [THIS WILL BE A LINK TO A SHINY APP].

```{r, results='asis'}
tab <- summary_table_study1 %>% 
  arrange(Runtime_mean, desc = FALSE) %>% 
  mutate(`Tuned hyperparameters` = hp_combination,
         AUC = paste0(AUC_mean, " (", AUC_sd, ")"),
         `Calibration slope` = paste0(Calslope_median, " (", Calslope_IQR, ")"),
         `Runtime (seconds)` = paste0(
           format(Runtime_mean, nsmall = 1),
           " (",
           str_pad(format(Runtime_sd, nsmall = 1, trim = TRUE), 6, pad = "0"),
           ")"
           )) %>% 
  dplyr::select(`Tuned hyperparameters`, AUC, `Calibration slope`, `RMSD(slope)`, `Runtime (seconds)`)

addtorow <- list()
addtorow$pos <- list(0)
addtorow$command <- paste0(
  paste0("\\multicolumn{1}{c}{", colnames(tab)[1], "}"),
  paste0("& \\multicolumn{1}{c}{", colnames(tab)[2:ncol(tab)], "}", collapse = ""),
  '\\\\',
  "\\multicolumn{1}{c}{}",
  paste0("& \\multicolumn{1}{c}{", c("Mean (SD)", "Median (IQR)", "", "Mean (SD)"), "}", collapse = ""),
  '\\\\'
)

tab %>%
  xtable(caption = "Average performance of hyperparameter combinations (aggregated over all scenarios). Rows are sorted in ascending order of runtime.",
         label = "tab:results1",
         align = c(rep("l", 2), rep("c", 3), "r")) %>% 
  print.xtable(include.rownames = FALSE,
               caption.placement = "top",
               table.placement = "tb",
               comment = FALSE,
               hline.after = c(-1, 0, 9),
               #size="\\fontsize{8pt}{8pt}\\selectfont",
               scalebox = "0.9",
               add.to.row = addtorow,
               include.colnames = F)
```

Large increases in runtime were observed when tuning 4 or more hyperparameters, with noticeably worse calibration performance and no observable change in other performance measures (Figure \ref{fig:study1fig}, Table \ref{tab:results1}). Varying hyperparameter tuning combinations had very limited effects on discrimination, with a mean AUC of `r round(mean_AUC, 2)` (SD = `r round(sd_AUC, 2)`) across all hyperparameter combinations. A large effect was observed on model calibration and computational time. Model calibration varied considerably within (Figure \ref{fig:study1fig}) and between (Table \ref{tab:results1}, Figure \ref{fig:study1fig}, Figure \ref{fig:s1calplot}) combinations. The best calibration performance was obtained by tuning `mtry` and `min.node.size`, which had a median calibration slope of `r summary_table_study1$'Calslope_median'[summary_table_study1$'hp_combination' == "mtry + min.node.size"]` across all scenarios. This performance was among the most stable with an IQR of `r summary_table_study1$'Calslope_IQR'[summary_table_study1$'hp_combination' == "mtry + min.node.size"]`. Tuning `mtry`, `min.node.size`, and `replace` performed similarly well, but took twice as long to run. `mtry` and `min.node.size` was thus selected as the best combination to tune. This is also visible in the calibration plots: calibration seems best when doing minimal tuning [OR SOMETHING TO THIS EXTENT ONCE I HAVE SAID PLOTS]

```{r, fig.cap="Performance of each hyperparameter tuning combination in the example scenario where $p$ = 8, $EF$ = 0.3, $n = 1N$\\label{fig:study1fig}", fig.height=8, out.width="100%"}
plot_ex_met
```

```{r, fig.cap="Calibration plots for a sample of the datasets where $EF$ = 0.5\\label{fig:s1calplot}", out.width="100%"}
knitr::include_graphics("./../Study1/Data/plot_EF_5.pdf")
```

Secondary outcomes showed similar patterns (Figure \ref{fig:study1fig}). Classification accuracy, Brier score, and logarithmic loss showed very little variation both within and between hyperparameter combinations. Calibration intercept and Cohen's Kappa showed variations within, but not between, hyperparameter combinations.

## Study 2: Optimisation metric

```{r}
load("./../Study2/Data/results.RData")
```

We tuned `mtry` and `min.node.size`. The average and Monte Carlo error of primary outcomes for each optimisation metric are presented in Table \ref{tab:results2}. Effects were homogeneous across scenarios. We illustrate results specific to the scenario where $p = 8$, $EF = 0.3$, and $n = 1N$ in Figures \ref{fig:study2fig} and \ref{fig:study2extra}. Calibration plots for the scenarios where $EF = 0.5$ are presented in Figure \ref{fig:s2calplot}. Results for other scenarios can be visualised at [THIS WILL BE A LINK TO A SHINY APP].

```{r, results='asis'}
tab <- summary_table_study2 %>% 
  mutate(`Optimisation metric` = recode(metric,
                                        LogLoss = "Logarithmic loss",
                                        CalSlope = "Calibration slope",
                                        BrierScore = "Brier score",
                                        CalInt = "Calibration intercept",
                                        Accuracy = "Classification accuracy",
                                        Kappa = "Cohen's Kappa"),
         AUC = paste0(AUC_mean, " (", AUC_sd, ")"),
         `Calibration slope` = paste0(Calslope_median, " (", Calslope_IQR, ")"),
         `Runtime (seconds)` = paste0(
           format(Runtime_mean, nsmall = 1),
           " (",
           Runtime_sd,
           ")"
           )) %>% 
  dplyr::select(`Optimisation metric`, AUC, `Calibration slope`, `RMSD(slope)`, `Runtime (seconds)`)

addtorow <- list()
addtorow$pos <- list(0)
addtorow$command <- paste0(
  paste0("\\multicolumn{1}{c}{", colnames(tab)[1], "}"),
  paste0("& \\multicolumn{1}{c}{", colnames(tab)[2:ncol(tab)], "}", collapse = ""),
  '\\\\',
  "\\multicolumn{1}{c}{}",
  paste0("& \\multicolumn{1}{c}{", c("Mean (SD)", "Median (IQR)", "", "Mean (SD)"), "}", collapse = ""),
  '\\\\'
)

tab %>%
  xtable(caption = "Average performance of hyperparameter combinations (aggregated over all scenarios). Rows are sorted in ascending order of RMSD(slope).",
         label = "tab:results2",
         align = c(rep("l", 2), rep("c", 3), "c")) %>% 
  print.xtable(include.rownames = FALSE,
               caption.placement = "top",
               table.placement = "tb",
               comment = FALSE,
               hline.after = c(-1, 0, 8),
               #size="\\fontsize{8pt}{8pt}\\selectfont",
               # scalebox = "0.8",
               add.to.row = addtorow,
               include.colnames = F)
```

Varying optimisation metrics had very limited effects on performance and computational time. Discrimination was equal across all optimisation metrics. All optimisation metrics showed similar model calibration performance, with the exception of optimising Cohen's Kappa and classification accuracy, which showed a worse median calibration performance. On the other hand, there are no visible differences in the calibration plots between optimisation metrics (Figure \ref{fig:s2calplot}). Runtime was comparable across all optimisation metrics. We select logarithmic loss to be optimised in Study 3, as it is among the best performing metrics and is an in-built metric in random forest optimisation packages (e.g. `tuneRanger` [@probst_hyperparameters_2019]), unlike deviance.

```{r, fig.cap="Performance of each optimisation metric in the example scenario where $p$ = 8, $EF$ = 0.3, $n$ = 1$N$\\label{fig:study2fig}", fig.height=8, fig.width=6}
plot_ex_met
```

```{r, fig.cap="Calibration plots for 200 of the datasets where $EF$ = 0.5\\label{fig:s2calplot}", out.width="100%"}
knitr::include_graphics("./../Study2/Output/plot_EF_5.pdf")
```

Secondary outcomes showed no substantial difference in performance (Figure \ref{fig:study2fig}) between optimisation metrics. We further investigated whether optimising a given metric in the training dataset led to better performance on that metric in the validation dataset. This is presented in Figure \ref{fig:study2extra}. For all metrics, it seems performance was not improved by optimising for that metric.

```{r, fig.cap="Comparison of the performance of optimisation metrics and corresponding performance metrics in the example scenario where $p$ = 8, $EF$ = 0.3, $n$ = 1$N$\\label{fig:study2extra}", fig.height=8, fig.width=6}
plot_ex_same
```

\newpage
