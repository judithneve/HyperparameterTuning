---
output:
  bookdown::pdf_book:
    citation_package: biblatex
    toc: false
bibliography: ["bibliography.bib"]
header-includes:
  \usepackage[backend=biber,style=nature,sorting=none]{biblatex}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
library(patchwork)
library(viridis)
library(xtable)
library(kableExtra)
library(bookdown)
options(scipen=999)
```

```{=tex}
\begin{titlepage}

\begin{center}
\Large Methodology and Statistics for the Behavioural, Biomedical and Social Sciences\\
\vspace{0.15\textwidth}
\textbf{\LARGE MASTER'S THESIS} 

\vspace{0.05\textwidth}

\rule{\textwidth}{1pt}\\[0.8cm]

\textbf { \LARGE Evaluating tuning strategies for random forest hyperparameters with regards to prediction performance of clinical prediction models and computational time.}
\\ [0.5cm]

\rule{\textwidth}{1pt}

\vspace{0.1\textwidth}
\LARGE{\textbf{Judith N\`eve}\\
0070661}
\end{center}
\begin{Large}
\begin{center}
    \vspace{0.2\textwidth}
    \textbf{Supervisors}\\
    Dr. Maarten van Smeden\\
    Zo\"e Dunias\\
\end{center}
\end{Large}

\vspace{0.05\textwidth}
\begin{large}
\begin{center}
  \textbf{Word count}\\
  ?/XXXX
\end{center}
\end{large}
\end{titlepage}
```

# Introduction

Machine learning models are increasingly used in medicine for diagnostic and prognostic purposes [@wessler_tufts_2017]. Risk estimates are used to assist clinical decisions (e.g., whether to undergo treatment). A popular set of techniques in clinical research is tree-based methods [@mitchell_machine_1997], among which random forests are particularly prevalent [@uddin_comparing_2019;@andaurnavarroSystematicReviewIdentifies2022]. Random forests' results are influenced by hyperparameters (e.g., the number of predictors sampled to make a split) [@mantovani_empirical_2019], which need to be set before training a model. Previous studies have shown classification accuracy (i.e., the proportion of observations correctly classified) to have little-to-no improvement when using optimal hyperparameters for a given dataset compared to hyperparameter software defaults [@bernard_influence_2009;@probst_hyperparameters_2019]. Classification accuracy is however not sufficient to evaluate a model's clinical utility: identifying a patient as positive or negative may not carry sufficient information, as the patient's predicted risk is a key element of medical decision-making. Moreover, classification accuracy depends on the chosen classification threshold, i.e., the predicted risk value above which an observation is classified as positive. For clinical purposes, discrimination (i.e., positive patients have higher risk predictions than negative patients) and calibration (i.e., risk predictions reflect patients' true risk) [@van_calster_calibration_2019] are two much more important performance metrics. Models with low discrimination may misclassify patients, while miscalibrated models can lead to over- or undertreatment if over- or underestimating patients' risks. Random forests can heavily suffer from miscalibration [@benedettoCanMachineLearning2020;@djulbegovicDiagnosticPredictiveModel2019].

A well-known way to improve model performance is to perform hyperparameter tuning, which identifies hyperparameter values for which a performance metric (e.g., deviance) is optimal for a given prediction model. This procedure requires choosing which metric is optimised, which hyperparameters are tuned, and how candidate hyperparameter values are generated. The latter two can greatly impact computational time: generally, the more hyperparameters are tuned or the more candidate values there are, the larger computational time is. Hyperparameter search algorithms may consider all possible hyperparameter values and their combinations, or use information collected earlier in the tuning procedure to tune efficiently. Computational time is important to consider in conjunction to the improvement in model performance: greater computational times and longer training periods of models may lead to a strain on resources and increase carbon emissions. For complex models, this can be as much as five times the average emissions of a car over its lifetime [@strubell_energy_2019].

There has been little focus on comparing tuning procedures on clinically relevant performance metrics. Previous research suggests the most gain in random forest performance as measured by classification accuracy, discrimination, and the Brier score could be made by tuning the number of predictors sampled to make a split and the proportion of the sample used to fit the tree [@probst_tunability_2019]. Yang and Shami [@yang_hyperparameter_2020] found tuning using most hyperparameter search algorithms improved classification accuracy compared to default hyperparameters. Some search algorithms had greatly increased computational times without improving classification accuracy further than shorter search algorithms. So far, no study has compared different optimisation metrics' effects on model performance or computational time. Moreover, existing studies focusing on tuning procedures for random forests use high-dimensional datasets (datasets with more features than observations) whereas clinical research typically uses low-dimensional datasets (datasets with more observations than features), for which guidance regarding hyperparameter tuning is lacking [@ellenbach_improved_2021].

The current project addresses the following question: what are the optimal hyperparameter tuning strategies for balancing predictive performance and computational time of tree-based methods in low-dimensional settings? There are three simulation studies in this project to identify (i) which hyperparameters to tune, (ii) which metric to optimise, and (iii) which search algorithm to use for optimal model performance.

\newpage

# Methods

This method section follows the ADEMP approach [@morris_using_2019].

## Aim

### Study 1: Hyperparameters to tune

Using datasets from the OpenML platform [@bischlOpenMLBenchmarkingSuites2021], Probst and colleagues [@probst_tunability_2019] found the number of predictors considered at a split and the sample fraction to be the two most influential hyperparameters on the discriminative performance of prediction models. These findings only investigated the effect of tuning one or two hyperparameters at once. Our first study aimed to extend these findings by considering (i) model calibration in addition to model discrimination and (ii) more combinations of hyperparameters. We examined the effect of tuning different combinations of hyperparameters on the performance of a prediction model and evaluate performance improvements in context of required computational times.

### Study 2: Optimisation metric

This study aimed to identify which metric to optimise in the tuning procedure leads to the best predictive performance of a prediction model. We tuned the combination of hyperparameters considered to be the most optimal in Study 1.

### Study 3: Hyperparameter search algorithm

text here.

## Data-generating mechanism

All studies used the same data-generating mechanism. Different datasets were generated for each study.

### Data-generating scenarios

A full factorial simulation design was used to consider the influence of data characteristics on model predictive performance and computational time. The varying simulation factors were the number of candidate predictors $p$ (range: 8, 16), the event fraction $EF$ (range: 0.1, 0.3, 0.5), and the sample size $N$ (range: $0.5n, n$). $n$ is the minimum sample size required to identify effects for a given number of regression coefficients (here, 1.25$p$), expected event fraction, and expected AUC (here, 0.8), as calculated by Riley and colleagues [@riley_calculating_2020]. A total of 12 (2\*3\*2) scenarios were considered. For each scenario, 500 training datasets were generated, yielding a total of 6,000 datasets per study.

Training and validation datasets were simulated under a logistic model with strong interactions. For each observation $i$ ($i$ = 1, ..., $N$), predictors $\mathbf{x}_i$ were drawn from a $p$-variate normal distribution with parameters detailed in Formula \ref{predictor_dist}. 0.25$p$ two-way interactions were included, with the $h^{th}$ ($h = 1, ..., 0.25p$) interaction being the product of the $h^{th}$ and the $(h+p/4)^{th}$ predictors. The binary outcome $y_i$ was drawn from a Bernoulli distribution conditional on the predictor values (Formula \ref{binomial}).

```{=tex}
\begin{equation}
    \mathbf{x}_i \sim \text{MVN}(\mathbf{0}, \left[ {\begin{array}{ccc}
    1 & 0.2 & ... \\
    0.2 & 1 & ... \\
    ... & ... & ...\\
  \end{array} } \right]).
  \label{predictor_dist}
\end{equation}
```

```{=tex}
\begin{equation}
    P(y_i = 1|\mathbf{x}_i) = \frac{1}{1 + exp(-(\beta_0 + \sum_{j=1}^{p/2}\beta x_{ij} + \sum_{j=1+p/2}^{3p/4}2\beta x_{ij} + \sum_{h=1}^{0.25*p}\gamma x_{ih}x_{i(h+0.25p)}))}.
    \label{binomial}
\end{equation}
```

In Formula \ref{binomial}, $\beta_0$, $\beta$, $\gamma$ are respectively the intercept, main effect regression coefficient, and interaction effect regression coefficient of the data generating model, hereafter called "true effects".

A validation dataset ($N = 100,000$ for study 1 and $N = 10,000$ for studies 2 and 3) was generated for each training dataset in order to evaluate model performance. The size of the validation dataset was reduced after study 1 due to storage constraints.

### True effect estimation

True effects were determined for each unique combination of $p$ and $EF$. Let $k$ denote a given combination. For each combination, the intercept $\beta_0^{(k)}$, predictor main effects $\boldsymbol\beta^{(k)}$, and predictor interaction effects $\boldsymbol\gamma^{(k)}$ were estimated using a large sample approximation ($N = 100,000$). The first $p/2$ main effects were set to be equal (i.e., $\beta_1^{(k)} = \beta_2^{(k)} = ... = \beta_{p/2}^{(k)}$). The next $p/4$ main effects were set to be twice the strength of the first $p/2$ main effects (i.e., $\beta_{1+p/2}^{(k)} = \beta_{2+p/2}^{(k)} = ... = \beta_{3p/4}^{(k)} = 2\beta_1^{(k)}$). The final $p/4$ main effects were set to 0. All interaction effects $\boldsymbol\gamma^{(k)}$ were set to be equal (i.e., $\gamma_1^{(k)} = \gamma_2^{(k)} = ... = \gamma_{0.25p}^{(k)}$). The R function `optim` was used to minimise a loss function measuring the sum of the squared difference between (i) 0.7 and the observed AUC in a model with no interactions, (ii) 0.8 and the observed AUC in a model with interactions, and (iii) the targeted event fraction and the average estimated probability $P(y_i = 1|\mathbf{x}_i)$ in the simulated dataset. This was repeated 20 times for each scenario. All generated effects were used to generate a validation dataset of $N = 100,000$. For each scenario, the best-performing set of effects was selected to be the set of effects with the smallest sum of absolute differences between (i) target prevalence and observed prevalence, (ii) 0.7 and the AUC of a model with no interactions, and (iii) 0.8 and the AUC of a model with interactions. A test dataset was generated to obtain performance estimates for the set of true effects. True effects and their performance are presented in Table \@ref(tab:betas-table).

```{r}
load("DGM_data/betas_validation.RData")
```

```{r, results='asis'}
# format the summary table to latex
colnames(validation_betas_matrix) <- c("p", "EF (target)", "$\\beta_0$", "$\\beta$", "$\\gamma$", "AUC (no interaction)", "AUC (interaction)", "EF (obtained)")

addtorow <- list()
addtorow$pos <- list(0)
addtorow$command <- paste(paste(
  colnames(validation_betas_matrix), collapse = " & "
),"\\\\")

validation_betas_matrix %>%
  as.data.frame() %>% 
  mutate(p = as.character(p)) %>% 
  # mutate(`Tuned hyperparameters` = gsub(" . ", "\n+ ", `Tuned hyperparameters`)) %>% 
  xtable(caption = "Performance of selected true effects",
         label = "tab:betas-table",
         align = rep("c", ncol(validation_betas_matrix) + 1)
         ) %>% 
  print.xtable(include.rownames = FALSE,
               caption.placement = "top",
               table.placement = "b",
               comment = FALSE,
               hline.after = c(-1, 0, 6),
               #size="\\fontsize{8pt}{8pt}\\selectfont",
               # scalebox = "0.8",
               add.to.row = addtorow,
               include.colnames = F)
```

## Methods

### Study 1: Hyperparameters to tune

We varied which hyperparameters were tuned when fitting a random forest model using the R package `ranger` [@ranger] via the R package `caret` [@caret]. We used grid search to optimise deviance. 5-fold cross-validation was used.

Considering all possible combinations of 5 hyperparameters leads to 32 tuning procedures on each dataset. Due to the unfeasibility of such a large-scale approach, we reduced the number of combinations studied: only hyperparameter combinations including at least `mtry` (the number of predictors randomly sampled to make a split) and `min.node.size` (the minimum number of observations for a node to be formed, i.e., the point at which a split should not be computed regardless of impurity) were considered. Other hyperparameters which could be included in combinations were `replace` (whether the data used to fit a single tree is sampled with or without replacement), `sample.fraction` (the proportion of the data used to fit a single tree), and `splitrule` (the way in which a split is picked). Default values and tuning ranges are presented in Table \ref{tab:hyp_ranges}.

```{=tex}
\begin{table}[tb]
    \centering
    \caption{Hyperparameter tuning ranges}
    \begin{tabular}{ccc}
         \hline
         Hyperparameter & Default & Range\\
         \hline
         \texttt{mtry} & $\sqrt{p}$ (rounded down) & 1-$p$\\
         \texttt{min.node.size} & 1 & 1-10\\ % STILL TO DETERMINE
         \hline
         \texttt{replace} & TRUE & TRUE, FALSE\\
         \texttt{sample.fraction} & 1 & 0.1, 0.2, ..., 0.9, 1\\
         \texttt{splitrule} & gini & gini, hellinger, extratrees\\
         \hline
    \end{tabular}
    \begin{tablenotes}
      \small
      \item For \texttt{splitrule} = extratrees, an additional parameter should be considered regarding the number of random splits to consider. This was set to its default of 1.
    \end{tablenotes}
    \label{tab:hyp_ranges}
\end{table}
```

This yielded 8 hyperparameter combinations. Hyperparameters not included in a given combination were set to their default value. A model using the default hyperparameters was fit to establish the baseline. All considered combinations were used to fit a random forest on each training dataset, leading to 90 tuning procedures being performed for these preliminary results.

### Study 2: Optimisation metric

We varied the metric to optimise when fitting a random forest using the R package `ranger` [@ranger] via the R package `caret` [@caret]. We used grid search (as is standard in this package) to tune the hyperparameters considered optimal in Study 1. 5-fold cross-validation was used as part of the tuning procedure. The considered candidate metrics are presented in Table \ref{tab:metrics} alongside their target values.

```{=tex}
\begin{table}[b]
    \caption{Optimisation metric targets}
    \centering
    \begin{tabular}{cccc}
        \hline
        Metric & Target & Range of possible values & Definition \\
        \hline
        Deviance & 0 & [0, $\infty$] & summed difference between the predicted risk and the true outcome\\
        Logarithmic loss & 0 & [0, 1] & mean difference between the predicted risk and the true outcome (penalising overconfident misclassifications)\\
        AUC & 1 & [0.5, 1] & how well the classes are separated\\
        Brier score & 0 & [0, 1] & difference between the predicted risk and the true outcome; can be seen as a composite measure of calibration and discrimination \cite{luijken_impact_2019}\\
        Calibration intercept & 0 & [$-\infty$, $\infty$] & difference between the average predicted risk and the true event rate\\
        Calibration slope & 1 & [0, $\infty$] & extent of over- or underestimation of risk\\
        Classification accuracy & 1 & [0, 1] & proportion of correctly classified observations\\
        Cohen's Kappa & 1 & [0, 1] & proportion of correctly classified observations (accounting for chance)\\
        \hline
    \end{tabular}
    \begin{tablenotes}
      \small
      \item For calibration intercept and slope, the target value is not a boundary value. Calibration intercept was optimised by minimising the squared value. Calibration slope was optimised by minimising the squared natural log of the value.
    \end{tablenotes}
    \label{tab:metrics}
\end{table}
```

That is, each dataset was tuned 8 times, leading to 54,000 tuning procedures.

### Study 3: Hyperparameter search algorithm

text here.

## Performance measures

The same outcomes were used in all studies. Primary outcomes were defined as those which were used to advise on the best tuning procedures. These measures were discrimination, calibration slope, root mean square of the log of the calibration slope over all the runs of a scenario (RMSD(slope)), and computational time. Discrimination was measured by the AUC and assessed whether positive observations have higher predicted risk than negative observation. Calibration was measured by the calibration slope (as calculated by Van Calster [@benvancalster_benvancalsterclassimb_calibration_2022]) and assessed the extent to which predicted risks reflect true risks. RMSD(slope) assessed the extent to which the calibration slopes varied within a data-generating mechanism and how much they differed from the ideal value of 1, as used by Van Calster and colleagues [@van_calster_regression_2020]. Computational time was the amount of time for which a given tuning procedure ran, measured in seconds.

Candidate optimisation metrics from study 2 were also included as secondary outcomes in all studies. These were the classification accuracy (the proportion of correctly classified observations at a threshold of 0.5) the calibration intercept (the extent to which the mean predicted risk reflects the true prevalence), the Brier score (which combines calibration and discrimination components), the logarithmic loss (how close the predicted risk comes to the observed outcome), and Cohen's Kappa (the proportion of correctly classified observations, accounting for chance).

Model performance metrics were estimated using the predictions of the model on an independently generated validation set generated under the same data generating mechanisms. For each data simulation scenario and tuning procedure combination, we computed the average and Monte Carlo error of each of these performance measures.

We evaluated and compared model predictive performance between hyperparameter combinations. We visualized whether certain hyperparameter combinations had a notably larger runtime compared to others, for a relatively low increase in performance. The best hyperparameter combination was assessed considering all primary outcomes.

## Software

Data was simulated and tuning procedures were performed on a high-performance computer. This was done in R version 4.2.2. Summary statistics and figures were generated on a personal computer using R version 4.1.2 [@base].

# Results

## Study 1: Hyperparameters to tune

```{r}
load("Study1/Data/results.RData")
```

The average and Monte Carlo error of primary outcomes for each hyperparameter combination are presented in Table \@ref(tab:results1). Effects were homogeneous across scenarios. We illustrate results specific to the scenario where $p = 8$, $EF = 0.3$, and $N = n$, in Figure \@ref(fig:study1fig). Results for other scenarios can be visualised at [THIS WILL BE A LINK TO A SHINY APP].

```{r, results='asis'}
tab <- summary_table_study1 %>% 
  arrange(Runtime_mean, desc = FALSE) %>% 
  mutate(`Tuned hyperparameters` = hp_combination,
         AUC = paste0(AUC_mean, " (", AUC_sd, ")"),
         `Calibration slope` = paste0(Calslope_median, " (", Calslope_IQR, ")"),
         `Runtime (seconds)` = paste0(
           format(Runtime_mean, nsmall = 1),
           " (",
           str_pad(format(Runtime_sd, nsmall = 1, trim = TRUE), 6, pad = "0"),
           ")"
           )) %>% 
  dplyr::select(`Tuned hyperparameters`, AUC, `Calibration slope`, `RMSD(slope)`, `Runtime (seconds)`)

addtorow <- list()
addtorow$pos <- list(0)
addtorow$command <- paste0(
  paste0("\\multicolumn{1}{c}{", colnames(tab)[1], "}"),
  paste0("& \\multicolumn{1}{c}{", colnames(tab)[2:ncol(tab)], "}", collapse = ""),
  '\\\\',
  "\\multicolumn{1}{c}{}",
  paste0("& \\multicolumn{1}{c}{", c("Mean (SD)", "Median (IQR)", "", "Mean(SD)"), "}", collapse = ""),
  '\\\\'
)

tab %>%
  xtable(caption = "Average performance of hyperparameter combinations (aggregated over all scenarios)",
         label = "tab:results1",
         align = c(rep("l", 2), rep("c", 3), "r")) %>% 
  print.xtable(include.rownames = FALSE,
               caption.placement = "top",
               table.placement = "b",
               comment = FALSE,
               hline.after = c(-1, 0, 9),
               #size="\\fontsize{8pt}{8pt}\\selectfont",
               scalebox = "0.8",
               add.to.row = addtorow,
               include.colnames = F)
```

These results show varying hyperparameter tuning combinations had very limited effects on discrimination, with a mean AUC of `r round(mean_AUC, 2)` (SD = `r round(sd_AUC, 2)`) across all hyperparameter combinations. A large effect was observed on model calibration and computational time. Model calibration varied considerably within and between combinations (Table \@ref(tab:results1), Figure \@ref(fig:study1fig)). The best calibration performance was obtained by tuning `mtry` and `min.node.size`, which had a median calibration slope of `r summary_table_study1$'Calslope_median'[summary_table_study1$'hp_combination' == "mtry + min.node.size"]` across all scenarios. This performance was among the most stable with an IQR of `r summary_table_study1$'Calslope_IQR'[summary_table_study1$'hp_combination' == "mtry + min.node.size"]`. [if I do make calibration plots, comments on them can go here]. Tuning `mtry`, `min.node.size`, and `replace` performed similarly well, but took twice as long to run. `mtry` and `min.node.size` was thus selected as the best combination to tune.

```{r study1fig, fig.cap="Performance of each hyperparameter tuning combination on the example scenario where p = 8, EF = 0.3, N = n", fig.height=8, fig.width=6}
plot_ex_met
```

More generally, large increases in runtime were observed when tuning 4 or more hyperparameters, with noticeably worse calibration performance and no observable change in other performance measures (Figure \@ref(fig:study1fig)).

Secondary outcomes showed similar patterns (Figure \@ref(fig:study1fig)). Classification accuracy, Brier score, and logarithmic loss showed very little variation both within and between hyperparameter combinations. Calibration intercept and Cohen's Kappa showed variations within, but not between, hyperparameter combinations.

## Study 2: Optimisation metric

```{r}
load("Study2/Data/results.RData")
```

The average and Monte Carlo error of primary outcomes for each optimisation metric are presented in Table \@ref(tab:results2). Effects were homogeneous across scenarios. We illustrate results specific to the scenario where $p = 8$, $EF = 0.3$, and $N = n$, in Figure \@ref(fig:study2fig). Results for other scenarios can be visualised at [THIS WILL BE A LINK TO A SHINY APP].

```{r, results='asis'}
tab <- summary_table_study2 %>% 
  mutate(`Optimisation metric` = metric,
         AUC = paste0(AUC_mean, " (", AUC_sd, ")"),
         `Calibration slope` = paste0(Calslope_median, " (", Calslope_IQR, ")"),
         `Runtime (seconds)` = paste0(
           format(Runtime_mean, nsmall = 1),
           " (",
           Runtime_sd,
           ")"
           )) %>% 
  dplyr::select(`Optimisation metric`, AUC, `Calibration slope`, `RMSD(slope)`, `Runtime (seconds)`)

addtorow <- list()
addtorow$pos <- list(0)
addtorow$command <- paste0(
  paste0("\\multicolumn{1}{c}{", colnames(tab)[1], "}"),
  paste0("& \\multicolumn{1}{c}{", colnames(tab)[2:ncol(tab)], "}", collapse = ""),
  '\\\\',
  "\\multicolumn{1}{c}{}",
  paste0("& \\multicolumn{1}{c}{", c("Mean (SD)", "Median (IQR)", "", "Mean(SD)"), "}", collapse = ""),
  '\\\\'
)

tab %>%
  xtable(caption = "Average performance of hyperparameter combinations (aggregated over all scenarios)",
         label = "tab:results2",
         align = c(rep("l", 2), rep("c", 3), "r")) %>% 
  print.xtable(include.rownames = FALSE,
               caption.placement = "top",
               table.placement = "b",
               comment = FALSE,
               hline.after = c(-1, 0, 8),
               #size="\\fontsize{8pt}{8pt}\\selectfont",
               # scalebox = "0.8",
               add.to.row = addtorow,
               include.colnames = F)
```

These results show varying optimisation metrics had very limited effects on performance and computational time. Model calibration varied showed some variations between optimisation metrics (Table \@ref(tab:results2), Figure \@ref(fig:study2fig)). The best calibration performance was obtained by optimising calibration slope, which had a median calibration slope of `r summary_table_study2$'Calslope_median'[summary_table_study2$'metric' == "CalSlope"]` across all scenarios. This performance was among the most stable with an IQR of `r `r summary_table_study2$'Calslope_IQR'[summary_table_study2$'metric' == "CalSlope"]`. [calibration plots comments go here]. Tuning deviance performed similarly.

```{r study2fig, fig.cap="Performance of each optimisation metric in the example scenario where p = 8, EF = 0.3, N = n", fig.height=8, fig.width=6}
plot_ex_met
```

Secondary outcomes showed no difference in performance (Figure \@ref(fig:study2fig)). We further investigated whether optimising a given metric in the training dataset led to better performance on that metric in the validation dataset. This is presented in Figure \@ref(fig:study2extra). [COMMENTS ON THIS ONCE I HAVE A BETTER PLOT]

```{r study2extra, fig.cap="Comparison of the performance of optimisation metrics and corresponding performance metrics in the example scenario where p = 8, EF = 0.3, N = n", fig.height=8, fig.width=6}
plot_ex_same
```

# Discussion

DON'T READ ON

In this report, we studied the impact of the choice of hyperparameters being tuned on model performance and computational time. This was a first step in our aim to identify optimal hyperparameter tuning strategies for balancing predictive performance and computational time of tree-based methods for the development of clinical prediction models in low-dimensional settings. We presented preliminary results computed using one data simulation scenario and ten simulation runs. Tuning led to increases in runtime with large but heterogeneous effects on model calibration. Other performance measures showed little-to-no effects. Computational time was highly influenced by hyperparameter combinations: in this simple scenario, this ranged from 0.2 seconds to over 40 minutes. These preliminary results suggest tuning number of predictors, replacement, minimum node size, and splitting rule leads on average to the best model performance. This hyperparameter combination had large amounts of variations. Not tuning hyperparameters led to more stable calibration performance, but consistently overestimated risk.

Findings regarding the impact of hyperparameter combination on computational time match expectations: tuning more hyperparameters increased computational time. This may have been exacerbated by using grid search to search for the optimal hyperparameters, as it is the most computationally intensive hyperparameter candidate value generation algorithm [@yang_hyperparameter_2020].

While these results are only preliminary and observed effects on model predictive performance may thus be a product of chance, it is important to consider alternative explanations for the observed results. Tuning is not beneficial for all data [@probst_hyperparameters_2019]: default hyperparameters may be close to optimal for the presented data-generating mechanism. This will be explored in the full-scale study, which will consider various event fractions, sample sizes, and numbers of predictors. We may also examine scenarios in which predictors have unequal effects, as one of the advantages of random forests is their ability to include variables with smaller, yet relevant, effects [@bernard_influence_2009;@probst_hyperparameters_2019]. Alternatively, optimal hyperparameter values may not have been identified. As classification accuracy is optimised, this may lead to missing optimal values for other performance metrics. In grid search, candidate values for each hyperparameter are determined by the user, leading to possibly missing the optimal value in low-dimensional settings [@van_calster_regression_2020;@vansmedenSampleSizeBinary2019]. This highlights possible large systematic problems in random forest tuning procedures, which are heavily focused on obtaining good classification accuracy in high-dimensional settings. We will investigate how predictive model performance can be improved by varying optimisation metrics and candidate hyperparameter search algorithms to adapt tuning procedures to low-dimensional, clinical settings.

Random forests are widely used in clinical research [@uddin_comparing_2019;@andaurnavarroSystematicReviewIdentifies2022], yet can suffer from miscalibration [@benedettoCanMachineLearning2020;@djulbegovicDiagnosticPredictiveModel2019]. In this preliminary study, we examined the impact of tuning different sets of hyperparameters on the predictive performance of random forest models for dichotomous risk prediction and on computational time. Results suggested tuning may have large effects on model predictive performance and computational time. This study will be conducted on a larger scale to provide more reliable findings, and follow-up studies will be conducted to investigate the effect of optimisation metrics and hyperparameter candidate value generation algorithms. We aim to provide guidelines regarding possible trade-offs between computationally intensive tuning procedures and model predictive performance. While this project has so far focused on random forests, other tree-based methods may be explored in future.

\newpage

# References
