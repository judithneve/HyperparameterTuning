---
title: "Results study 1"
author: "Judith Neve"
date: '2023-02-26'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

# Read in primary data

```{r}
# read all the files into a dataframe
df <- list.files(path = "./../Data/perfs/", pattern = ".rds") %>%
  paste0("./../Data/perfs/", .) %>% 
  map(readRDS) %>% 
  bind_rows()

df <- df %>% 
  # recode hyperparameter combination to say none
  mutate(hp_combination =
           ifelse(hp_combination == "",
                "None",
                hp_combination) %>%
           factor(),
         time = time*60) # convert runtime to seconds
```

# Primary data checks

```{r}
# check the number of unique datasets: we expect this to be 6000
unique(df$start_seed) %>% length()
```

```{r}
## during the study, if there were missing datasets due to failures, they were identified in the following way:

# check which ones are missing
expected_seeds <- c((1:3000)*100 + 8, (1:3000)*100 + 16)
expected_seeds[!(expected_seeds %in% unique(df$start_seed))] %>% paste(collapse = ", ") # this is what we pasted in "failed" (next chunk) to check how long the rerun took
(expected_seeds[!(expected_seeds %in% unique(df$start_seed))]/100) %>% floor() %>% paste(collapse = ",") # this is what we pasted into an sh file
```

```{r}
# vector of seeds that had failed runs
failed <- c(6416, 7616, 8216, 10016, 19016, 22016, 22616, 23216, 23816, 24416, 25016, 25616, 26216, 26816, 27416, 28016, 28616, 29216, 29816,
            30416, 45416, 47816, 56216, 56816, 58616, 70616, 85616, 86816,
            5216, 9416, 13016, 16616, 20216, 30416, 45416, 47816, 56216, 56816, 58616, 70616, 85616, 86816,
            93416, 98216, 98816, 103016, 106016, 106616, 107216, 107816, 115016, 119216, 119816, 120416, 133016, 133616, 145616, 148016, 148616, 149216, 149816, 159416, 160616, 165416, 175016, 175616, 176816, 177416, 182216, 184616, 185216, 188216, 188816, 189416, 190016, 191216, 191816, 193016, 193616, 194216, 194816, 195416, 197216, 202616, 209216, 209816, 210416, 214616, 215816, 219416, 220016, 222416, 223016, 223616, 224816, 227216, 229616, 230216, 232016, 232616, 237416, 238016, 238616, 241016, 241616, 242216, 242816, 244616, 245216, 245816, 246416, 247016, 248216, 249416, 251216, 252416, 253016, 253616, 256616, 259016, 260816, 263816, 264416, 271016, 271616, 274616, 275816, 276416, 277616, 278816, 280616, 283016, 283616, 284816, 286616, 287816, 288416, 289616, 290816, 291416, 293216, 293816, 294416, 295616, 296216, 298616, 299216, 47216, 49016, 50816, 51416, 55016, 55616, 58016, 61016, 62216, 62816, 63416, 64616, 65216, 65816, 66416, 67016, 68816, 69416, 70016, 71216, 73016, 74216, 76016, 76616, 77216, 77816, 78416, 79016, 79616, 80216, 80816, 82616, 83816, 84416, 85016, 86216, 88616, 89216,
            14816, 18416, 27416, 15416, 21416, 28016, 14216, 24416, 29816, 41216, 42416, 44816, 32816, 34016, 37616, 40016) %>%
  unique()

failed_1 <- (failed - 16)/100
```

```{r}
# which scenarios failed?
failed[(failed %% 100) == 8] # empty vector: none of the 8-predictor ones failed

# how many failed runs?
failed %>% length()
# how many failed runs took longer than 30 hours?
df_failed <- df %>% 
  filter(start_seed %in% failed) %>% 
  group_by(start_seed) %>% 
  summarise(time = sum(time))
df_failed %>% 
  filter(time > 30*60*60) %>% 
  nrow()
# longer than 32 hours?
df_failed_rerun <- df_failed %>% 
  filter(time > 32*60*60)
df_failed_rerun %>% 
  nrow()
failed_1_re <- (df_failed_rerun$start_seed - 16)/100
```

# Plots

## Help dataframes

```{r scale setting}
# set the min and max value for the y-axis of each facet
df_scale_setter <- tibble(
  AUC = rep(c(0.5, 1), each = 9),
  BrierScore = rep(c(0, 1), each = 9),
  CalIntercept = rep(c(-1, 1), each = 9),
  CalSlope = rep(c(0, 2.5), each = 9),
  LogLoss = rep(c(0, 1), each = 9),
  Accuracy = rep(c(0, 1), each = 9),
  CohensKappa = rep(c(0, 1), each = 9),
  time = 0,
  hp_combination = rep(unique(df$hp_combination), 2)
) %>%
  pivot_longer(AUC:CohensKappa,
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric, levels = c("AUC", "CalSlope", "BrierScore", "CalIntercept", "LogLoss", "Accuracy", "CohensKappa")))

# information about what the "ideal" measure is
df_targets <- tibble(
  AUC = 1,
  BrierScore = 0,
  CalIntercept = 0,
  CalSlope = 1,
  LogLoss = 0,
  Accuracy = 1,
  CohensKappa = 1,
  time = 0,
  hp_combination = unique(df$hp_combination)
) %>%
  pivot_longer(AUC:CohensKappa,
               names_to = "Metric",
               values_to = "Target") %>%
  mutate(Metric = factor(Metric, levels = c("AUC", "CalSlope", "BrierScore", "CalIntercept", "LogLoss", "Accuracy", "CohensKappa")))

# set the limit for the x-axis: longest time for 8 predictors & longest time for 16 predictors (depending on the scenario)
df_timelimits <- expand.grid(
  hp_combination = unique(df$hp_combination),
  p              = c(8, 16),
  EF             = c(0.1, 0.3, 0.5),
  n_prop         = c(0.5, 1),
  Metric         = factor(c("AUC", "CalSlope", "BrierScore", "CalIntercept", "LogLoss", "Accuracy", "CohensKappa"), levels = c("AUC", "CalSlope", "BrierScore", "CalIntercept", "LogLoss", "Accuracy", "CohensKappa")),
  Performance    = 0
) %>% 
  mutate(time = ifelse(p == 8,
                       max(df %>% filter(p == 8) %>% pull(time)),
                       max(df %>% filter(p == 16) %>% pull(time))),
         scenario = paste(n_prop, p, EF)) %>% 
  arrange(p)

# labeller vectors
## performance metric
Metric.labs <- c("Calibration slope", "Calibration intercept", "Brier score", "Logarithmic loss", "AUC", "Classification accuracy", "Cohen's Kappa")
names(Metric.labs) <- c("CalSlope", "CalIntercept", "BrierScore", "LogLoss", "AUC", "Accuracy", "CohensKappa")
## hyperparameter combinations
HP.labs <- unique(df$hp_combination)
names(HP.labs) <- HP.labs
HP.labs <- c(HP.labs[which(HP.labs == "None")], sort(HP.labs[which(HP.labs != "None")]))[c(1,2,3,9,5,4,6,8,7)]
names(HP.labs)[9] <- "mtry + min.node.size\n+ sample.fraction + replace + splitrule"

# define the colour palette
HPcomb_pal <- viridis::turbo(n = 9)
```

## Report plots

```{r scenario by metric}
# full plot: facet by scenario and metric
# this is not saved - not in the report - can be seen in the shiny app or run here
plot_sc_met <- df %>%
  group_by(n_prop, p, EF) %>% 
  mutate(scenario = paste(n_prop, p, EF)) %>% 
  pivot_longer(AUC:CohensKappa,
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric,
                         levels = c("AUC", "CalSlope", "CalIntercept", "BrierScore", "LogLoss", "Accuracy", "CohensKappa"))) %>%
  ggplot(aes(x = time,
             y = Performance,
             group = hp_combination)) +
  geom_point(aes(col = hp_combination,
                 shape = hp_combination)) +
  geom_point(data = df_scale_setter, alpha = 0) +
  geom_point(data = df_timelimits, alpha = 0) +
  geom_hline(data = df_targets, aes(yintercept = Target), col = "red", lty = "dotted") +
  facet_wrap(factor(scenario, levels = unique(df_timelimits$scenario)) ~ Metric,
             scales = "free",
             labeller = labeller(Metric = Metric.labs),
             ncol = 7) +
  theme_classic() +
  theme(legend.text = element_text(size = 8),
        legend.key.size = unit(0.4, 'cm'),
        legend.background = element_rect(),
        panel.spacing = unit(1.2, "lines"),
        plot.caption = element_text(hjust = 0)) +
  scale_color_manual(breaks = HP.labs,
                     labels = names(HP.labs),
                     values = HPcomb_pal) +
  scale_shape_manual(breaks = HP.labs,
                     labels = names(HP.labs),
                     values = c(1, rep(16, 8))) +
  labs(x = "Runtime (seconds)",
       col = "Hyperparameter combination",
       shape = "Hyperparameter combination")
```

```{r}
# example plot: one scenario, facetted by performance metric
plot_ex_met <- df %>%
  filter(n_prop == 1,
         p      == 16,
         EF     == 0.5) %>% 
  pivot_longer(AUC:CohensKappa,
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric,
                         levels = c("AUC", "CalSlope", "CalIntercept", "BrierScore", "LogLoss", "Accuracy", "CohensKappa"))) %>%
  ggplot(aes(x = time,
             y = Performance,
             group = hp_combination)) +
  geom_point(aes(col = hp_combination,
                 shape = hp_combination)) +
  geom_point(data = df_scale_setter, alpha = 0) +
  geom_hline(data = df_targets, aes(yintercept = Target), col = "red", lty = "dotted") +
  facet_wrap(~ Metric,
             scales = "free",
             labeller = labeller(Metric = Metric.labs),
             ncol = 2) +
  theme_classic() +
  theme(#legend.position = c(.75, .1),
        legend.text = element_text(size = 8),
        legend.key.size = unit(0.4, 'cm'),
        legend.background = element_rect(),
        panel.spacing = unit(1.2, "lines"),
        plot.caption = element_text(hjust = 0)) +
  scale_color_manual(breaks = HP.labs,
                     labels = names(HP.labs),
                     values = HPcomb_pal) +
  scale_shape_manual(breaks = HP.labs,
                     labels = names(HP.labs),
                     values = c(1, rep(16, 8))) +
  labs(x = "Runtime (seconds)",
       col = "Hyperparameter combination",
       shape = "Hyperparameter combination")
```

# Summary statistics

```{r}
mean_AUC_splitrule <- df %>% 
  filter(grepl("splitrule", hp_combination)) %>% 
  pull(AUC) %>%
  mean()
sd_AUC_splitrule   <- df %>% 
  filter(grepl("splitrule", hp_combination)) %>% 
  pull(AUC) %>%
  sd()
mean_AUC_nosplitrule <- df %>% 
  filter(!grepl("splitrule", hp_combination)) %>% 
  pull(AUC) %>%
  mean()
sd_AUC_nosplitrule   <- df %>% 
  filter(!grepl("splitrule", hp_combination)) %>% 
  pull(AUC) %>%
  sd()
```

```{r}
# primary outcome table
summary_table_study1 <- df %>%
  group_by(hp_combination) %>% 
  summarise(AUC_mean = round(mean(AUC), 2),
            AUC_sd = round(sd(AUC), 2),
            Calslope_median = round(median(CalSlope), 2),
            Calslope_IQR = round(IQR(CalSlope), 2),
            #Accuracy = paste0(round(mean(Accuracy),2), " (", round(sd(Accuracy), 2), ")"),
            `RMSD(slope)` = round(sqrt(mean((log(CalSlope))^2)), 2),
            Runtime_mean = round(mean(time), 1),
            Runtime_sd = round(sd(time), 1))
```

```{r}
# primary outcome table - per scenario
summary_table_study1_sc <- df %>%
  group_by(hp_combination, p, EF, n_prop) %>% 
  summarise(n = n(),
            AUC_mean = round(mean(AUC), 2),
            AUC_sd = round(sd(AUC), 2),
            Calslope_median = round(median(CalSlope), 2),
            Calslope_IQR = round(IQR(CalSlope), 2),
            #Accuracy = paste0(round(mean(Accuracy),2), " (", round(sd(Accuracy), 2), ")"),
            `RMSD(slope)` = round(sqrt(mean((log(CalSlope))^2)), 2),
            Runtime_mean = round(mean(time), 1),
            Runtime_sd = round(sd(time), 1))
```

# Save the relevant plots and figures

```{r}
save(plot_ex_met, summary_table_study1, mean_AUC_splitrule, mean_AUC_nosplitrule, sd_AUC_splitrule, sd_AUC_nosplitrule, summary_table_study1_sc, failed_1, failed_1_re, file = "./../Output/results.RData")
```

