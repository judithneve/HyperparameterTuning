---
title: "Study 2: plot and table generation"
author: "Judith Neve"
date: '2023-02-26'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

# Read in data

```{r}
# read all the files into a dataframe
df <- list.files(path = "./Data/sim/", pattern = ".rds") %>%
  paste0("./Data/sim/", .) %>% 
  map(readRDS) %>% 
  bind_rows() %>% 
  mutate(time = time*60) # convert the runtime to seconds
```

# Primary data checks

```{r}
# check we have the correct number of datasets: expect 6000
unique(df$start_seed) %>% length()
# if datasets are missing: check which datasets we don't have
expected_seeds <- (1:6000)*100
expected_seeds[!(expected_seeds %in% unique(df$start_seed))]/100
# 788 failed
failed <- c(788) %>%
  unique()
```

## Check time it took to rerun failed

```{r}
# in hours: is it more than 3:30:00 (the allotted runtime for this study)
df %>%
  filter(start_seed %in% failed) %>% 
  group_by(start_seed) %>% 
  summarise(time = sum(time)/3600)
```

# Plots

## Help dataframes

```{r scale setting}
# set the min and max value for the y-axis of each facet
df_scale_setter <- tibble(
  AUC = rep(c(0.5, 1), each = 8),
  BrierScore = rep(c(0, 1), each = 8),
  CalIntercept = rep(c(-1, 1), each = 8),
  CalSlope = rep(c(0, 5), each = 8),
  LogLoss = rep(c(0, 1), each = 8),
  Accuracy = rep(c(0, 1), each = 8),
  CohensKappa = rep(c(0, 1), each = 8),
  time = 0,
  metric = rep(unique(df$metric), 2)
) %>%
  pivot_longer(AUC:CohensKappa,
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric, levels = c("AUC", "CalSlope", "BrierScore", "CalIntercept", "LogLoss", "Accuracy", "CohensKappa")))

# information about what the "ideal" measure is
df_targets <- tibble(
  AUC = 1,
  BrierScore = 0,
  CalIntercept = 0,
  CalSlope = 1,
  LogLoss = 0,
  Accuracy = 1,
  CohensKappa = 1,
  time = 0,
  metric = unique(df$metric)
) %>%
  pivot_longer(AUC:CohensKappa,
               names_to = "Metric",
               values_to = "Target") %>%
  mutate(Metric = factor(Metric, levels = c("AUC", "CalSlope", "BrierScore", "CalIntercept", "LogLoss", "Accuracy", "CohensKappa")))

# set the limit for the x-axis: longest time for 8 predictors & longest time for 16 predictors (depending on the scenario)
df_timelimits <- expand.grid(
  metric = unique(df$metric),
  p      = c(8, 16),
  EF     = c(0.1, 0.3, 0.5),
  n_prop = c(0.5, 1),
  Metric = factor(c("AUC", "CalSlope", "BrierScore", "CalIntercept", "LogLoss", "Accuracy", "CohensKappa"), levels = c("AUC", "CalSlope", "BrierScore", "CalIntercept", "LogLoss", "Accuracy", "CohensKappa")),
  Performance          = 0
) %>% 
  mutate(time = ifelse(p == 8,
                       max(df %>% filter(p == 8) %>% pull(time)),
                       max(df %>% filter(p == 16) %>% pull(time))),
         scenario = paste(n_prop, p, EF)) %>% 
  arrange(p)

# labeller vectors
## performance metric
Metric.labs <- paste("Performance metric:", c("calibration slope", "calibration intercept", "Brier score", "logarithmic loss", "AUC", "classification accuracy", "Cohen's Kappa"))
names(Metric.labs) <- c("CalSlope", "CalIntercept", "BrierScore", "LogLoss", "AUC", "Accuracy", "CohensKappa")
## optimisation metric
opt.labs <- c("CalSlope", "CalInt", "BrierScore", "LogLoss", "AUC", "Accuracy", "Kappa", "Deviance")
names(opt.labs) <- c(Metric.labs, "Deviance")
opt.labs <- opt.labs[c(8, 5, 4, 6, 7, 3, 2, 1)]

# define the colour palettes
metric_pal <- viridis::plasma(n = 8)
# c("#b9dd56","#98544c","#57829b","#6d8640","#ffb58a","#979265","#67f3a7","#3fbb2b")
same_pal <- viridis::cividis(2)
```

## Report plots

```{r scenario by metric}
# full plot: facet by scenario and metric
plot_sc_met <- df %>%
  group_by(n_prop, p, EF) %>% 
  mutate(scenario = paste(n_prop, p, EF)) %>% 
  pivot_longer(AUC:CohensKappa,
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric,
                         levels = c("AUC", "CalSlope", "CalIntercept", "BrierScore", "LogLoss", "Accuracy", "CohensKappa"))) %>%
  ggplot(aes(x = time,
             y = Performance,
             group = metric)) +
  geom_point(aes(col = metric,
                 shape = metric)) +
  geom_point(data = df_scale_setter, alpha = 0) +
  geom_point(data = df_timelimits, alpha = 0) +
  geom_hline(data = df_targets, aes(yintercept = Target), col = "red", lty = "dotted") +
  facet_wrap(factor(scenario, levels = unique(df_timelimits$scenario)) ~ Metric,
             scales = "free",
             labeller = labeller(Metric = Metric.labs),
             ncol = 7) +
  theme_classic() +
  theme(legend.text = element_text(size = 8),
        legend.key.size = unit(0.4, 'cm'),
        legend.background = element_rect(),
        panel.spacing = unit(1.2, "lines"),
        plot.caption = element_text(hjust = 0)) +
  scale_color_manual(breaks = names(opt.labs),
                     labels = opt.labs,
                     values = metric_pal) +
  scale_shape_manual(breaks = names(opt.labs),
                     labels = opt.labs,
                     values = rep(16, 8)) +
  labs(caption = "Red dotted lines show ideal performance.",
       x = "Runtime (seconds)",
       col = "Optimisation metric",
       shape = "Optimisation metric")

# ggsave("PilotPlot.png", plot_sc_met, width = 20, height = 40, units = "in")
```

```{r}
# example plot: one scenario, facetted by performance metric
plot_ex_met <- df %>%
  filter(n_prop == 1,
         p      == 8,
         EF     == 0.3) %>% 
  pivot_longer(AUC:CohensKappa,
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric,
                         levels = c("AUC", "CalSlope", "CalIntercept", "BrierScore", "LogLoss", "Accuracy", "CohensKappa"))) %>%
  ggplot(aes(x = time,
             y = Performance,
             group = metric)) +
  geom_point(aes(col = metric), alpha = 0.7) +
  geom_point(data = df_scale_setter, alpha = 0) +
  geom_hline(data = df_targets, aes(yintercept = Target), col = "red", lty = "dotted") +
  facet_wrap(~ Metric,
             scales = "free",
             labeller = labeller(Metric = Metric.labs),
             ncol = 2) +
  theme_classic() +
  theme(legend.position = c(.75, .1),
        legend.text = element_text(size = 8),
        legend.key.size = unit(0.4, 'cm'),
        legend.background = element_rect(),
        panel.spacing = unit(1.2, "lines"),
        plot.caption = element_text(hjust = 0)) +
  scale_color_manual(breaks = opt.labs,
                     labels = names(opt.labs),
                     values = metric_pal) +
  scale_shape_manual(breaks = opt.labs,
                     labels = names(opt.labs),
                     values = rep(16, 8)) +
  labs(caption = "Red dotted lines show ideal performance.",
       x = "Runtime (seconds)",
       col = "Optimisation metric",
       shape = "Optimisation metric")
```

```{r}
# example plot: one scenario, facetted by performance metric - coloured by whether the performance and optimisation metrics are the same
plot_ex_same <- df %>%
  filter(n_prop == 1,
         p      == 8,
         EF     == 0.3) %>% 
  pivot_longer(AUC:CohensKappa,
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric,
                         levels = c("AUC", "CalSlope", "CalIntercept", "BrierScore", "LogLoss", "Accuracy", "CohensKappa")),
         metric_st = recode(metric,
                            CalInt = "CalIntercept",
                            Kappa  = "CohensKappa"), 
         Same = ifelse(Metric == metric_st, T, F)) %>% # NOTE: wrong for calint and kappa
  ggplot(aes(x = time,
             y = Performance,
             group = metric)) +
  geom_point(aes(col = Same,
                 shape = Same),
             alpha = 0.75) +
  # xlim(0, max(df$time)) +
  geom_point(data = df_scale_setter, alpha = 0) +
  # geom_point(data = df_timelimits, alpha = 0) +
  geom_hline(data = df_targets, aes(yintercept = Target), col = "red", lty = "dotted") +
  facet_wrap(~ Metric,
             scales = "free",
             labeller = labeller(Metric = Metric.labs),
             ncol = 2) +
  theme_classic() +
  theme(legend.position = c(.75, .1),
        legend.text = element_text(size = 8),
        legend.key.size = unit(0.4, 'cm'),
        legend.background = element_rect(),
        panel.spacing = unit(1.2, "lines"),
        plot.caption = element_text(hjust = 0)) +
  scale_color_manual(breaks = c(T, F),
                     labels = c("Same optimisation and performance metric",
                                "Different optimisation and performance metric"),
                     values = same_pal) +
  scale_shape_manual(breaks = c(T, F),
                     labels = c("Same optimisation and performance metric",
                                "Different optimisation and performance metric"),
                     values = c(16, 1)) +
  labs(caption = "Red dotted lines show ideal performance.",
       x = "Runtime (seconds)",
       col = "Optimisation metric",
       shape = "Optimisation metric")
```


# Summary statistics

```{r}
# overall AUC
mean_AUC <- df$AUC %>% mean()
sd_AUC   <- df$AUC %>% sd()
```

```{r}
# primary outcome table
summary_table_study2 <- df %>%
  group_by(metric) %>% 
  summarise(AUC_mean = round(mean(AUC), 2),
            AUC_sd = round(sd(AUC), 2),
            Calslope_median = round(median(CalSlope), 2),
            Calslope_IQR = round(IQR(CalSlope), 2),
            #Accuracy = paste0(round(mean(Accuracy),2), " (", round(sd(Accuracy), 2), ")"),
            `RMSD(slope)` = round(sqrt(mean((log(CalSlope))^2)), 2),
            Runtime_mean = round(mean(time), 1),
            Runtime_sd = round(sd(time), 1)) %>% 
  arrange(`RMSD(slope)`)
```

```{r}
# primary outcome table - per scenario
summary_table_study2_sc <- df %>%
  group_by(metric, p, EF, n_prop) %>% 
  summarise(n = n(),
            AUC_mean = round(mean(AUC), 2),
            AUC_sd = round(sd(AUC), 2),
            Calslope_median = round(median(CalSlope), 2),
            Calslope_IQR = round(IQR(CalSlope), 2),
            #Accuracy = paste0(round(mean(Accuracy),2), " (", round(sd(Accuracy), 2), ")"),
            `RMSD(slope)` = round(sqrt(mean((log(CalSlope))^2)), 2),
            Runtime_mean = round(mean(time), 1),
            Runtime_sd = round(sd(time), 1))
```

# Per scenario tile plots

```{r}
# outcome plots
plot_calslope_s2 <- df %>%
  mutate(scenario = paste(n_prop, p, EF)) %>% 
  group_by(scenario, metric) %>% 
  summarise(calslope = median(CalSlope)) %>% 
  ggplot(aes(x = scenario, y = metric)) +
  geom_tile(aes(fill = calslope)) +  ##
  geom_text(aes(label = round(calslope, 2))) + ##
  scale_fill_gradient2(low = "white", high = "white", mid = "green", midpoint = 0, trans = "log")  + ## 
  xlab("Scenario") + 
  ylab("Optimisation metric") + 
  theme(panel.grid.major.x=element_blank(),
        panel.grid.minor.x=element_blank(), 
        panel.grid.major.y=element_blank(), 
        panel.grid.minor.y=element_blank(), 
        panel.background=element_rect(fill="white"))
plot_auc_s2 <- df %>%
  mutate(scenario = paste(n_prop, p, EF)) %>% 
  group_by(scenario, metric) %>% 
  summarise(AUC = mean(AUC)) %>% 
  ggplot(aes(x = scenario, y = metric)) +
  geom_tile(aes(fill = AUC)) +
  geom_text(aes(label = round(AUC, 2))) +
  scale_fill_gradient2(low = "white", high = "green")  +
  xlab("Scenario") + 
  ylab("Optimisation metric") + 
  theme(panel.grid.major.x=element_blank(),
        panel.grid.minor.x=element_blank(), 
        panel.grid.major.y=element_blank(), 
        panel.grid.minor.y=element_blank(), 
        panel.background=element_rect(fill="white"))
```

# Save the relevant plots and figures

```{r}
save(plot_ex_met, plot_ex_same, summary_table_study2, mean_AUC, sd_AUC, summary_table_study2_sc, plot_calslope_s2, plot_auc_s2, file = "Data/results.RData")
```
