---
title: "Study 2: plot and table generation"
author: "Judith Neve"
date: '2023-02-26'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

# Read in data

```{r}
# read all the files into a dataframe
df <- list.files(path = "./../Data/perfs/", pattern = ".rds") %>%
  paste0("./../Data/perfs/", .) %>% 
  map(readRDS) %>% 
  bind_rows() %>% 
  filter(metric != "Deviance") %>% # deviance and logloss are equivalent
  mutate(time = time*60) # convert the runtime to seconds
```

# Primary data checks

```{r}
# check we have the correct number of datasets: expect 6000
unique(df$start_seed) %>% length()
# if datasets are missing: check which datasets we don't have
expected_seeds <- (1:6000)*100
expected_seeds[!(expected_seeds %in% unique(df$start_seed))]/100
# 788 failed
failed <- c(788) %>%
  unique()
failed_2 <- failed
```

```{r}
# how many failed runs?
failed %>% length()
# how many failed runs took longer than 3:30 hours?
df_failed <- df %>% 
  filter(start_seed %in% (failed*100)) %>% 
  group_by(start_seed) %>% 
  summarise(time = sum(time))
df_failed %>% 
  filter(time > 3.5*60*60) %>% 
  nrow()
```

# Plots

## Help dataframes

```{r scale setting}
# set the min and max value for the y-axis of each facet
df_scale_setter <- tibble(
  AUC = rep(c(0.5, 1), each = 7),
  BrierScore = rep(c(0, 1), each = 7),
  CalIntercept = rep(c(-1, 1), each = 7),
  CalSlope = rep(c(0, 2.5), each = 7),
  LogLoss = rep(c(0, 1), each = 7),
  Accuracy = rep(c(0, 1), each = 7),
  CohensKappa = rep(c(0, 1), each = 7),
  time = 0,
  metric = rep(unique(df$metric), 2)
) %>%
  pivot_longer(AUC:CohensKappa,
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric, levels = c("AUC", "CalSlope", "BrierScore", "CalIntercept", "LogLoss", "Accuracy", "CohensKappa")))

# information about what the "ideal" measure is
df_targets <- tibble(
  AUC = 1,
  BrierScore = 0,
  CalIntercept = 0,
  CalSlope = 1,
  LogLoss = 0,
  Accuracy = 1,
  CohensKappa = 1,
  time = 0,
  metric = unique(df$metric)
) %>%
  pivot_longer(AUC:CohensKappa,
               names_to = "Metric",
               values_to = "Target") %>%
  mutate(Metric = factor(Metric, levels = c("AUC", "CalSlope", "BrierScore", "CalIntercept", "LogLoss", "Accuracy", "CohensKappa")))

# set the limit for the x-axis: longest time for 8 predictors & longest time for 16 predictors (depending on the scenario)
df_timelimits <- expand.grid(
  metric = unique(df$metric),
  p      = c(8, 16),
  EF     = c(0.1, 0.3, 0.5),
  n_prop = c(0.5, 1),
  Metric = factor(c("AUC", "CalSlope", "BrierScore", "CalIntercept", "LogLoss", "Accuracy", "CohensKappa"), levels = c("AUC", "CalSlope", "BrierScore", "CalIntercept", "LogLoss", "Accuracy", "CohensKappa")),
  Performance          = 0
) %>% 
  mutate(time = ifelse(p == 8,
                       max(df %>% filter(p == 8) %>% pull(time)),
                       max(df %>% filter(p == 16) %>% pull(time))),
         scenario = paste(n_prop, p, EF)) %>% 
  arrange(p)

# labeller vectors
## performance metric
Metric.labs <- paste("Performance metric:\n", c("calibration slope", "calibration intercept", "Brier score", "logarithmic loss", "AUC", "classification accuracy", "Cohen's Kappa"))
names(Metric.labs) <- c("CalSlope", "CalIntercept", "BrierScore", "LogLoss", "AUC", "Accuracy", "CohensKappa")
## Optimisation criterion
opt.labs <- c("CalSlope", "CalInt", "BrierScore", "LogLoss", "AUC", "Accuracy", "Kappa")
names(opt.labs) <- c("Calibration slope", "Calibration intercept", "Brier score", "Logarithmic loss", "AUC", "Classification accuracy", "Cohen's Kappa")
opt.labs <- opt.labs[c(4, 5, 6, 7, 3, 2, 1)]

# define the colour palettes
metric_pal <- viridis::plasma(n = 7)
# c("#b9dd56","#98544c","#57829b","#6d8640","#ffb58a","#979265","#67f3a7","#3fbb2b")
same_pal <- viridis::cividis(2)
```

## Report plots

```{r scenario by metric}
# full plot: facet by scenario and metric
# this is not saved - not in the report - can be seen in the shiny app or run here
plot_sc_met <- df %>%
  group_by(n_prop, p, EF) %>% 
  mutate(scenario = paste(n_prop, p, EF)) %>% 
  pivot_longer(AUC:CohensKappa,
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric,
                         levels = c("AUC", "CalSlope", "CalIntercept", "BrierScore", "LogLoss", "Accuracy", "CohensKappa"))) %>%
  ggplot(aes(x = time,
             y = Performance,
             group = metric)) +
  geom_point(aes(col = metric,
                 shape = metric)) +
  geom_point(data = df_scale_setter, alpha = 0) +
  geom_point(data = df_timelimits, alpha = 0) +
  geom_hline(data = df_targets, aes(yintercept = Target), col = "red", lty = "dotted") +
  facet_wrap(factor(scenario, levels = unique(df_timelimits$scenario)) ~ Metric,
             scales = "free",
             labeller = labeller(Metric = Metric.labs),
             ncol = 7) +
  theme_classic() +
  theme(legend.text = element_text(size = 8),
        legend.key.size = unit(0.4, 'cm'),
        legend.background = element_rect(),
        panel.spacing = unit(1.2, "lines"),
        plot.caption = element_text(hjust = 0)) +
  scale_color_manual(breaks = names(opt.labs),
                     labels = opt.labs,
                     values = metric_pal) +
  scale_shape_manual(breaks = names(opt.labs),
                     labels = opt.labs,
                     values = rep(16, 8)) +
  labs(x = "Runtime (seconds)",
       col = "Optimisation criterion",
       shape = "Optimisation criterion")

# ggsave("PilotPlot.png", plot_sc_met, width = 20, height = 40, units = "in")
```

```{r}
# example plot: one scenario, facetted by performance metric
plot_ex_met <- df %>%
  filter(n_prop == 1,
         p      == 16,
         EF     == 0.5) %>% 
  pivot_longer(AUC:CohensKappa,
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric,
                         levels = c("AUC", "CalSlope", "CalIntercept", "BrierScore", "LogLoss", "Accuracy", "CohensKappa"))) %>%
  ggplot(aes(x = time,
             y = Performance,
             group = metric)) +
  geom_point(aes(col = metric), alpha = 0.7) +
  geom_point(data = df_scale_setter, alpha = 0) +
  geom_hline(data = df_targets, aes(yintercept = Target), col = "red", lty = "dotted") +
  facet_wrap(~ Metric,
             scales = "free",
             labeller = labeller(Metric = Metric.labs),
             ncol = 2) +
  theme_classic() +
  theme(legend.position = c(.75, .1),
        legend.text = element_text(size = 8),
        legend.key.size = unit(0.4, 'cm'),
        legend.background = element_rect(),
        panel.spacing = unit(1.2, "lines"),
        plot.caption = element_text(hjust = 0)) +
  scale_color_manual(breaks = opt.labs,
                     labels = names(opt.labs),
                     values = metric_pal) +
  scale_shape_manual(breaks = opt.labs,
                     labels = names(opt.labs),
                     values = rep(16, 8)) +
  labs(x = "Runtime (seconds)",
       col = "Optimisation criterion",
       shape = "Optimisation criterion")
```


```{r}
# example plot: one scenario, facetted by performance metric - coloured by whether the performance and Optimisation criterions are the same
x.labs <- opt.labs
x.labs["Cohen's Kappa"] <- "CohensKappa"
x.labs["Calibration intercept"] <- "CalIntercept"

plot_ex_same <- df %>%
  # group_by(n_prop, p, EF) %>% 
  # mutate(scenario = paste(n_prop, p, EF)) %>% 
  # filter(n_prop == 1,
  #        p      == 16,
  #        EF     == 0.5) %>% 
  pivot_longer(AUC:CohensKappa,
               names_to = "Metric",
               values_to = "Performance") %>%
  mutate(Metric = factor(Metric,
                         levels = c("AUC", "CalSlope", "CalIntercept", "BrierScore", "LogLoss", "Accuracy", "CohensKappa")),
         metric_st = recode(metric,
                            CalInt = "CalIntercept",
                            Kappa  = "CohensKappa"), 
         Same = ifelse(Metric == metric_st, metric_st, "Other metrics") %>% 
           factor(levels = c("Other metrics", x.labs),
                  labels = c("Other metrics", names(x.labs)))) %>%
  ggplot(aes(x = Same,
             y = Performance)) +
  geom_boxplot() +
  coord_flip() +
  # xlim(0, max(df$time)) +
  # geom_point(data = df_scale_setter, alpha = 0) +
  # geom_point(data = df_timelimits, alpha = 0) +
  geom_hline(data = df_targets, aes(yintercept = Target), col = "red", lty = "dotted") +
  facet_wrap(~ Metric,
             scales = "free",
             labeller = labeller(Metric = Metric.labs),
             ncol = 2) +
  theme_classic() +
  theme(legend.position = c(.75, .1),
        legend.text = element_text(size = 8),
        legend.key.size = unit(0.4, 'cm'),
        legend.background = element_rect(),
        panel.spacing = unit(1.2, "lines"),
        plot.caption = element_text(hjust = 0)) +
  # scale_color_manual(breaks = c(T, F),
  #                    labels = c("Same optimisation and performance metric",
  #                               "Different optimisation and performance metric"),
  #                    values = same_pal) +
  # scale_shape_manual(breaks = c(T, F),
  #                    labels = c("Same optimisation and performance metric",
  #                               "Different optimisation and performance metric"),
  #                    values = c(16, 1)) +
  labs(x = "Optimisation criterion")

# ggsave("PilotPlot_box.png", plot_ex_same, width = 20, height = 40, units = "in")
```


# Summary statistics

```{r}
# overall AUC
mean_runtime <- df$time %>% mean()
sd_runtime   <- df$time %>% sd()
mean_runtime_sc <- df %>% group_by(n_prop, p, EF) %>% summarise(mean = mean(time),
                                                             sd = sd(time))
```

```{r}
# primary outcome table
summary_table_study2 <- df %>%
  group_by(metric) %>% 
  summarise(AUC_mean = round(mean(AUC), 2),
            AUC_sd = round(sd(AUC), 2),
            Calslope_median = round(median(CalSlope), 2),
            Calslope_IQR = round(IQR(CalSlope), 2),
            #Accuracy = paste0(round(mean(Accuracy),2), " (", round(sd(Accuracy), 2), ")"),
            `RMSD(slope)` = round(sqrt(mean((log(CalSlope))^2)), 2),
            Runtime_mean = round(mean(time), 1),
            Runtime_sd = round(sd(time), 1)) %>% 
  arrange(`RMSD(slope)`)
```

```{r}
# primary outcome table - per scenario
summary_table_study2_sc <- df %>%
  group_by(metric, p, EF, n_prop) %>% 
  summarise(n = n(),
            AUC_mean = round(mean(AUC), 2),
            AUC_sd = round(sd(AUC), 2),
            Calslope_median = round(median(CalSlope), 2),
            Calslope_IQR = round(IQR(CalSlope), 2),
            #Accuracy = paste0(round(mean(Accuracy),2), " (", round(sd(Accuracy), 2), ")"),
            `RMSD(slope)` = round(sqrt(mean((log(CalSlope))^2)), 2),
            Runtime_mean = round(mean(time), 1),
            Runtime_sd = round(sd(time), 1))
```

# Save the relevant plots and figures

```{r}
save(plot_ex_met, plot_ex_same, summary_table_study2, mean_runtime, sd_runtime, mean_runtime_sc, summary_table_study2_sc, failed_2, file = "./../Output/results.RData")
```

